{"title":"Lesson 3: Introduction to Simple Linear Regression (SLR)","markdown":{"yaml":{"title":"Lesson 3: Introduction to Simple Linear Regression (SLR)","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"01/13/2025","categories":["Week 1"],"format":{"revealjs":{"theme":"../simple_NW.scss","chalkboard":true,"slide-number":true,"show-slide-number":"all","width":1955,"height":1100,"footer":"Lesson 3: SLR 1","html-math-method":"mathjax","highlight-style":"ayu"}},"execute":{"echo":true,"freeze":"auto"},"editor":{"markdown":{"wrap":72}}},"headingText":"terminal: for icons","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)    \nlibrary(openintro)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra) # NEW!!!\nlibrary(readxl)\n\n# quarto install extension quarto-ext/fontawesome\n\n# set ggplot theme for slides \ntheme_set(theme_gray(base_size = 22))\n# theme_update(text = element_text(size=16))  # set global text size for ggplots\n\n```\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n\n## Process of regression data analysis\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n\n\n\n## Let's start with an example\n\n::: columns\n::: {.column width=\"55%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\ngapm1 <- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\")\n\ngapm = gapm1 %>% \n  drop_na(FemaleLiteracyRate, \n          LifeExpectancyYrs, \n          four_regions, \n          FoodSupplykcPPD)\n\ngapm_slr_plot = gapm %>%\n  ggplot(aes(x = FemaleLiteracyRate,\n             y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\ngapm_slr_plot\n\n```\n:::\n\n::: {.column width=\"45%\"}\n \n\nLife expectancy vs. female literacy rate\n\n \n\n-   Each point on the plot is for a different country\n\n \n\n-   $X$ = country's adult female literacy rate\n\n \n\n-   $Y$ = country's life expectancy (years)\n:::\n:::\n\n::: heq\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n:::\n\n## Reference: How did I code that? {.smaller}\n\n![](../img_slides/pause.png){.absolute top=\"83%\" right=\"0%\" width=\"120\" height=\"120\"}\n\n```{r}\n#| fig-height: 6.5\n#| fig-width: 10\n#| fig-align: center\n\ngapm %>%\n  ggplot(aes(x = FemaleLiteracyRate,\n             y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n```\n\n## Research and dataset description\n\n::: hl3\n**Research question:** Is there an association between life expectancy and female literacy rates?\n:::\n\n-   Data file: `Gapminder_vars_2011.xlsx`\n\n-   Data were downloaded from\n    [Gapminder](https://www.gapminder.org/data/){.uri}\n    -   2011 is the most recent year with the most complete data\n    -   Observational study measuring different characteristics of countries, including population, health, environment, work, etc.\n\n-   [**Life\n    expectancy**](https://www.gapminder.org/data/documentation/gd004/) =\n    the average number of years a newborn child would live if current\n    mortality patterns were to stay the same.\n\n-   [**Adult literacy rate**](http://data.uis.unesco.org/) is the\n    percentage of people ages 15 and above who can, with understanding,\n    read and write a short, simple statement on their everyday life.\n    \n \n\n- [National Literacy Trust](https://literacytrust.org.uk/research-services/research-reports/literacy-and-life-expectancy/) in England has studied the link between these two variables\n  - Please note that they clearly state that literacy is linked to life expectancy **through many socioeconomic and health factors**\n\n\n## Poll Everywhere Question 1\n\n## Get to know the data (1/3)\n\n-   Load data\n\n```{r}\nlibrary(readxl)\ngapm1 <- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\")\n```\n\n## Get to know the data (2/3)\n\n-   Glimpse of the data\n\n```{r}\nglimpse(gapm1)\n```\n\n-   Note the missing values for our variables of interest\n\n## Get to know the data (3/3)\n\n-   Get a sense of the summary statistics\n\n```{r}\ngapm1 %>% \n  select(LifeExpectancyYrs, \n         FemaleLiteracyRate) %>% \n  summary()\n```\n\n## Remove missing values (1/2)\n\n-   Remove rows with missing data for life expectancy and female\n    literacy rate\n\n```{r}\ngapm <- gapm1 %>% drop_na(LifeExpectancyYrs, FemaleLiteracyRate)\nglimpse(gapm)\n```\n\n-   No missing values now for our variables of interest\n\n## Remove missing values (2/2)\n\n-   And no more missing values when we look only at our two variables of\n    interest\n\n```{r}\ngapm %>%\n  select(LifeExpectancyYrs, \n          FemaleLiteracyRate) %>% \n  get_summary_stats()\n```\n\n::: definition\n::: def-title\nNote\n:::\n\n::: def-cont\n-   Removing the rows with missing data was not needed to run the\n    regression model.\n-   I did this step since later we will be calculating the standard\n    deviations of the explanatory and response variables for *just the\n    values included in the regression model*. It'll be easier to do this\n    if we remove the missing values now.\n:::\n:::\n\n## Poll Everywhere Question 2\n\n# Learning Objectives\n\n::: lob\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n:::\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## Questions we can ask with a simple linear regression model\n\n::: columns\n::: {.column width=\"50%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\ngapm_slr_plot \n\n```\n:::\n\n::: {.column width=\"50%\"}\n-   How do we...\n    -   calculate slope & intercept?\n    -   interpret slope & intercept?\n    -   do inference for slope & intercept?\n        -   CI, p-value\n    -   do prediction with regression line?\n        -   CI for prediction?\n-   Does the model fit the data well?\n    -   Should we be using a line to model the data?\n-   Should we add additional variables to the model?\n    -   multiple/multivariable regression\n:::\n:::\n\n::: heq\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n:::\n\n## Association vs. prediction\n\n::: columns\n::: {.column width=\"50%\"}\n::: proof1\n::: proof-title\nAssociation\n:::\n\n::: proof-cont\n-   What is the association between countries’ life expectancy and\n    female literacy rate?\n-   Use the slope of the line or correlation coefficient\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: definition\n::: def-title\nPrediction\n:::\n\n::: def-cont\n-   What is the expected life expectancy for a country with a\n    specified female literacy rate?    \n:::\n:::\n:::\n:::\n\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n\n```{r}\n#| fig-height: 7\n#| fig-width: 12\n#| echo: false\n#| fig-align: center\n\ngapm_slr_plot \n\n```\n\n## Three types of study design (there are more)\n\n::: columns\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nExperiment\n:::\n\n::: def-cont\n-   Observational units are randomly assigned to important predictor\n    levels\n\n    -   Random assignment controls for confounding variables (age,\n        gender, race, etc.)\n\n    -   “gold standard” for determining causality\n\n    -   Observational unit is often at the participant-level\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: example\n::: ex-title\nQuasi-experiment\n:::\n\n::: ex-cont\n-   Participants are assigned to intervention levels without\n    randomization\n\n-   Not common study design\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: proposition\n::: prop-title\nObservational\n:::\n\n::: prop-cont\n-   No randomization or assignment of intervention conditions\n\n-   In general cannot infer causality\n\n    -   However, there are casual inference methods…\n:::\n:::\n:::\n:::\n\n## Let's revisit the regression analysis process\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n\n## Poll Everywhere Question 3\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n::: lob\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n:::\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## Simple Linear Regression Model\n\nThe (population) regression model is denoted by:\n\n \n\n::: heq\n$$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n:::\n\n \n\n::: columns\n::: column\n#### Observable sample data\n\n-   $Y$ is our dependent variable\n\n    -   Aka outcome or response variable\n\n-   $X$ is our independent variable\n\n    -   Aka predictor, regressor, exposure variable\n:::\n\n::: column\n#### Unobservable population parameters\n\n-   $\\beta_0$ and $\\beta_1$ are **unknown** population parameters\n\n-   $\\epsilon$ (epsilon) is the error about the line\n\n    -   It is assumed to be a random variable with a...\n\n        -   Normal distribution with mean 0 and constant variance\n            $\\sigma^2$\n\n        -   i.e. $\\epsilon \\sim N(0, \\sigma^2)$\n:::\n:::\n\n## Simple Linear Regression Model (another way to view components)\n\nThe (population) regression model is denoted by:\n\n \n\n::: heq\n$$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n:::\n\n \n\n### Components\n\n|            |                                            |\n|------------|--------------------------------------------|\n| $Y$        | response, outcome, dependent variable      |\n| $\\beta_0$  | intercept                                  |\n| $\\beta_1$  | slope                                      |\n| $X$        | predictor, covariate, independent variable |\n| $\\epsilon$ | residuals, error term                      |\n\n## If the population parameters are unobservable, how did we get the line for life expectancy?\n\n::: columns\n::: {.column width=\"40%\"}\n \n\n::: hl\nNote: the **population model is the true, underlying model** that we are\ntrying to estimate using our sample data\n\n-   Our goal in simple linear regression is to estimate $\\beta_0$ and\n    $\\beta_1$\n:::\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n#| fig-align: center\n\ngapm_slr_plot \n\n```\n:::\n:::\n\n## Poll Everywhere Question 4\n\n## Okay, so how do we estimate the regression line? {visibility=\"hidden\"}\n\n \n\nAt this point, we are going to move over to an R shiny app that I made.\n\n \n\nLet's see if we can eyeball the best-fit line!\n\n## Regression line = best-fit line\n\n::: columns\n::: {.column width=\"50%\"}\n$$\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X $$\n\n-   $\\widehat{Y}$ is the predicted outcome for a specific value of $X$\n-   $\\widehat{\\beta}_0$ is the intercept *of the best-fit line*\n-   $\\widehat{\\beta}_1$ is the slope *of the best-fit line*, i.e., the\n    increase in $\\widehat{Y}$ for every increase of one (unit increase)\n    in $X$\n    -   slope = *rise over run*\n:::\n\n::: {.column width=\"50%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\ngapm_slr_plot \n```\n:::\n:::\n\n## Simple Linear Regression Model\n\n::: columns\n::: {.column width=\"50%\"}\n#### Population regression *model*\n\n::: lob\n$$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n:::\n\n \n\n##### Components\n\n|            |                                            |\n|------------|--------------------------------------------|\n| $Y$        | response, outcome, dependent variable      |\n| $\\beta_0$  | intercept                                  |\n| $\\beta_1$  | slope                                      |\n| $X$        | predictor, covariate, independent variable |\n| $\\epsilon$ | residuals, error term                      |\n:::\n\n::: {.column width=\"50%\"}\n#### Estimated regression *line*\n\n::: hl\n$$\\widehat{Y} =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X$$\n:::\n\n \n\n##### Components\n\n|                     |                                                   |\n|---------------------|---------------------------------------------------|\n| $\\widehat{Y}$       | *estimated expected* response given predictor $X$ |\n| $\\widehat{\\beta}_0$ | *estimated* intercept                             |\n| $\\widehat{\\beta}_1$ | *estimated* slope                                 |\n| $X$                 | predictor, covariate, independent variable        |\n:::\n:::\n\n## We get it, Nicky! How do we estimate the regression line?\n\nFirst let's take a break!!\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n::: lob\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n:::\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## It all starts with a residual...\n\n::: columns\n::: {.column width=\"50%\"}\n-   Recall, one characteristic of our population model was that the\n    residuals, $\\epsilon$, were Normally distributed:\n    $\\epsilon \\sim N(0, \\sigma^2)$\n\n-   In our population regression model, we had:\n    $$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n\n-   We can also take the average (expected) value of the population\n    model\n\n-   We take the expected value of both sides and get:\n\n$$\\begin{aligned} \n        E[Y] & = E[\\beta_0 + \\beta_1X + \\epsilon] \\\\\n        E[Y] & = E[\\beta_0] + E[\\beta_1X] + E[\\epsilon] \\\\\n        E[Y] & = \\beta_0 + \\beta_1X + E[\\epsilon] \\\\\n        E[Y|X] & = \\beta_0 + \\beta_1X \\\\\n\\end{aligned}$$\n\n-   We call $E[Y|X]$ the expected value (or average) of $Y$ given $X$\n:::\n\n::: {.column width=\"50%\"}\n![](../img_slides/OLSassumptions-1.png){fig-align=\"center\"}\n:::\n:::\n\n## So now we have two representations of our population model\n\n::: columns\n::: {.column width=\"50%\"}\n::: definition\n::: def-title\nWith observed $Y$ values and residuals:\n:::\n\n::: def-cont\n$$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: fact\n::: fact-title\nWith the population expected value of $Y$ given $X$:\n:::\n\n::: fact-cont\n$$E[Y|X] = \\beta_0 + \\beta_1X$$\n:::\n:::\n:::\n:::\n\nUsing the two forms of the model, we can figure out a formula for our\nresiduals:\n\n$$\\begin{aligned}\nY & = (\\beta_0 + \\beta_1X) + \\epsilon \\\\\nY & = E[Y|X] + \\epsilon \\\\\nY - E[Y|X] & = \\epsilon \\\\ \n\\epsilon & = Y - E[Y|X]\n\\end{aligned}$$\n\nAnd so we have our **true, population model**, residuals!\n\n::: hl\nThis is an important fact! For the **population model**, the residuals:\n$\\epsilon = Y - E[Y|X]$\n:::\n\n## Back to our estimated model\n\nWe have the same two representations of our estimated/fitted model:\n\n::: columns\n::: {.column width=\"50%\"}\n::: definition\n::: def-title\nWith observed values:\n:::\n\n::: def-cont\n$$Y =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X + \\widehat{\\epsilon}$$\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: fact\n::: fact-title\nWith the estimated expected value of $Y$ given $X$:\n:::\n\n::: fact-cont\n$$\\begin{aligned} \n\\widehat{E}[Y|X] & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{E[Y|X]} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{Y} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\end{aligned}$$\n:::\n:::\n:::\n:::\n\nUsing the two forms of the model, we can figure out a formula for our\nestimated residuals:\n\n$$\\begin{aligned}\nY & = (\\widehat{\\beta}_0 + \\widehat{\\beta}_1X) + \\widehat\\epsilon \\\\\nY & = \\widehat{Y} + \\widehat\\epsilon \\\\\n\\widehat\\epsilon & = Y - \\widehat{Y}\n\\end{aligned}$$\n\n::: lob\nThis is an important fact! For the **estimated/fitted model**, the\nresiduals: $\\widehat\\epsilon = Y - \\widehat{Y}$\n:::\n\n## *Individual* $i$ residuals in the estimated/fitted model\n\n::: columns\n::: {.column width=\"45%\"}\n-   **Observed values for each country** $i$: $Y_i$\n    -   Value in the dataset for country $i$\n-   **Fitted value for each country** $i$: $\\widehat{Y}_i$\n    -   Value that falls on the best-fit line for a specific $X_i$\n    -   If two individuals have the same $X_i$, then they have the same\n        $\\widehat{Y}_i$\n:::\n\n::: {.column width=\"55%\"}\n```{r}\n#| message: false\n#| echo: false\n#| fig-height: 8\n#| fig-width: 11\n#| fig-align: center\n# code from https://drsimonj.svbtle.com/visualising-residuals\n\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nregression_points <- augment(model1)\n# summary(model1)\n# sum(model1$residuals^2)\n\nggplot(regression_points, \n       aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  # geom_segment(aes(\n  #   xend = female_literacy_rate_2011, \n  #   yend = .fitted), \n  #   alpha = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F14124\", size=3) +\n  # > Color adjustments made here...\n  geom_point(color = \"black\", size = 4) +  # Color mapped here\n  #scale_color_gradient2(low = \"#213c96\", mid = \"white\", high = \"#F14124\") +  # Colors to use here\n    #guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 4) +\nlabs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))  \n```\n:::\n:::\n\n## *Individual* $i$ residuals in the estimated/fitted model\n\n::: columns\n::: {.column width=\"45%\"}\n-   **Observed values for each individual** $i$: $Y_i$\n\n    -   Value in the dataset for individual $i$\n\n-   **Fitted value for each individual** $i$: $\\widehat{Y}_i$\n\n    -   Value that falls on the best-fit line for a specific $X_i$\n    -   If two individuals have the same $X_i$, then they have the same\n        $\\widehat{Y}_i$\n\n::: hl3\n-   **Residual for each individual:**\n    $\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i$\n\n    -   Difference between the observed and fitted value\n:::\n:::\n\n::: {.column width=\"55%\"}\n```{r}\n#| message: false\n#| echo: false\n#| fig-height: 8\n#| fig-width: 11\n#| fig-align: center\n# code from https://drsimonj.svbtle.com/visualising-residuals\n\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nregression_points <- augment(model1)\n# summary(model1)\n# sum(model1$residuals^2)\n\nggplot(regression_points, \n       aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_segment(aes(\n    xend = FemaleLiteracyRate, \n    yend = .fitted), \n    alpha = 1, \n    color = \"#4FADF3\", \n    size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F14124\", size=3) +\n  # > Color adjustments made here...\n  geom_point(color = \"black\", size = 4) +  # Color mapped here\n  #scale_color_gradient2(low = \"#213c96\", mid = \"white\", high = \"#F14124\") +  # Colors to use here\n    #guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 4) +\nlabs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))  \n```\n:::\n:::\n\n## Poll Everywhere Question 5\n\n## So what do we do with the residuals?\n\n-   We want to **minimize the residuals**\n\n    -   Aka minimize the difference between the observed $Y$ value and\n        the estimated expected response given the predictor (\n        $\\widehat{E}[Y|X]$ )\n\n-   **We can use ordinary least squares (OLS) to do this in linear\n    regression!**\n\n-   Idea behind this: reduce the total error between the fitted line and\n    the observed point (error between is called residuals)\n\n    -   Vague use of total error: more precisely, we want to **reduce\n        the sum of squared errors**\n    -   Think back to my R Shiny app!\n    -   We need to mathematically define this!\n\n \n\n \n\n-   Note: there are other ways to estimate the best-fit line!!\n\n    -   Example: Maximum likelihood estimation\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n::: lob\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n:::\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## Setting up for ordinary least squares\n\n::: columns\n::: {.column width=\"75%\"}\n-   Sum of Squared Errors (SSE)\n\n$$ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\end{aligned}$$\n:::\n\n::: {.column width=\"25%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   $\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i$\n\n-   $\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1X_i$\n:::\n:::\n:::\n:::\n\n::: hl\nThen we want to find the estimated coefficient values that minimize the\nSSE!\n:::\n\n## Steps to estimate coefficients using OLS\n\n1.  Set up SSE (previous slide)\n\n2.  Minimize SSE with respect to coefficient estimates\n\n    -   Need to solve a system of equations\n\n3.  Compute derivative of SSE wrt $\\widehat\\beta_0$\n\n4.  Set derivative of SSE wrt $\\widehat\\beta_0 = 0$\n\n5.  Compute derivative of SSE wrt $\\widehat\\beta_1$\n\n6.  Set derivative of SSE wrt $\\widehat\\beta_1 = 0$\n\n7.  Substitute $\\widehat\\beta_1$ back into $\\widehat\\beta_0$\n\n## 2. Minimize SSE with respect to coefficients\n\n-   Want to minimize with respect to (wrt) the potential coefficient\n    estimates ( $\\widehat\\beta_0$ and $\\widehat\\beta_1$)\n\n-   Take derivative of SSE wrt $\\widehat\\beta_0$ and $\\widehat\\beta_1$\n    and set equal to zero to find minimum SSE\n\n$$\n\\dfrac{\\partial SSE}{\\partial \\widehat\\beta_0} = 0 \\text{ and } \\dfrac{\\partial SSE}{\\partial \\widehat\\beta_1} = 0\n$$\n\n-   Solve the above system of equations in steps 3-6\n\n## 3. Compute derivative of SSE wrt $\\widehat\\beta_0$\n\n::: columns\n::: {.column width=\"80%\"}\n$$\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n$$\n\n$$\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0}& =\\frac{\\partial\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)^2}{\\partial{\\widehat{\\beta}}_0}=\n\\sum_{i=1}^{n}\\frac{{\\partial\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}^2}{\\partial{\\widehat{\\beta}}_0} \\\\\n& =\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\\left(-1\\right)}=\\sum_{i=1}^{n}{-2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & = -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\n\\end{aligned}$$\n:::\n\n::: {.column width=\"20%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   Derivative rule: derivative of sum is sum of derivative\n\n-   Derivative rule: chain rule\n:::\n:::\n:::\n:::\n\n## 4. Set derivative of SSE wrt $\\widehat\\beta_0 = 0$\n\n::: columns\n::: {.column width=\"75%\"}\n$$\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & =0 \\\\ -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\\n\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\ \\sum_{i=1}^{n}Y_i-n{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\sum_{i=1}^{n}X_i & =0 \\\\\n\\frac{1}{n}\\sum_{i=1}^{n}Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\frac{1}{n}\\sum_{i=1}^{n}X_i & =0 \\\\ \n\\overline{Y}-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\overline{X} & =0 \\\\\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\n\\end{aligned}$$\n:::\n\n::: {.column width=\"25%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   $\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i$\n-   $\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i$\n:::\n:::\n:::\n:::\n\n## 5. Compute derivative of SSE wrt $\\widehat\\beta_1$\n\n::: columns\n::: {.column width=\"80%\"}\n$$\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n$$\n\n$$\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1}& =\\frac{\\partial\\sum_{i=1}^{n}{(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1}=\\sum_{i=1}^{n}\\frac{{\\partial(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1} \\\\\n&=\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)(-X_i)}=\\sum_{i=1}^{n}{-2X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\ &=-2\\sum_{i=1}^{n}{X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}\n\\end{aligned}$$\n:::\n\n::: {.column width=\"20%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   Derivative rule: derivative of sum is sum of derivative\n\n-   Derivative rule: chain rule\n:::\n:::\n:::\n:::\n\n## 6. Set derivative of SSE wrt $\\widehat\\beta_1 = 0$\n\n::: columns\n::: {.column width=\"70%\"}\n$$\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1} & =0 \\\\ \\sum_{i=1}^{n}\\left({X_iY}_i-{\\widehat{\\beta}}_0X_i-{\\widehat{\\beta}}_1{X_i}^2\\right)&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i{\\widehat{\\beta}}_0}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1}&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\left(\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\right)}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\overline{Y}}+\\sum_{i=1}^{n}{{\\widehat{\\beta}}_1X_i\\overline{X}}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+\\sum_{i=1}^{n}{({\\widehat{\\beta}}_1X_i\\overline{X}}-{X_i}^2{\\widehat{\\beta}}_1) &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+{\\widehat{\\beta}}_1\\sum_{i=1}^{n}{X_i(\\overline{X}}-X_i) &=0 \\\\\n\\end{aligned}$$\n:::\n\n::: {.column width=\"30%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   ${\\widehat{\\beta}}_0=\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}$\n-   $\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i$\n-   $\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i$\n:::\n:::\n\n         \n\n \n\n::: heq\n$${\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}$$\n:::\n:::\n:::\n\n## 7. Substitute $\\widehat\\beta_1$ back into $\\widehat\\beta_0$\n\n### Final coefficient estimates for SLR\n\n::: columns\n::: {.column width=\"50%\"}\n::: proof1\n::: proof-title\nCoefficient estimate for $\\widehat\\beta_1$\n:::\n\n::: proof-cont\n$${\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}$$\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: fact\n::: fact-title\nCoefficient estimate for $\\widehat\\beta_0$\n:::\n\n::: fact-cont\n$$\\begin{aligned}\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X} \\\\\n{\\widehat{\\beta}}_0 & = \\overline{Y} - \\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})} \\overline{X} \\\\\n\\end{aligned}$$\n:::\n:::\n:::\n:::\n\n## Poll Everywhere Question 6\n\n## Do I need to do all that work every time??\n\n![](../img_slides/hoopla.jpeg){fig-align=\"center\"}\n\n## Regression in R: `lm()`\n\n-   Let's discuss the syntax of this function\n\n```{r}\n#| out-height: 800px \n\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n```\n\n \n\n \n\nIn the general form:\n\n```{r}\n#| eval: false\n\nlm( Y ~ X, data = dataset_name)\ndataset_name %>% lm( formula = Y ~ X )\n```\n\n## Regression in R: `lm()` + `summary()`\n\n```{r}\n#| out-height: 800px \n\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nsummary(model1)\n```\n\n\n## Regression in R: `lm()` + `tidy()`\n\n \n\n```{r}\ntidy(model1) %>% \n  gt() %>% \n  tab_options(table.font.size = 45)\n```\n\n \n\n-   Regression equation for our model (which we saw a looong time ago):\n\n::: heq\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n:::\n\n## How do we interpret the coefficients?\n\n::: heq\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n:::\n\n-   **Intercept ($\\hat{\\beta}_0$)**\n    -   The expected outcome for the $Y$-variable when the $X$-variable (if continuous)\n        is 0\n    -   **Example:** The expected/average life expectancy is 50.9 years\n        for a country with 0% female literacy.\n-   **Slope ($\\hat{\\beta}_1$)**\n    -   For every increase of 1 unit in the $X$-variable (if continuous), there is an\n        expected increase of $\\widehat\\beta_1$ units in the\n        $Y$-variable.\n\n    -   We only say that there is an expected increase and not\n        necessarily a causal increase.\n\n    -   **Example:** For every 1 percent increase in the female literacy\n        rate, life expectancy increases, on average, 0.232 years.\n          - Can also say \"...average life expectancy increases 0.232...\"\n\n## Next time\n\n-   More on interpreting the estimate coefficients\n\n-   Inference of our estimated coefficients\n\n-   Inference of estimated expected $Y$ given $X$\n\n-   Prediction\n\n-   Hypothesis testing!\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)    \nlibrary(openintro)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra) # NEW!!!\nlibrary(readxl)\n\n# terminal: for icons\n# quarto install extension quarto-ext/fontawesome\n\n# set ggplot theme for slides \ntheme_set(theme_gray(base_size = 22))\n# theme_update(text = element_text(size=16))  # set global text size for ggplots\n\n```\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n\n## Process of regression data analysis\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n\n\n\n## Let's start with an example\n\n::: columns\n::: {.column width=\"55%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\ngapm1 <- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\")\n\ngapm = gapm1 %>% \n  drop_na(FemaleLiteracyRate, \n          LifeExpectancyYrs, \n          four_regions, \n          FoodSupplykcPPD)\n\ngapm_slr_plot = gapm %>%\n  ggplot(aes(x = FemaleLiteracyRate,\n             y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\ngapm_slr_plot\n\n```\n:::\n\n::: {.column width=\"45%\"}\n \n\nLife expectancy vs. female literacy rate\n\n \n\n-   Each point on the plot is for a different country\n\n \n\n-   $X$ = country's adult female literacy rate\n\n \n\n-   $Y$ = country's life expectancy (years)\n:::\n:::\n\n::: heq\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n:::\n\n## Reference: How did I code that? {.smaller}\n\n![](../img_slides/pause.png){.absolute top=\"83%\" right=\"0%\" width=\"120\" height=\"120\"}\n\n```{r}\n#| fig-height: 6.5\n#| fig-width: 10\n#| fig-align: center\n\ngapm %>%\n  ggplot(aes(x = FemaleLiteracyRate,\n             y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n```\n\n## Research and dataset description\n\n::: hl3\n**Research question:** Is there an association between life expectancy and female literacy rates?\n:::\n\n-   Data file: `Gapminder_vars_2011.xlsx`\n\n-   Data were downloaded from\n    [Gapminder](https://www.gapminder.org/data/){.uri}\n    -   2011 is the most recent year with the most complete data\n    -   Observational study measuring different characteristics of countries, including population, health, environment, work, etc.\n\n-   [**Life\n    expectancy**](https://www.gapminder.org/data/documentation/gd004/) =\n    the average number of years a newborn child would live if current\n    mortality patterns were to stay the same.\n\n-   [**Adult literacy rate**](http://data.uis.unesco.org/) is the\n    percentage of people ages 15 and above who can, with understanding,\n    read and write a short, simple statement on their everyday life.\n    \n \n\n- [National Literacy Trust](https://literacytrust.org.uk/research-services/research-reports/literacy-and-life-expectancy/) in England has studied the link between these two variables\n  - Please note that they clearly state that literacy is linked to life expectancy **through many socioeconomic and health factors**\n\n\n## Poll Everywhere Question 1\n\n## Get to know the data (1/3)\n\n-   Load data\n\n```{r}\nlibrary(readxl)\ngapm1 <- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\")\n```\n\n## Get to know the data (2/3)\n\n-   Glimpse of the data\n\n```{r}\nglimpse(gapm1)\n```\n\n-   Note the missing values for our variables of interest\n\n## Get to know the data (3/3)\n\n-   Get a sense of the summary statistics\n\n```{r}\ngapm1 %>% \n  select(LifeExpectancyYrs, \n         FemaleLiteracyRate) %>% \n  summary()\n```\n\n## Remove missing values (1/2)\n\n-   Remove rows with missing data for life expectancy and female\n    literacy rate\n\n```{r}\ngapm <- gapm1 %>% drop_na(LifeExpectancyYrs, FemaleLiteracyRate)\nglimpse(gapm)\n```\n\n-   No missing values now for our variables of interest\n\n## Remove missing values (2/2)\n\n-   And no more missing values when we look only at our two variables of\n    interest\n\n```{r}\ngapm %>%\n  select(LifeExpectancyYrs, \n          FemaleLiteracyRate) %>% \n  get_summary_stats()\n```\n\n::: definition\n::: def-title\nNote\n:::\n\n::: def-cont\n-   Removing the rows with missing data was not needed to run the\n    regression model.\n-   I did this step since later we will be calculating the standard\n    deviations of the explanatory and response variables for *just the\n    values included in the regression model*. It'll be easier to do this\n    if we remove the missing values now.\n:::\n:::\n\n## Poll Everywhere Question 2\n\n# Learning Objectives\n\n::: lob\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n:::\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## Questions we can ask with a simple linear regression model\n\n::: columns\n::: {.column width=\"50%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\ngapm_slr_plot \n\n```\n:::\n\n::: {.column width=\"50%\"}\n-   How do we...\n    -   calculate slope & intercept?\n    -   interpret slope & intercept?\n    -   do inference for slope & intercept?\n        -   CI, p-value\n    -   do prediction with regression line?\n        -   CI for prediction?\n-   Does the model fit the data well?\n    -   Should we be using a line to model the data?\n-   Should we add additional variables to the model?\n    -   multiple/multivariable regression\n:::\n:::\n\n::: heq\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n:::\n\n## Association vs. prediction\n\n::: columns\n::: {.column width=\"50%\"}\n::: proof1\n::: proof-title\nAssociation\n:::\n\n::: proof-cont\n-   What is the association between countries’ life expectancy and\n    female literacy rate?\n-   Use the slope of the line or correlation coefficient\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: definition\n::: def-title\nPrediction\n:::\n\n::: def-cont\n-   What is the expected life expectancy for a country with a\n    specified female literacy rate?    \n:::\n:::\n:::\n:::\n\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n\n```{r}\n#| fig-height: 7\n#| fig-width: 12\n#| echo: false\n#| fig-align: center\n\ngapm_slr_plot \n\n```\n\n## Three types of study design (there are more)\n\n::: columns\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nExperiment\n:::\n\n::: def-cont\n-   Observational units are randomly assigned to important predictor\n    levels\n\n    -   Random assignment controls for confounding variables (age,\n        gender, race, etc.)\n\n    -   “gold standard” for determining causality\n\n    -   Observational unit is often at the participant-level\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: example\n::: ex-title\nQuasi-experiment\n:::\n\n::: ex-cont\n-   Participants are assigned to intervention levels without\n    randomization\n\n-   Not common study design\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: proposition\n::: prop-title\nObservational\n:::\n\n::: prop-cont\n-   No randomization or assignment of intervention conditions\n\n-   In general cannot infer causality\n\n    -   However, there are casual inference methods…\n:::\n:::\n:::\n:::\n\n## Let's revisit the regression analysis process\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n\n## Poll Everywhere Question 3\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n::: lob\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n:::\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## Simple Linear Regression Model\n\nThe (population) regression model is denoted by:\n\n \n\n::: heq\n$$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n:::\n\n \n\n::: columns\n::: column\n#### Observable sample data\n\n-   $Y$ is our dependent variable\n\n    -   Aka outcome or response variable\n\n-   $X$ is our independent variable\n\n    -   Aka predictor, regressor, exposure variable\n:::\n\n::: column\n#### Unobservable population parameters\n\n-   $\\beta_0$ and $\\beta_1$ are **unknown** population parameters\n\n-   $\\epsilon$ (epsilon) is the error about the line\n\n    -   It is assumed to be a random variable with a...\n\n        -   Normal distribution with mean 0 and constant variance\n            $\\sigma^2$\n\n        -   i.e. $\\epsilon \\sim N(0, \\sigma^2)$\n:::\n:::\n\n## Simple Linear Regression Model (another way to view components)\n\nThe (population) regression model is denoted by:\n\n \n\n::: heq\n$$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n:::\n\n \n\n### Components\n\n|            |                                            |\n|------------|--------------------------------------------|\n| $Y$        | response, outcome, dependent variable      |\n| $\\beta_0$  | intercept                                  |\n| $\\beta_1$  | slope                                      |\n| $X$        | predictor, covariate, independent variable |\n| $\\epsilon$ | residuals, error term                      |\n\n## If the population parameters are unobservable, how did we get the line for life expectancy?\n\n::: columns\n::: {.column width=\"40%\"}\n \n\n::: hl\nNote: the **population model is the true, underlying model** that we are\ntrying to estimate using our sample data\n\n-   Our goal in simple linear regression is to estimate $\\beta_0$ and\n    $\\beta_1$\n:::\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n#| fig-align: center\n\ngapm_slr_plot \n\n```\n:::\n:::\n\n## Poll Everywhere Question 4\n\n## Okay, so how do we estimate the regression line? {visibility=\"hidden\"}\n\n \n\nAt this point, we are going to move over to an R shiny app that I made.\n\n \n\nLet's see if we can eyeball the best-fit line!\n\n## Regression line = best-fit line\n\n::: columns\n::: {.column width=\"50%\"}\n$$\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X $$\n\n-   $\\widehat{Y}$ is the predicted outcome for a specific value of $X$\n-   $\\widehat{\\beta}_0$ is the intercept *of the best-fit line*\n-   $\\widehat{\\beta}_1$ is the slope *of the best-fit line*, i.e., the\n    increase in $\\widehat{Y}$ for every increase of one (unit increase)\n    in $X$\n    -   slope = *rise over run*\n:::\n\n::: {.column width=\"50%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\ngapm_slr_plot \n```\n:::\n:::\n\n## Simple Linear Regression Model\n\n::: columns\n::: {.column width=\"50%\"}\n#### Population regression *model*\n\n::: lob\n$$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n:::\n\n \n\n##### Components\n\n|            |                                            |\n|------------|--------------------------------------------|\n| $Y$        | response, outcome, dependent variable      |\n| $\\beta_0$  | intercept                                  |\n| $\\beta_1$  | slope                                      |\n| $X$        | predictor, covariate, independent variable |\n| $\\epsilon$ | residuals, error term                      |\n:::\n\n::: {.column width=\"50%\"}\n#### Estimated regression *line*\n\n::: hl\n$$\\widehat{Y} =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X$$\n:::\n\n \n\n##### Components\n\n|                     |                                                   |\n|---------------------|---------------------------------------------------|\n| $\\widehat{Y}$       | *estimated expected* response given predictor $X$ |\n| $\\widehat{\\beta}_0$ | *estimated* intercept                             |\n| $\\widehat{\\beta}_1$ | *estimated* slope                                 |\n| $X$                 | predictor, covariate, independent variable        |\n:::\n:::\n\n## We get it, Nicky! How do we estimate the regression line?\n\nFirst let's take a break!!\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n::: lob\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n:::\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## It all starts with a residual...\n\n::: columns\n::: {.column width=\"50%\"}\n-   Recall, one characteristic of our population model was that the\n    residuals, $\\epsilon$, were Normally distributed:\n    $\\epsilon \\sim N(0, \\sigma^2)$\n\n-   In our population regression model, we had:\n    $$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n\n-   We can also take the average (expected) value of the population\n    model\n\n-   We take the expected value of both sides and get:\n\n$$\\begin{aligned} \n        E[Y] & = E[\\beta_0 + \\beta_1X + \\epsilon] \\\\\n        E[Y] & = E[\\beta_0] + E[\\beta_1X] + E[\\epsilon] \\\\\n        E[Y] & = \\beta_0 + \\beta_1X + E[\\epsilon] \\\\\n        E[Y|X] & = \\beta_0 + \\beta_1X \\\\\n\\end{aligned}$$\n\n-   We call $E[Y|X]$ the expected value (or average) of $Y$ given $X$\n:::\n\n::: {.column width=\"50%\"}\n![](../img_slides/OLSassumptions-1.png){fig-align=\"center\"}\n:::\n:::\n\n## So now we have two representations of our population model\n\n::: columns\n::: {.column width=\"50%\"}\n::: definition\n::: def-title\nWith observed $Y$ values and residuals:\n:::\n\n::: def-cont\n$$Y =  \\beta_0 + \\beta_1X + \\epsilon$$\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: fact\n::: fact-title\nWith the population expected value of $Y$ given $X$:\n:::\n\n::: fact-cont\n$$E[Y|X] = \\beta_0 + \\beta_1X$$\n:::\n:::\n:::\n:::\n\nUsing the two forms of the model, we can figure out a formula for our\nresiduals:\n\n$$\\begin{aligned}\nY & = (\\beta_0 + \\beta_1X) + \\epsilon \\\\\nY & = E[Y|X] + \\epsilon \\\\\nY - E[Y|X] & = \\epsilon \\\\ \n\\epsilon & = Y - E[Y|X]\n\\end{aligned}$$\n\nAnd so we have our **true, population model**, residuals!\n\n::: hl\nThis is an important fact! For the **population model**, the residuals:\n$\\epsilon = Y - E[Y|X]$\n:::\n\n## Back to our estimated model\n\nWe have the same two representations of our estimated/fitted model:\n\n::: columns\n::: {.column width=\"50%\"}\n::: definition\n::: def-title\nWith observed values:\n:::\n\n::: def-cont\n$$Y =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X + \\widehat{\\epsilon}$$\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: fact\n::: fact-title\nWith the estimated expected value of $Y$ given $X$:\n:::\n\n::: fact-cont\n$$\\begin{aligned} \n\\widehat{E}[Y|X] & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{E[Y|X]} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{Y} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\end{aligned}$$\n:::\n:::\n:::\n:::\n\nUsing the two forms of the model, we can figure out a formula for our\nestimated residuals:\n\n$$\\begin{aligned}\nY & = (\\widehat{\\beta}_0 + \\widehat{\\beta}_1X) + \\widehat\\epsilon \\\\\nY & = \\widehat{Y} + \\widehat\\epsilon \\\\\n\\widehat\\epsilon & = Y - \\widehat{Y}\n\\end{aligned}$$\n\n::: lob\nThis is an important fact! For the **estimated/fitted model**, the\nresiduals: $\\widehat\\epsilon = Y - \\widehat{Y}$\n:::\n\n## *Individual* $i$ residuals in the estimated/fitted model\n\n::: columns\n::: {.column width=\"45%\"}\n-   **Observed values for each country** $i$: $Y_i$\n    -   Value in the dataset for country $i$\n-   **Fitted value for each country** $i$: $\\widehat{Y}_i$\n    -   Value that falls on the best-fit line for a specific $X_i$\n    -   If two individuals have the same $X_i$, then they have the same\n        $\\widehat{Y}_i$\n:::\n\n::: {.column width=\"55%\"}\n```{r}\n#| message: false\n#| echo: false\n#| fig-height: 8\n#| fig-width: 11\n#| fig-align: center\n# code from https://drsimonj.svbtle.com/visualising-residuals\n\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nregression_points <- augment(model1)\n# summary(model1)\n# sum(model1$residuals^2)\n\nggplot(regression_points, \n       aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  # geom_segment(aes(\n  #   xend = female_literacy_rate_2011, \n  #   yend = .fitted), \n  #   alpha = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F14124\", size=3) +\n  # > Color adjustments made here...\n  geom_point(color = \"black\", size = 4) +  # Color mapped here\n  #scale_color_gradient2(low = \"#213c96\", mid = \"white\", high = \"#F14124\") +  # Colors to use here\n    #guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 4) +\nlabs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))  \n```\n:::\n:::\n\n## *Individual* $i$ residuals in the estimated/fitted model\n\n::: columns\n::: {.column width=\"45%\"}\n-   **Observed values for each individual** $i$: $Y_i$\n\n    -   Value in the dataset for individual $i$\n\n-   **Fitted value for each individual** $i$: $\\widehat{Y}_i$\n\n    -   Value that falls on the best-fit line for a specific $X_i$\n    -   If two individuals have the same $X_i$, then they have the same\n        $\\widehat{Y}_i$\n\n::: hl3\n-   **Residual for each individual:**\n    $\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i$\n\n    -   Difference between the observed and fitted value\n:::\n:::\n\n::: {.column width=\"55%\"}\n```{r}\n#| message: false\n#| echo: false\n#| fig-height: 8\n#| fig-width: 11\n#| fig-align: center\n# code from https://drsimonj.svbtle.com/visualising-residuals\n\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nregression_points <- augment(model1)\n# summary(model1)\n# sum(model1$residuals^2)\n\nggplot(regression_points, \n       aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_segment(aes(\n    xend = FemaleLiteracyRate, \n    yend = .fitted), \n    alpha = 1, \n    color = \"#4FADF3\", \n    size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F14124\", size=3) +\n  # > Color adjustments made here...\n  geom_point(color = \"black\", size = 4) +  # Color mapped here\n  #scale_color_gradient2(low = \"#213c96\", mid = \"white\", high = \"#F14124\") +  # Colors to use here\n    #guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 4) +\nlabs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))  \n```\n:::\n:::\n\n## Poll Everywhere Question 5\n\n## So what do we do with the residuals?\n\n-   We want to **minimize the residuals**\n\n    -   Aka minimize the difference between the observed $Y$ value and\n        the estimated expected response given the predictor (\n        $\\widehat{E}[Y|X]$ )\n\n-   **We can use ordinary least squares (OLS) to do this in linear\n    regression!**\n\n-   Idea behind this: reduce the total error between the fitted line and\n    the observed point (error between is called residuals)\n\n    -   Vague use of total error: more precisely, we want to **reduce\n        the sum of squared errors**\n    -   Think back to my R Shiny app!\n    -   We need to mathematically define this!\n\n \n\n \n\n-   Note: there are other ways to estimate the best-fit line!!\n\n    -   Example: Maximum likelihood estimation\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n::: lob\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n:::\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## Setting up for ordinary least squares\n\n::: columns\n::: {.column width=\"75%\"}\n-   Sum of Squared Errors (SSE)\n\n$$ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\end{aligned}$$\n:::\n\n::: {.column width=\"25%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   $\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i$\n\n-   $\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1X_i$\n:::\n:::\n:::\n:::\n\n::: hl\nThen we want to find the estimated coefficient values that minimize the\nSSE!\n:::\n\n## Steps to estimate coefficients using OLS\n\n1.  Set up SSE (previous slide)\n\n2.  Minimize SSE with respect to coefficient estimates\n\n    -   Need to solve a system of equations\n\n3.  Compute derivative of SSE wrt $\\widehat\\beta_0$\n\n4.  Set derivative of SSE wrt $\\widehat\\beta_0 = 0$\n\n5.  Compute derivative of SSE wrt $\\widehat\\beta_1$\n\n6.  Set derivative of SSE wrt $\\widehat\\beta_1 = 0$\n\n7.  Substitute $\\widehat\\beta_1$ back into $\\widehat\\beta_0$\n\n## 2. Minimize SSE with respect to coefficients\n\n-   Want to minimize with respect to (wrt) the potential coefficient\n    estimates ( $\\widehat\\beta_0$ and $\\widehat\\beta_1$)\n\n-   Take derivative of SSE wrt $\\widehat\\beta_0$ and $\\widehat\\beta_1$\n    and set equal to zero to find minimum SSE\n\n$$\n\\dfrac{\\partial SSE}{\\partial \\widehat\\beta_0} = 0 \\text{ and } \\dfrac{\\partial SSE}{\\partial \\widehat\\beta_1} = 0\n$$\n\n-   Solve the above system of equations in steps 3-6\n\n## 3. Compute derivative of SSE wrt $\\widehat\\beta_0$\n\n::: columns\n::: {.column width=\"80%\"}\n$$\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n$$\n\n$$\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0}& =\\frac{\\partial\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)^2}{\\partial{\\widehat{\\beta}}_0}=\n\\sum_{i=1}^{n}\\frac{{\\partial\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}^2}{\\partial{\\widehat{\\beta}}_0} \\\\\n& =\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\\left(-1\\right)}=\\sum_{i=1}^{n}{-2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & = -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\n\\end{aligned}$$\n:::\n\n::: {.column width=\"20%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   Derivative rule: derivative of sum is sum of derivative\n\n-   Derivative rule: chain rule\n:::\n:::\n:::\n:::\n\n## 4. Set derivative of SSE wrt $\\widehat\\beta_0 = 0$\n\n::: columns\n::: {.column width=\"75%\"}\n$$\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & =0 \\\\ -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\\n\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\ \\sum_{i=1}^{n}Y_i-n{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\sum_{i=1}^{n}X_i & =0 \\\\\n\\frac{1}{n}\\sum_{i=1}^{n}Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\frac{1}{n}\\sum_{i=1}^{n}X_i & =0 \\\\ \n\\overline{Y}-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\overline{X} & =0 \\\\\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\n\\end{aligned}$$\n:::\n\n::: {.column width=\"25%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   $\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i$\n-   $\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i$\n:::\n:::\n:::\n:::\n\n## 5. Compute derivative of SSE wrt $\\widehat\\beta_1$\n\n::: columns\n::: {.column width=\"80%\"}\n$$\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n$$\n\n$$\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1}& =\\frac{\\partial\\sum_{i=1}^{n}{(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1}=\\sum_{i=1}^{n}\\frac{{\\partial(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1} \\\\\n&=\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)(-X_i)}=\\sum_{i=1}^{n}{-2X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\ &=-2\\sum_{i=1}^{n}{X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}\n\\end{aligned}$$\n:::\n\n::: {.column width=\"20%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   Derivative rule: derivative of sum is sum of derivative\n\n-   Derivative rule: chain rule\n:::\n:::\n:::\n:::\n\n## 6. Set derivative of SSE wrt $\\widehat\\beta_1 = 0$\n\n::: columns\n::: {.column width=\"70%\"}\n$$\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1} & =0 \\\\ \\sum_{i=1}^{n}\\left({X_iY}_i-{\\widehat{\\beta}}_0X_i-{\\widehat{\\beta}}_1{X_i}^2\\right)&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i{\\widehat{\\beta}}_0}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1}&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\left(\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\right)}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\overline{Y}}+\\sum_{i=1}^{n}{{\\widehat{\\beta}}_1X_i\\overline{X}}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+\\sum_{i=1}^{n}{({\\widehat{\\beta}}_1X_i\\overline{X}}-{X_i}^2{\\widehat{\\beta}}_1) &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+{\\widehat{\\beta}}_1\\sum_{i=1}^{n}{X_i(\\overline{X}}-X_i) &=0 \\\\\n\\end{aligned}$$\n:::\n\n::: {.column width=\"30%\"}\n::: fact\n::: fact-title\nThings to use\n:::\n\n::: fact-cont\n-   ${\\widehat{\\beta}}_0=\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}$\n-   $\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i$\n-   $\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i$\n:::\n:::\n\n         \n\n \n\n::: heq\n$${\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}$$\n:::\n:::\n:::\n\n## 7. Substitute $\\widehat\\beta_1$ back into $\\widehat\\beta_0$\n\n### Final coefficient estimates for SLR\n\n::: columns\n::: {.column width=\"50%\"}\n::: proof1\n::: proof-title\nCoefficient estimate for $\\widehat\\beta_1$\n:::\n\n::: proof-cont\n$${\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}$$\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: fact\n::: fact-title\nCoefficient estimate for $\\widehat\\beta_0$\n:::\n\n::: fact-cont\n$$\\begin{aligned}\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X} \\\\\n{\\widehat{\\beta}}_0 & = \\overline{Y} - \\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})} \\overline{X} \\\\\n\\end{aligned}$$\n:::\n:::\n:::\n:::\n\n## Poll Everywhere Question 6\n\n## Do I need to do all that work every time??\n\n![](../img_slides/hoopla.jpeg){fig-align=\"center\"}\n\n## Regression in R: `lm()`\n\n-   Let's discuss the syntax of this function\n\n```{r}\n#| out-height: 800px \n\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n```\n\n \n\n \n\nIn the general form:\n\n```{r}\n#| eval: false\n\nlm( Y ~ X, data = dataset_name)\ndataset_name %>% lm( formula = Y ~ X )\n```\n\n## Regression in R: `lm()` + `summary()`\n\n```{r}\n#| out-height: 800px \n\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nsummary(model1)\n```\n\n\n## Regression in R: `lm()` + `tidy()`\n\n \n\n```{r}\ntidy(model1) %>% \n  gt() %>% \n  tab_options(table.font.size = 45)\n```\n\n \n\n-   Regression equation for our model (which we saw a looong time ago):\n\n::: heq\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n:::\n\n## How do we interpret the coefficients?\n\n::: heq\n$$\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}$$\n:::\n\n-   **Intercept ($\\hat{\\beta}_0$)**\n    -   The expected outcome for the $Y$-variable when the $X$-variable (if continuous)\n        is 0\n    -   **Example:** The expected/average life expectancy is 50.9 years\n        for a country with 0% female literacy.\n-   **Slope ($\\hat{\\beta}_1$)**\n    -   For every increase of 1 unit in the $X$-variable (if continuous), there is an\n        expected increase of $\\widehat\\beta_1$ units in the\n        $Y$-variable.\n\n    -   We only say that there is an expected increase and not\n        necessarily a causal increase.\n\n    -   **Example:** For every 1 percent increase in the female literacy\n        rate, life expectancy increases, on average, 0.232 years.\n          - Can also say \"...average life expectancy increases 0.232...\"\n\n## Next time\n\n-   More on interpreting the estimate coefficients\n\n-   Inference of our estimated coefficients\n\n-   Inference of estimated expected $Y$ given $X$\n\n-   Prediction\n\n-   Hypothesis testing!\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","highlight-style":"ayu","output-file":"03_SLR.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.25","auto-stretch":true,"title":"Lesson 3: Introduction to Simple Linear Regression (SLR)","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"01/13/2025","categories":["Week 1"],"editor":{"markdown":{"wrap":72}},"theme":"../simple_NW.scss","chalkboard":true,"slideNumber":true,"showSlideNumber":"all","width":1955,"height":1100,"footer":"Lesson 3: SLR 1"}}},"projectFormats":["html"]}