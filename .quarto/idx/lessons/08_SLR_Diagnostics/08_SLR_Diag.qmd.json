{"title":"Lesson 8: SLR: Model Diagnostics","markdown":{"yaml":{"title":"Lesson 8: SLR: Model Diagnostics","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"02/3/2025","format":{"revealjs":{"theme":"../simple_NW.scss","chalkboard":true,"slide-number":true,"show-slide-number":"all","width":1955,"height":1100,"footer":"Lesson 8: SLR 5","html-math-method":"mathjax","highlight-style":"ayu"}},"execute":{"echo":true,"freeze":"auto"},"editor":{"markdown":{"wrap":72}}},"headingText":"Learning Objectives","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)    \nlibrary(openintro)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra)   # grid.arrange()\nlibrary(readxl)\nlibrary(describedata) # gladder()\nlibrary(gridExtra)   # grid.arrange()\nlibrary(ggfortify)  # autoplot(model)\nlibrary(gtsummary)\nlibrary(broom.helpers)\n\nknitr::opts_chunk$set(echo = TRUE, fig.height=3, fig.width=5,\n                      message = FALSE, warning = FALSE)\n```\n\n```{r}\n#| include: false\n#| message: false\n#| warning: false\ngapm1 <- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\") \ngapm <- gapm1 %>% drop_na(LifeExpectancyYrs, FemaleLiteracyRate)\n```\n\n\n\n1.  Use visualizations and cut off points to flag potentially\n    influential points using residuals, leverage, and Cook's distance\n\n2.  Handle influential points and assumption violations by checking data\n    errors, reassessing the model, and making data transformations.\n\n3.  Implement a model with data transformations and determine if it\n    improves the model fit.\n    \n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n\n## Process of regression data analysis\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n## Let's remind ourselves of the model that we have been working with\n\n-   We have been looking at the association between life expectancy and\n    female literacy rate\n\n-   We used OLS to find the coefficient estimates of our best-fit line\n\n::: columns\n::: {.column width=\"55%\"}\n$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$\n\n```{r}\n#| echo: false\n\nmodel1 <- lm(LifeExpectancyYrs ~\n               FemaleLiteracyRate,\n                 data = gapm)\n# Get regression table:\ntidy(model1) %>% gt() %>% \n tab_options(table.font.size = 37) %>%\n fmt_number(decimals = 2)\n```\n\n```{=tex}\n\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\n```\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"43%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n```\n:::\n:::\n\n## Our residuals will help us a lot in our diagnostics!\n\n::: columns\n::: {.column width=\"35%\"}\n \n\n-   The **residuals** $\\widehat\\epsilon_i$ are the vertical distances\n    between\n\n    -   the observed data $(X_i, Y_i)$\n    -   the fitted values (regression line)\n        $\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i$\n:::\n\n::: {.column width=\"65%\"}\n$$\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n$$\n\n```{r}\n#| message: false\n#| echo: false\n#| fig-height: 8\n#| fig-width: 11\n#| fig-align: center\n# code from https://drsimonj.svbtle.com/visualising-residuals\n\nmodel1 <- lm(LifeExpectancyYrs ~ FemaleLiteracyRate,\n                 data = gapm)\nregression_points <- augment(model1)\n# summary(model1)\n# sum(model1$residuals^2)\n\nggplot(regression_points, \n       aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_segment(aes(\n    xend = FemaleLiteracyRate, \n    yend = .fitted), \n    alpha = 1, \n    color = \"#4FADF3\", \n    size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F14124\", size=3) +\n  # > Color adjustments made here...\n  geom_point(color = \"black\", size = 4) +  # Color mapped here\n  #scale_color_gradient2(low = \"#213c96\", mid = \"white\", high = \"#F14124\") +  # Colors to use here\n    #guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 4) +\nlabs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))  \n```\n:::\n:::\n\n## `augment()`: getting extra information on the fitted model\n\n-   Run `model1` through `augment()` (`model1` is input)\n\n    -   So we assigned `model1` as the output of the `lm()` function\n        (`model1` is output)\n\n-   Will give us values about each observation in the context of the\n    fitted regression model\n\n    -   cook's distance (`.cooksd`), fitted value (`.fitted`,\n        $\\widehat{Y}_i$), leverage (`.hat`), residual (`.resid`),\n        standardized residuals (`.std.resid`)\n\n```{r}\naug1 <- augment(model1) \nglimpse(aug1)\n```\n\n[RDocumentation on the `augment()`\nfunction.](https://www.rdocumentation.org/packages/broom/versions/1.0.4/topics/augment.lm)\n\n## Revisiting our LINE assumptions\n\n::: columns\n::: column\n::: definition\n::: def-title\n**\\[L\\] Linearity** of relationship between variables\n:::\n\n::: def-cont\nCheck if there is a linear relationship between the mean response (Y)\nand the explanatory variable (X)\n:::\n:::\n:::\n\n::: column\n::: proof1\n::: proof-title\n**\\[I\\] Independence** of the $Y$ values\n:::\n\n::: proof-cont\nCheck that the observations are independent\n:::\n:::\n:::\n\n::: column\n::: theorem\n::: thm-title\n**\\[N\\] Normality** of the $Y$'s given $X$ (residuals)\n:::\n\n::: thm-cont\nCheck that the responses (at each level X) are normally distributed\n\n-   Usually measured through the residuals\n:::\n:::\n:::\n\n::: column\n::: fact\n::: fact-title\n**\\[E\\] Equality** of variance of the residuals (homoscedasticity)\n:::\n\n::: fact-cont\nCheck that the variance (or standard deviation) of the responses is\nequal for all levels of X\n\n-   Usually measured through the residuals\n:::\n:::\n:::\n:::\n\n# Learning Objectives\n\n::: lob\n1.  Use visualizations and cut off points to flag potentially\n    influential points using residuals, leverage, and Cook's distance\n:::\n\n2.  Handle influential points and assumption violations by checking data\n    errors, reassessing the model, and making data transformations.\n\n3.  Implement a model with data transformations and determine if it\n    improves the model fit.\n\n## Types of influential points\n\n::: columns\n::: column\n::: fact\n::: fact-title\n**Outliers**\n:::\n\n::: fact-cont\n-   An observation ($X_i, Y_i$) whose response $Y_i$ does not follow the\n    general trend of the rest of the data\n    \n![](../img_slides/outliers.png){fig-align=\"center\" width=\"450\"}\n:::\n:::\n\n \n\n \n:::\n\n::: column\n::: definition\n::: def-title\n**High leverage observations**\n:::\n\n::: def-cont\n-   An observation ($X_i, Y_i$) whose predictor $X_i$ has an extreme\n    value\n-   $X_i$ can be an extremely high or low value compared to the rest of\n    the observations\n\n![](../img_slides/high_leverage.png){fig-align=\"center\" width=\"450\"}\n:::\n:::\n:::\n:::\n\n## Tools to measure influential points\n\n- Internally standardized residual (outlier)\n\n \n\n- Leverage (high leverage point)\n\n \n\n- Cook's distance (overall influence, both)\n\n## Poll Everywhere Question 1\n\n## Outliers\n\n::: columns\n::: {.column width=\"60%\"}\n-   An observation ($X_i, Y_i$) whose response $Y_i$ does not follow the\n    general trend of the rest of the data\n    \n \n\n-   How do we determine if a point is an outlier?\n\n    -   Scatterplot of $Y$ vs. $X$\n    -   Followed by evaluation of its residual (and standardized\n        residual)\n          - Typically use the **internally standardized residual** (aka studentized residual)\n:::\n::: {.column width=\"40%\"}\n![](../img_slides/poll_ev_q2.png){fig-align=\"center\" width=\"500\"}\n:::\n:::\n\n## Identifying outliers\n\n::: columns\n::: {.column width=\"37%\"}\n::: theorem\n::: thm-title\nInternally standardized residual\n:::\n\n::: thm-cont\n$$\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n$$\n:::\n:::\n:::\n\n::: {.column width=\"63%\"}\n-   We flag an observation if the standardized residual is \"large\"\n\n    -   Different sources will define \"large\" differently\n\n    -   PennState site uses $|r_i| > 3$\n\n    -   `autoplot()` shows the 3 observations with the highest\n        standardized residuals\n\n    -   Other sources use $|r_i| > 2$, which is a little more\n        conservative\n:::\n:::\n\n::: columns\n::: {.column width=\"55%\"}\n \n\n```{r}\n#| fig-height: 4.5\n#| fig-width: 7\n#| fig-align: center\n#| eval: false\nggplot(data = aug1) + \n  geom_histogram(aes(x = .std.resid))\n```\n:::\n\n::: {.column width=\"45%\"}\n```{r}\n#| fig-height: 5\n#| fig-width: 7.5\n#| fig-align: center\n#| echo: false\n\nggplot(data = aug1) + \n  geom_histogram(aes(x = .std.resid))  +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25))  \n```\n:::\n:::\n\n## Countries that are outliers ($|r_i| > 3$)\n\n-   We can identify the countries that are outliers\n\n```{r}\n#| echo: false\n\n# names(gapm)\n# names(aug1)\n```\n\n```{r}\n#| echo: false\n\naug1 = left_join(aug1, gapm, \n                 by = c(\"LifeExpectancyYrs\", \n                        \"FemaleLiteracyRate\"))\naug1 = aug1 %>%\n  relocate(country, .before = LifeExpectancyYrs) %>%\n  relocate(.std.resid, .after = FemaleLiteracyRate)\n```\n\n```{r}\naug1 %>% \n  filter(abs(.std.resid) > 3)\n```\n\n## Visual: Countries that are outliers ($|r_i| > 3$)\n\nLabel only countries with large internally standardized residuals:\n\n```{r}\n#| fig-align: center\n\nggplot(aug1, aes(x = FemaleLiteracyRate, y = LifeExpectancyYrs,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(abs(.std.resid) > 3, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$FemaleLiteracyRate), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$LifeExpectancyYrs), color = \"grey\")\n```\n\n## What does the model look like without outliers?\n\nSensitivity analysis removing countries that are outliers\n\n```{r}\naug1_no_out <- aug1 %>% filter(abs(.std.resid) <= 3) \n\nmodel1_no_out <- aug1_no_out %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_no_out) %>% gt() %>% # Without outliers\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\ntidy(model1) %>% gt() %>% # With outliers\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n## High leverage observations\n    \n::: columns\n::: {.column width=\"60%\"}\n-   An observation ($X_i, Y_i$) whose response $X_i$ is considered\n    \"extreme\" compared to the other values of $X$\n\n \n\n-   How do we determine if a point has high leverage?\n\n    -   Scatterplot of $Y$ vs. $X$\n    -   Calculating the **leverage** of each observation\n:::\n::: {.column width=\"40%\"}\n![](../img_slides/poll_ev_q2.png){fig-align=\"center\" width=\"500\"}\n:::\n:::\n\n\n## Leverage $h_i$\n\n::: definition\n::: def-title\nLeverage\n:::\n::: def-cont\nMeasure of the distance between the x value ($X_i$) for the data point ($i$) and the mean of the x values ($\\overline{X}$) for all $n$ data points\n:::\n:::\n-   Values of leverage are: $0 \\leq h_i \\leq 1$\n-   We flag an observation if the leverage is \"high\"\n    -   Different sources will define \"high\" differently\n\n    -   Some textbooks use $h_i > 4/n$ where $n$ = sample size\n\n    -   Some people suggest $h_i > 6/n$\n\n    -   PennState site uses $h_i > 3p/n$ where $p$ = number of\n        regression coefficients\n\n## Countries with high leverage ($h_i > 4/n$)\n\n-   We can look at the countries that have high leverage\n\n```{r}\n#| echo: false\n\n# names(gapm)\n# names(aug1)\n#gapm = gapm %>% mutate(.rownames = 1:n() %>% as.character())\n```\n\n```{r}\naug1 = aug1 %>% relocate(.hat, .after = FemaleLiteracyRate)\n\naug1 %>% filter(.hat > 4/80) %>% arrange(desc(.hat))\n```\n\n## Poll Everywhere Question 2\n\n## Visual: Countries with high leverage ($h_i > 4/n$)\n\nLabel only countries with large leverage:\n\n```{r}\n#| fig-align: center\n\nggplot(aug1, aes(x = FemaleLiteracyRate, y = LifeExpectancyYrs,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat > 4/80, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$FemaleLiteracyRate), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$LifeExpectancyYrs), color = \"grey\")\n```\n\n## What does the model look like without the high leverage points?\n\nSensitivity analysis removing countries with high leverage\n\n```{r}\naug1_lowlev <- aug1 %>% filter(.hat <= 4/80)\n\nmodel1_lowlev <- aug1_lowlev %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_lowlev) %>% gt() %>% # Without high-leverage points\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\ntidy(model1) %>% gt() %>% # With high leverage points\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n## Cook's distance\n\n-   Measures the overall influence of an observation\n\n \n\n-   Attempts to measure how much influence a single observation has over\n    the fitted model\n\n    -   Measures how all fitted values change when the $ith$ observation\n        is removed from the model\n\n    -   Combines leverage and outlier information\n\n## Identifying points with high Cook's distance\n\n::: columns\n::: column\nThe Cook's distance for the $i^{th}$ observation is\n\n$$d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2$$ where $h_i$ is the leverage\nand $r_i$ is the studentized residual\n:::\n\n::: column\n-   Another rule for Cook's distance that is not strict:\n    -   Investigate observations that have $d_i > 1$\n-   Cook's distance values are already in the augment tibble: `.cooksd`\n:::\n:::\n\n```{r}\naug1 = aug1 %>% relocate(.cooksd, .after = FemaleLiteracyRate)\naug1 %>% arrange(desc(.cooksd))\n```\n\n## Plotting Cook's Distance\n\n- `plot(model)` shows figures similar to `autoplot()`\n  - 4th plot is Cook's distance (not available in `autoplot()`)\n```{r fig.height=4}\n#| fig-align: center\n\nplot(model1, which = 4)\n```\n\n## What does the model look like without the high Cook's distance points?\n\nSensitivity analysis removing countries with high Cook's distance\n\n```{r}\naug1_lowcd <- aug1 %>% filter(.cooksd <= 0.04)\nmodel1_lowcd <- aug1_lowcd %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_lowcd) %>% gt() %>% # Without high Cook's distance points\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\ntidy(model1) %>% gt() %>% # With high Cook's distance points\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n## Model without those 4 points: QQ Plot, Residual plot {visibility=\"hidden\"}\n\n::: columns\n::: column\n```{r}\n#| fig.width: 7.5\n#| fig.height: 7\n#| fig.align: center\n#| echo: false\n\nggplot(aug1_lowcd, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line() + # line\n  labs(title = \"QQ plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30), \n        title = element_text(size = 30)) +\n  labs(x = \"Theoretical quantiles\", \n       y = \"Data quantiles\")\n```\n:::\n\n::: column\n```{r}\n#| fig.width: 7.5\n#| fig.height: 7\n#| fig.align: center\n#| echo: false\nggplot(aug1_lowcd, \n       aes(x = FemaleLiteracyRate, \n           y = .resid)) + \n  geom_point(size = 2) +\n  geom_abline( intercept = 0, slope = 0,\n    size = 2, color = \"#FF8021\") +\n  labs(title = \"Residual plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30), \n        title = element_text(size = 30)) \n```\n:::\n:::\n\nI am okay with this!\n\n-   And don't forget: we may want more variables in our model!\n\n-   You do not need to produce plots with the influential points taken\n    out\n\n## Summary of how we identify influential points\n\n-   Use scatterplot of $Y$ vs. $X$ to see if any points fall outside of\n    range we expect\n\n-   Use standardized residuals, leverage, and Cook's distance to further\n    identify those points\n\n-   Look at the models run with and without the identified points to\n    check for drastic changes\n\n    -   Look at QQ plot and residuals to see if assumptions hold without\n        those points\n\n    -   Look at coefficient estimates to see if they change in sign and\n        large magnitude\n\n \n\n-   Next: how to handle? *It's a little wishy washy*\n\n# Learning Objectives\n\n1.  Use visualizations and cut off points to flag potentially\n    influential points using residuals, leverage, and Cook's distance\n\n::: lob\n2.  Handle influential points and assumption violations by checking data\n    errors, reassessing the model, and making data transformations.\n:::\n\n3.  Implement a model with data transformations and determine if it\n    improves the model fit.\n\n## How do we deal with influential points?\n    \n-   If an observation is influential, **we perform a sensitivity analysis**:\n    - We took out the influential points we identified then reran the model\n    - Often, you'll see that the \"influential points\" have not drastically changed your estimates\n        - A change in sign (for example: positive slope to negative slope)\n        - A really large increase (think more than 2x the original value)\n\n-   If an observation is influential, **we check data errors**:\n\n    -   Was there a data entry or collection problem?\n\n    -   If you have reason to believe that the observation does not hold\n        within the population (or gives you cause to redefine your\n        population)\n\n-   If an observation is influential, **we check our model**:\n\n    -   Did you leave out any important predictors?\n\n    -   Should you consider adding some interaction terms?\n\n    -   Is there any nonlinearity that needs to be modeled?\n    \n## Important note on influential observations\n\n-   It's always weird to be using numbers to help you diagnose an issue,\n    but the issue kinda gets unresolved\n\n \n\n-   Basically, deleting an observation should be justified outside of\n    the numbers!\n\n    -   If it's an honest data point, then it's giving us important\n        information!\n        \n \n\n-   [A really well thought out explanation from\n    StackExchange](https://stats.stackexchange.com/questions/81058/how-to-handle-leverage-values)\n    \n    \n\n## Checking our model\n\n-   An observation **may be** influential if the model is not correctly specified\n    -   We may also see issues with the LINE assumptions\n\n-   What are our options to specify the model \"correctly?\"\n\n    -   See if we need to add predictors to our model\n\n        -   Nicky's thought for our life expectancy example\n\n    -   Try a transformation if there is an issue with linearity or\n    normality\n\n    -   Try a transformation if there is unequal variance\n\n    -   Try a weighted least squares approach if unequal variance (might be\n    lesson at end of course)\n\n    -   Try a robust estimation procedure if we have a lot of outlier issues\n    (outside scope of class)\n\n# Learning Objectives\n\n1.  Use visualizations and cut off points to flag potentially\n    influential points using residuals, leverage, and Cook's distance\n\n2.  Handle influential points and assumption violations by checking data\n    errors, reassessing the model, and making data transformations.\n\n::: lob\n3.  Implement a model with data transformations and determine if it\n    improves the model fit.\n:::\n\n## Transformations\n\n-   When we have issues with our LINE (mostly linearity, normality, or\n    equality of variance) assumptions\n\n    -   We can use transformations to improve the fit of the model\n\n-   Transformations can...\n\n    -   Make the relationship more linear\n\n    -   Make the residuals more normal\n\n    -   \"Stabilize\" the variance so that it is more constant\n\n    -   It can also bring in or reduce outliers\n\n-   We can transform the dependent ($Y$) variable and/or the independent\n    ($X$) variable\n\n    -   Usually we want to try transforming the $X$ first\n\n \n\n-   **Requires trial and error!!**\n-   **Major drawback:** interpreting the model becomes harder!\n\n## Common transformations\n\n-   Tukey's transformation (power) ladder\n\n    -   Use `R`'s `gladder()` command from the `describedata` package\n\n| Power p | -3              | -2              | -1            | -1/2                 | 0         | 1/2        | 1   | 2     | 3     |\n|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n|         | $\\frac{1}{x^3}$ | $\\frac{1}{x^2}$ | $\\frac{1}{x}$ | $\\frac{1}{\\sqrt{x}}$ | $\\log(x)$ | $\\sqrt{x}$ | $x$ | $x^2$ | $x^3$ |\n\n::: columns\n::: {.column width=\"50%\"}\n-   How to use the power ladder for the general distribution shape\n\n    -   If data are skewed left, we need to compress smaller values\n        towards the rest of the data\n\n        -   Go \"up\" ladder to transformations with power \\> 1\n\n    -   If data are skewed right, we need to compress larger values\n        towards the rest of the data\n\n        -   Go \"down\" ladder to transformations with power \\< 1\n:::\n\n::: {.column width=\"50%\"}\n-   How to use the power ladder for heteroscedasticity\n\n    -   If higher $X$ values have more spread\n\n        -   Compress larger values towards the rest of the data\n\n        -   Go \"down\" ladder to transformations with power \\< 1\n\n    -   If lower $X$ values have more spread\n\n        -   Compress smaller values towards the rest of the data\n\n        -   Go \"up\" ladder to transformations with power \\> 1\n:::\n:::\n\n## Poll Everywhere Question 3\n\n\n## Transform independent variable?\n\n::: columns\n::: column\n```{r}\n#| out-width: 100%\n\nggplot(gapm, \n       aes(x = FemaleLiteracyRate)) +\n  geom_histogram()\n```\n:::\n::: column\n\n- Looks like more spread on the left side\n- Use powers greater than 1 \n  - $FLR^2$ and $FLR^3$\n:::\n:::\n\n## `gladder()` of female literacy rate\n\n```{r fig.width=7, fig.height=5}\n#| fig-align: center\ngladder(gapm$FemaleLiteracyRate)\n```\n\n## `ladder()` of female literacy rate {visibility=\"hidden\"}\n\n::: columns\n::: {.column width=\"40%\"}\n-   `ladder()` output tests various transformations of the data for\n    normality\n-   Shapiro-Wilkes test is used to assess for normality\n    -   $H_0$: data are from a normal population\n    -   $H_A$: data are NOT from a normal population\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| fig-align: center\nladder(gapm$FemaleLiteracyRate) %>% \n  gt() %>%\n  tab_options(table.font.size = 40) %>%\n  fmt_number(decimals = 3)\n```\n:::\n:::\n\n## Transform dependent variable?\n\n::: columns\n::: column\n\n```{r}\n#| out-width: 100%\n\nggplot(gapm, \n       aes(x = LifeExpectancyYrs)) +\n  geom_histogram()\n```\n:::\n::: column\n\n- Looks like more spread on the left side as well\n- Use powers greater than 1 \n  - $LE^2$ and $LE^3$\n  \n:::\n:::\n\n## `gladder()` of life expectancy\n\n```{r fig.width=7, fig.height=5}\n#| fig-align: center\ngladder(gapm$LifeExpectancyYrs)\n```\n\n## `ladder()` of life expectancy {visibility=\"hidden\"}\n\n::: columns\n::: {.column width=\"40%\"}\n-   `ladder()` output tests various transformations of the data for\n    normality\n-   Shapiro-Wilkes test is used to assess for normality\n    -   $H_0$: data are from a normal population\n    -   $H_A$: data are NOT from a normal population\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| fig-align: center\nladder(gapm$LifeExpectancyYrs) %>% \n  gt() %>%\n  tab_options(table.font.size = 40) %>%\n  fmt_number(decimals = 3)\n```\n:::\n:::\n\n## Tips\n\n-   Recall, assessing our LINE assumptions are not on $Y$ alone!! (it's $Y|X$)\n\n    -   We can use `gladder()` to get a sense of what our\n        transformations will do to the data, but we need to check with\n        our residuals again!!\n\n-   Transformations usually work better if **all values** are positive (or\n    negative)\n\n-   If observation has a 0, then we cannot perform certain\n    transformations\n\n-   Log function only defined for positive values\n\n    -   We might take the $log(X+1)$ if $X$ includes a 0 value\n\n-   When we make cubic or square transformations, we MUST include the\n    original $X$ in the model\n\n    -   We do not do this for $Y$ though\n\n## Add quadratic and cubic transformations to dataset\n\n-   Helpful to make a new variable with the transformation in your\n    dataset\n\n```{r}\ngapm <- gapm %>% \n  mutate(LE_2 = LifeExpectancyYrs^2,\n         LE_3 = LifeExpectancyYrs^3,\n         FLR_2 = FemaleLiteracyRate^2,\n         FLR_3 = FemaleLiteracyRate^3)\n\ncolnames(gapm)\n```\n\n## We are going to compare a few different models with transformations\n\nWe are going to call life expectancy $LE$ and female literacy rate $FLR$\n\n-   Model 1: $LE = \\beta_0 + \\beta_1 FLR + \\epsilon$\n-   Model 2: $LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon$\n-   Model 3: $LE^3 = \\beta_0 + \\beta_1 FLR + \\epsilon$\n-   Model 4: $LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\epsilon$\n-   Model 5:\n    $LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon$\n-   Model 6:\n    $LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon$\n\n## Poll Everywhere Question 4\n\n## Compare Scatterplots: does linearity improve?\n\n```{r fig.width=10, fig.height=5}\n#| echo: false\n\nplot_m1 <- ggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod1: LE ~ FLR\")\n\nplot_m2 <- ggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LE_2)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod2: LE^2 ~ FLR\")\n\nplot_m3 <- ggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LE_3)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod3: LE^3 ~ FLR\")\n\nplot_m4 <- ggplot(gapm, aes(x = FLR_2,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod4: LE ~ FLR + FLR^2\")\n\nplot_m5 <- ggplot(gapm, aes(x = FLR_3,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod5: LE ~ FLR + FLR^2 + FLR^3\")\n\nplot_m6 <- ggplot(gapm, aes(x = FLR_3,\n                 y = LE_3)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod6: LE^3 ~ FLR + FLR^2 + FLR^3\")\n\ngrid.arrange(plot_m1, plot_m2, plot_m3, \n             plot_m4, plot_m5, plot_m6,\n             nrow = 2)\n\n```\n\n\n## Run models with transformations: examples\n\n**Model 2:** $LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon$\n\n```{r}\nmodel2 <- lm(LE_2 ~ FemaleLiteracyRate,\n             data = gapm)\n```\n\n```{r}\n#| echo: false\ntidy(model2) %>% gt() %>%\n  tab_options(table.font.size = 40) %>%\n  fmt_number(decimals = 3)\n```\n\n**Model 6:**\n$LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon$\n\n```{r}\nmodel6 <- lm(LE_3 ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\n```\n\n```{r}\n#| echo: false\ntidy(model6) %>% gt() %>%\n  tab_options(table.font.size = 40) %>%\n  fmt_number(decimals = 3)\n```\n\n```{r}\n#| echo: false\naug2 <- augment(model2)\naug6 <- augment(model6)\nmodel3 <- lm(LE_3 ~ FemaleLiteracyRate,\n             data = gapm)\n\naug3 <- augment(model3)\n```\n\n```{r}\n#| echo: false\nmodel4 <- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2,\n             data = gapm)\n\naug4 <- augment(model4)\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model4)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model4, which = 5)\n```\n\n```{r}\n#| echo: false\nmodel5 <- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\n\naug5 <- augment(model5)\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model5)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model5, which = 5)\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model6)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model6, which = 5)\n```\n\n## Normal Q-Q plots comparison\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| fig-align: center\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 2)\nplot(model2, which = 2)\nplot(model3, which = 2)\nplot(model4, which = 2)\nplot(model5, which = 2)\nplot(model6, which = 2)\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column\n```\n\n## Residual plots comparison\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| fig-align: center\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 1)\nplot(model2, which = 1)\nplot(model3, which = 1)\nplot(model4, which = 1)\nplot(model5, which = 1)\nplot(model6, which = 1)\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column\n```\n\n## Summary of transformations\n\n-   If the model without the transformation is **blatantly violating a\n    LINE assumption**\n    \n    -   Then a transformation is a good idea\n    -   If transformations do not help, then keep it untransformed\n\n \n\n-   If the model without a transformation is **not following the LINE assumptions very well, but is mostly okay**\n\n    -   Then try to avoid a transformation\n    -   Think about what predictors might need to be added\n    -   Especially if you keep seeing the same points as influential\n    \n \n\n-   If **interpretability** is important in your final work, then **transformations are not a great solution**\n\n## Models comparison {visibility=\"hidden\"}\n\n```{r}\n# library(gtsummary) for tbl_regression() and tbl_merge()\n\ntbl_model1 <- tbl_regression(model1)\n\ntbl_model2 <- tbl_regression(model2)\n\ntbl_model3 <- tbl_regression(model3)\n\ntbl_model4 <- tbl_regression(model4)\n\ntbl_model5 <- tbl_regression(model5)\n\ntbl_model6 <- tbl_regression(model6)\n\n# Compare models 1-3\ntbl_merge(\n  tbls = list(tbl_model1, tbl_model2, tbl_model3),\n  tab_spanner = c(\"Model 1: y=LE\", \"Model 2: y=LE^2\", \"Model 3: y=LE^3\")\n  )\n\n# Compare models 4-6\ntbl_merge(\n  tbls = list(tbl_model4, tbl_model5, tbl_model6),\n  tab_spanner = c(\"Model 4: y=LE\", \"Model 5: y=LE\", \"Model 6: y=LE^3\")\n  )\n```\n\n## Other fit statistics comparison {visibility=\"hidden\"}\n\n```{r}\nglance(model1) %>% gt()\nglance(model2) %>% gt()\nglance(model3) %>% gt()\nglance(model4) %>% gt()\nglance(model5) %>% gt()\nglance(model6) %>% gt()\n```\n\n## Example: Chapter 5 Problem 9 {visibility=\"hidden\"}\n\n-   In an experiment designed to describe the dose–response curve for\n    vitamin K, individual rats were depleted of their vitamin K reserves\n    and then fed dried liver for 4 days at different dosage levels.\n-   The response of each rat was measured as the concentration of a\n    clotting agent needed to clot a sample of its blood in 3 minutes.\n-   The results of the experiment on 12 rats are given in the following\n    table; **values are expressed in common logarithms for both dose and\n    response**.\n    -   *Note: by \"common logarithm\" the authors mean a base 10\n        logarithm*\n\n> Question: why did they choose a log-log transformation?\n\n```{r}\nrats <- read_excel(\"data/CH05Q09.xls\")\nglimpse(rats)\n\nloglog_plot <- ggplot(rats, aes(x = LOGDOSE, y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Transformed variables\")\nloglog_plot\n```\n\n## Reference: all run models {.smaller}\n\n::: columns\n::: column\nModel 2: $LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon$\n\n```{r}\nmodel2 <- lm(LE_2 ~ FemaleLiteracyRate,\n             data = gapm)\ntidy(model2) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model2)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model2, which = 5)\n```\n\nModel 3: $LE^3 \\sim FLR$\n\n```{r}\nmodel3 <- lm(LE_3 ~ FemaleLiteracyRate,\n             data = gapm)\ntidy(model3) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model3)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model3, which = 5)\n```\n\nModel 4: $LE \\sim FLR + FLR^2$\n\n```{r}\nmodel4 <- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2,\n             data = gapm)\ntidy(model4) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model4)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model4, which = 5)\n```\n:::\n\n::: column\nModel 5: $LE \\sim FLR + FLR^2 + FLR^3$\n\n```{r}\nmodel5 <- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\ntidy(model5) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model5)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model5, which = 5)\n```\n\nModel 6: $LE^3 \\sim FLR + FLR^2 + FLR^3$\n\n```{r}\nmodel6 <- lm(LE_3 ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\ntidy(model6) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model6)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model6, which = 5)\n```\n:::\n:::\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)    \nlibrary(openintro)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra)   # grid.arrange()\nlibrary(readxl)\nlibrary(describedata) # gladder()\nlibrary(gridExtra)   # grid.arrange()\nlibrary(ggfortify)  # autoplot(model)\nlibrary(gtsummary)\nlibrary(broom.helpers)\n\nknitr::opts_chunk$set(echo = TRUE, fig.height=3, fig.width=5,\n                      message = FALSE, warning = FALSE)\n```\n\n```{r}\n#| include: false\n#| message: false\n#| warning: false\ngapm1 <- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\") \ngapm <- gapm1 %>% drop_na(LifeExpectancyYrs, FemaleLiteracyRate)\n```\n\n\n# Learning Objectives\n\n1.  Use visualizations and cut off points to flag potentially\n    influential points using residuals, leverage, and Cook's distance\n\n2.  Handle influential points and assumption violations by checking data\n    errors, reassessing the model, and making data transformations.\n\n3.  Implement a model with data transformations and determine if it\n    improves the model fit.\n    \n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n\n## Process of regression data analysis\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n## Let's remind ourselves of the model that we have been working with\n\n-   We have been looking at the association between life expectancy and\n    female literacy rate\n\n-   We used OLS to find the coefficient estimates of our best-fit line\n\n::: columns\n::: {.column width=\"55%\"}\n$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$\n\n```{r}\n#| echo: false\n\nmodel1 <- lm(LifeExpectancyYrs ~\n               FemaleLiteracyRate,\n                 data = gapm)\n# Get regression table:\ntidy(model1) %>% gt() %>% \n tab_options(table.font.size = 37) %>%\n fmt_number(decimals = 2)\n```\n\n```{=tex}\n\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\n```\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"43%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n```\n:::\n:::\n\n## Our residuals will help us a lot in our diagnostics!\n\n::: columns\n::: {.column width=\"35%\"}\n \n\n-   The **residuals** $\\widehat\\epsilon_i$ are the vertical distances\n    between\n\n    -   the observed data $(X_i, Y_i)$\n    -   the fitted values (regression line)\n        $\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i$\n:::\n\n::: {.column width=\"65%\"}\n$$\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n$$\n\n```{r}\n#| message: false\n#| echo: false\n#| fig-height: 8\n#| fig-width: 11\n#| fig-align: center\n# code from https://drsimonj.svbtle.com/visualising-residuals\n\nmodel1 <- lm(LifeExpectancyYrs ~ FemaleLiteracyRate,\n                 data = gapm)\nregression_points <- augment(model1)\n# summary(model1)\n# sum(model1$residuals^2)\n\nggplot(regression_points, \n       aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_segment(aes(\n    xend = FemaleLiteracyRate, \n    yend = .fitted), \n    alpha = 1, \n    color = \"#4FADF3\", \n    size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F14124\", size=3) +\n  # > Color adjustments made here...\n  geom_point(color = \"black\", size = 4) +  # Color mapped here\n  #scale_color_gradient2(low = \"#213c96\", mid = \"white\", high = \"#F14124\") +  # Colors to use here\n    #guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 4) +\nlabs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))  \n```\n:::\n:::\n\n## `augment()`: getting extra information on the fitted model\n\n-   Run `model1` through `augment()` (`model1` is input)\n\n    -   So we assigned `model1` as the output of the `lm()` function\n        (`model1` is output)\n\n-   Will give us values about each observation in the context of the\n    fitted regression model\n\n    -   cook's distance (`.cooksd`), fitted value (`.fitted`,\n        $\\widehat{Y}_i$), leverage (`.hat`), residual (`.resid`),\n        standardized residuals (`.std.resid`)\n\n```{r}\naug1 <- augment(model1) \nglimpse(aug1)\n```\n\n[RDocumentation on the `augment()`\nfunction.](https://www.rdocumentation.org/packages/broom/versions/1.0.4/topics/augment.lm)\n\n## Revisiting our LINE assumptions\n\n::: columns\n::: column\n::: definition\n::: def-title\n**\\[L\\] Linearity** of relationship between variables\n:::\n\n::: def-cont\nCheck if there is a linear relationship between the mean response (Y)\nand the explanatory variable (X)\n:::\n:::\n:::\n\n::: column\n::: proof1\n::: proof-title\n**\\[I\\] Independence** of the $Y$ values\n:::\n\n::: proof-cont\nCheck that the observations are independent\n:::\n:::\n:::\n\n::: column\n::: theorem\n::: thm-title\n**\\[N\\] Normality** of the $Y$'s given $X$ (residuals)\n:::\n\n::: thm-cont\nCheck that the responses (at each level X) are normally distributed\n\n-   Usually measured through the residuals\n:::\n:::\n:::\n\n::: column\n::: fact\n::: fact-title\n**\\[E\\] Equality** of variance of the residuals (homoscedasticity)\n:::\n\n::: fact-cont\nCheck that the variance (or standard deviation) of the responses is\nequal for all levels of X\n\n-   Usually measured through the residuals\n:::\n:::\n:::\n:::\n\n# Learning Objectives\n\n::: lob\n1.  Use visualizations and cut off points to flag potentially\n    influential points using residuals, leverage, and Cook's distance\n:::\n\n2.  Handle influential points and assumption violations by checking data\n    errors, reassessing the model, and making data transformations.\n\n3.  Implement a model with data transformations and determine if it\n    improves the model fit.\n\n## Types of influential points\n\n::: columns\n::: column\n::: fact\n::: fact-title\n**Outliers**\n:::\n\n::: fact-cont\n-   An observation ($X_i, Y_i$) whose response $Y_i$ does not follow the\n    general trend of the rest of the data\n    \n![](../img_slides/outliers.png){fig-align=\"center\" width=\"450\"}\n:::\n:::\n\n \n\n \n:::\n\n::: column\n::: definition\n::: def-title\n**High leverage observations**\n:::\n\n::: def-cont\n-   An observation ($X_i, Y_i$) whose predictor $X_i$ has an extreme\n    value\n-   $X_i$ can be an extremely high or low value compared to the rest of\n    the observations\n\n![](../img_slides/high_leverage.png){fig-align=\"center\" width=\"450\"}\n:::\n:::\n:::\n:::\n\n## Tools to measure influential points\n\n- Internally standardized residual (outlier)\n\n \n\n- Leverage (high leverage point)\n\n \n\n- Cook's distance (overall influence, both)\n\n## Poll Everywhere Question 1\n\n## Outliers\n\n::: columns\n::: {.column width=\"60%\"}\n-   An observation ($X_i, Y_i$) whose response $Y_i$ does not follow the\n    general trend of the rest of the data\n    \n \n\n-   How do we determine if a point is an outlier?\n\n    -   Scatterplot of $Y$ vs. $X$\n    -   Followed by evaluation of its residual (and standardized\n        residual)\n          - Typically use the **internally standardized residual** (aka studentized residual)\n:::\n::: {.column width=\"40%\"}\n![](../img_slides/poll_ev_q2.png){fig-align=\"center\" width=\"500\"}\n:::\n:::\n\n## Identifying outliers\n\n::: columns\n::: {.column width=\"37%\"}\n::: theorem\n::: thm-title\nInternally standardized residual\n:::\n\n::: thm-cont\n$$\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n$$\n:::\n:::\n:::\n\n::: {.column width=\"63%\"}\n-   We flag an observation if the standardized residual is \"large\"\n\n    -   Different sources will define \"large\" differently\n\n    -   PennState site uses $|r_i| > 3$\n\n    -   `autoplot()` shows the 3 observations with the highest\n        standardized residuals\n\n    -   Other sources use $|r_i| > 2$, which is a little more\n        conservative\n:::\n:::\n\n::: columns\n::: {.column width=\"55%\"}\n \n\n```{r}\n#| fig-height: 4.5\n#| fig-width: 7\n#| fig-align: center\n#| eval: false\nggplot(data = aug1) + \n  geom_histogram(aes(x = .std.resid))\n```\n:::\n\n::: {.column width=\"45%\"}\n```{r}\n#| fig-height: 5\n#| fig-width: 7.5\n#| fig-align: center\n#| echo: false\n\nggplot(data = aug1) + \n  geom_histogram(aes(x = .std.resid))  +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25))  \n```\n:::\n:::\n\n## Countries that are outliers ($|r_i| > 3$)\n\n-   We can identify the countries that are outliers\n\n```{r}\n#| echo: false\n\n# names(gapm)\n# names(aug1)\n```\n\n```{r}\n#| echo: false\n\naug1 = left_join(aug1, gapm, \n                 by = c(\"LifeExpectancyYrs\", \n                        \"FemaleLiteracyRate\"))\naug1 = aug1 %>%\n  relocate(country, .before = LifeExpectancyYrs) %>%\n  relocate(.std.resid, .after = FemaleLiteracyRate)\n```\n\n```{r}\naug1 %>% \n  filter(abs(.std.resid) > 3)\n```\n\n## Visual: Countries that are outliers ($|r_i| > 3$)\n\nLabel only countries with large internally standardized residuals:\n\n```{r}\n#| fig-align: center\n\nggplot(aug1, aes(x = FemaleLiteracyRate, y = LifeExpectancyYrs,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(abs(.std.resid) > 3, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$FemaleLiteracyRate), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$LifeExpectancyYrs), color = \"grey\")\n```\n\n## What does the model look like without outliers?\n\nSensitivity analysis removing countries that are outliers\n\n```{r}\naug1_no_out <- aug1 %>% filter(abs(.std.resid) <= 3) \n\nmodel1_no_out <- aug1_no_out %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_no_out) %>% gt() %>% # Without outliers\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\ntidy(model1) %>% gt() %>% # With outliers\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n## High leverage observations\n    \n::: columns\n::: {.column width=\"60%\"}\n-   An observation ($X_i, Y_i$) whose response $X_i$ is considered\n    \"extreme\" compared to the other values of $X$\n\n \n\n-   How do we determine if a point has high leverage?\n\n    -   Scatterplot of $Y$ vs. $X$\n    -   Calculating the **leverage** of each observation\n:::\n::: {.column width=\"40%\"}\n![](../img_slides/poll_ev_q2.png){fig-align=\"center\" width=\"500\"}\n:::\n:::\n\n\n## Leverage $h_i$\n\n::: definition\n::: def-title\nLeverage\n:::\n::: def-cont\nMeasure of the distance between the x value ($X_i$) for the data point ($i$) and the mean of the x values ($\\overline{X}$) for all $n$ data points\n:::\n:::\n-   Values of leverage are: $0 \\leq h_i \\leq 1$\n-   We flag an observation if the leverage is \"high\"\n    -   Different sources will define \"high\" differently\n\n    -   Some textbooks use $h_i > 4/n$ where $n$ = sample size\n\n    -   Some people suggest $h_i > 6/n$\n\n    -   PennState site uses $h_i > 3p/n$ where $p$ = number of\n        regression coefficients\n\n## Countries with high leverage ($h_i > 4/n$)\n\n-   We can look at the countries that have high leverage\n\n```{r}\n#| echo: false\n\n# names(gapm)\n# names(aug1)\n#gapm = gapm %>% mutate(.rownames = 1:n() %>% as.character())\n```\n\n```{r}\naug1 = aug1 %>% relocate(.hat, .after = FemaleLiteracyRate)\n\naug1 %>% filter(.hat > 4/80) %>% arrange(desc(.hat))\n```\n\n## Poll Everywhere Question 2\n\n## Visual: Countries with high leverage ($h_i > 4/n$)\n\nLabel only countries with large leverage:\n\n```{r}\n#| fig-align: center\n\nggplot(aug1, aes(x = FemaleLiteracyRate, y = LifeExpectancyYrs,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat > 4/80, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$FemaleLiteracyRate), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$LifeExpectancyYrs), color = \"grey\")\n```\n\n## What does the model look like without the high leverage points?\n\nSensitivity analysis removing countries with high leverage\n\n```{r}\naug1_lowlev <- aug1 %>% filter(.hat <= 4/80)\n\nmodel1_lowlev <- aug1_lowlev %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_lowlev) %>% gt() %>% # Without high-leverage points\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\ntidy(model1) %>% gt() %>% # With high leverage points\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n## Cook's distance\n\n-   Measures the overall influence of an observation\n\n \n\n-   Attempts to measure how much influence a single observation has over\n    the fitted model\n\n    -   Measures how all fitted values change when the $ith$ observation\n        is removed from the model\n\n    -   Combines leverage and outlier information\n\n## Identifying points with high Cook's distance\n\n::: columns\n::: column\nThe Cook's distance for the $i^{th}$ observation is\n\n$$d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2$$ where $h_i$ is the leverage\nand $r_i$ is the studentized residual\n:::\n\n::: column\n-   Another rule for Cook's distance that is not strict:\n    -   Investigate observations that have $d_i > 1$\n-   Cook's distance values are already in the augment tibble: `.cooksd`\n:::\n:::\n\n```{r}\naug1 = aug1 %>% relocate(.cooksd, .after = FemaleLiteracyRate)\naug1 %>% arrange(desc(.cooksd))\n```\n\n## Plotting Cook's Distance\n\n- `plot(model)` shows figures similar to `autoplot()`\n  - 4th plot is Cook's distance (not available in `autoplot()`)\n```{r fig.height=4}\n#| fig-align: center\n\nplot(model1, which = 4)\n```\n\n## What does the model look like without the high Cook's distance points?\n\nSensitivity analysis removing countries with high Cook's distance\n\n```{r}\naug1_lowcd <- aug1 %>% filter(.cooksd <= 0.04)\nmodel1_lowcd <- aug1_lowcd %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_lowcd) %>% gt() %>% # Without high Cook's distance points\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\ntidy(model1) %>% gt() %>% # With high Cook's distance points\n tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n## Model without those 4 points: QQ Plot, Residual plot {visibility=\"hidden\"}\n\n::: columns\n::: column\n```{r}\n#| fig.width: 7.5\n#| fig.height: 7\n#| fig.align: center\n#| echo: false\n\nggplot(aug1_lowcd, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line() + # line\n  labs(title = \"QQ plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30), \n        title = element_text(size = 30)) +\n  labs(x = \"Theoretical quantiles\", \n       y = \"Data quantiles\")\n```\n:::\n\n::: column\n```{r}\n#| fig.width: 7.5\n#| fig.height: 7\n#| fig.align: center\n#| echo: false\nggplot(aug1_lowcd, \n       aes(x = FemaleLiteracyRate, \n           y = .resid)) + \n  geom_point(size = 2) +\n  geom_abline( intercept = 0, slope = 0,\n    size = 2, color = \"#FF8021\") +\n  labs(title = \"Residual plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30), \n        title = element_text(size = 30)) \n```\n:::\n:::\n\nI am okay with this!\n\n-   And don't forget: we may want more variables in our model!\n\n-   You do not need to produce plots with the influential points taken\n    out\n\n## Summary of how we identify influential points\n\n-   Use scatterplot of $Y$ vs. $X$ to see if any points fall outside of\n    range we expect\n\n-   Use standardized residuals, leverage, and Cook's distance to further\n    identify those points\n\n-   Look at the models run with and without the identified points to\n    check for drastic changes\n\n    -   Look at QQ plot and residuals to see if assumptions hold without\n        those points\n\n    -   Look at coefficient estimates to see if they change in sign and\n        large magnitude\n\n \n\n-   Next: how to handle? *It's a little wishy washy*\n\n# Learning Objectives\n\n1.  Use visualizations and cut off points to flag potentially\n    influential points using residuals, leverage, and Cook's distance\n\n::: lob\n2.  Handle influential points and assumption violations by checking data\n    errors, reassessing the model, and making data transformations.\n:::\n\n3.  Implement a model with data transformations and determine if it\n    improves the model fit.\n\n## How do we deal with influential points?\n    \n-   If an observation is influential, **we perform a sensitivity analysis**:\n    - We took out the influential points we identified then reran the model\n    - Often, you'll see that the \"influential points\" have not drastically changed your estimates\n        - A change in sign (for example: positive slope to negative slope)\n        - A really large increase (think more than 2x the original value)\n\n-   If an observation is influential, **we check data errors**:\n\n    -   Was there a data entry or collection problem?\n\n    -   If you have reason to believe that the observation does not hold\n        within the population (or gives you cause to redefine your\n        population)\n\n-   If an observation is influential, **we check our model**:\n\n    -   Did you leave out any important predictors?\n\n    -   Should you consider adding some interaction terms?\n\n    -   Is there any nonlinearity that needs to be modeled?\n    \n## Important note on influential observations\n\n-   It's always weird to be using numbers to help you diagnose an issue,\n    but the issue kinda gets unresolved\n\n \n\n-   Basically, deleting an observation should be justified outside of\n    the numbers!\n\n    -   If it's an honest data point, then it's giving us important\n        information!\n        \n \n\n-   [A really well thought out explanation from\n    StackExchange](https://stats.stackexchange.com/questions/81058/how-to-handle-leverage-values)\n    \n    \n\n## Checking our model\n\n-   An observation **may be** influential if the model is not correctly specified\n    -   We may also see issues with the LINE assumptions\n\n-   What are our options to specify the model \"correctly?\"\n\n    -   See if we need to add predictors to our model\n\n        -   Nicky's thought for our life expectancy example\n\n    -   Try a transformation if there is an issue with linearity or\n    normality\n\n    -   Try a transformation if there is unequal variance\n\n    -   Try a weighted least squares approach if unequal variance (might be\n    lesson at end of course)\n\n    -   Try a robust estimation procedure if we have a lot of outlier issues\n    (outside scope of class)\n\n# Learning Objectives\n\n1.  Use visualizations and cut off points to flag potentially\n    influential points using residuals, leverage, and Cook's distance\n\n2.  Handle influential points and assumption violations by checking data\n    errors, reassessing the model, and making data transformations.\n\n::: lob\n3.  Implement a model with data transformations and determine if it\n    improves the model fit.\n:::\n\n## Transformations\n\n-   When we have issues with our LINE (mostly linearity, normality, or\n    equality of variance) assumptions\n\n    -   We can use transformations to improve the fit of the model\n\n-   Transformations can...\n\n    -   Make the relationship more linear\n\n    -   Make the residuals more normal\n\n    -   \"Stabilize\" the variance so that it is more constant\n\n    -   It can also bring in or reduce outliers\n\n-   We can transform the dependent ($Y$) variable and/or the independent\n    ($X$) variable\n\n    -   Usually we want to try transforming the $X$ first\n\n \n\n-   **Requires trial and error!!**\n-   **Major drawback:** interpreting the model becomes harder!\n\n## Common transformations\n\n-   Tukey's transformation (power) ladder\n\n    -   Use `R`'s `gladder()` command from the `describedata` package\n\n| Power p | -3              | -2              | -1            | -1/2                 | 0         | 1/2        | 1   | 2     | 3     |\n|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n|         | $\\frac{1}{x^3}$ | $\\frac{1}{x^2}$ | $\\frac{1}{x}$ | $\\frac{1}{\\sqrt{x}}$ | $\\log(x)$ | $\\sqrt{x}$ | $x$ | $x^2$ | $x^3$ |\n\n::: columns\n::: {.column width=\"50%\"}\n-   How to use the power ladder for the general distribution shape\n\n    -   If data are skewed left, we need to compress smaller values\n        towards the rest of the data\n\n        -   Go \"up\" ladder to transformations with power \\> 1\n\n    -   If data are skewed right, we need to compress larger values\n        towards the rest of the data\n\n        -   Go \"down\" ladder to transformations with power \\< 1\n:::\n\n::: {.column width=\"50%\"}\n-   How to use the power ladder for heteroscedasticity\n\n    -   If higher $X$ values have more spread\n\n        -   Compress larger values towards the rest of the data\n\n        -   Go \"down\" ladder to transformations with power \\< 1\n\n    -   If lower $X$ values have more spread\n\n        -   Compress smaller values towards the rest of the data\n\n        -   Go \"up\" ladder to transformations with power \\> 1\n:::\n:::\n\n## Poll Everywhere Question 3\n\n\n## Transform independent variable?\n\n::: columns\n::: column\n```{r}\n#| out-width: 100%\n\nggplot(gapm, \n       aes(x = FemaleLiteracyRate)) +\n  geom_histogram()\n```\n:::\n::: column\n\n- Looks like more spread on the left side\n- Use powers greater than 1 \n  - $FLR^2$ and $FLR^3$\n:::\n:::\n\n## `gladder()` of female literacy rate\n\n```{r fig.width=7, fig.height=5}\n#| fig-align: center\ngladder(gapm$FemaleLiteracyRate)\n```\n\n## `ladder()` of female literacy rate {visibility=\"hidden\"}\n\n::: columns\n::: {.column width=\"40%\"}\n-   `ladder()` output tests various transformations of the data for\n    normality\n-   Shapiro-Wilkes test is used to assess for normality\n    -   $H_0$: data are from a normal population\n    -   $H_A$: data are NOT from a normal population\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| fig-align: center\nladder(gapm$FemaleLiteracyRate) %>% \n  gt() %>%\n  tab_options(table.font.size = 40) %>%\n  fmt_number(decimals = 3)\n```\n:::\n:::\n\n## Transform dependent variable?\n\n::: columns\n::: column\n\n```{r}\n#| out-width: 100%\n\nggplot(gapm, \n       aes(x = LifeExpectancyYrs)) +\n  geom_histogram()\n```\n:::\n::: column\n\n- Looks like more spread on the left side as well\n- Use powers greater than 1 \n  - $LE^2$ and $LE^3$\n  \n:::\n:::\n\n## `gladder()` of life expectancy\n\n```{r fig.width=7, fig.height=5}\n#| fig-align: center\ngladder(gapm$LifeExpectancyYrs)\n```\n\n## `ladder()` of life expectancy {visibility=\"hidden\"}\n\n::: columns\n::: {.column width=\"40%\"}\n-   `ladder()` output tests various transformations of the data for\n    normality\n-   Shapiro-Wilkes test is used to assess for normality\n    -   $H_0$: data are from a normal population\n    -   $H_A$: data are NOT from a normal population\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| fig-align: center\nladder(gapm$LifeExpectancyYrs) %>% \n  gt() %>%\n  tab_options(table.font.size = 40) %>%\n  fmt_number(decimals = 3)\n```\n:::\n:::\n\n## Tips\n\n-   Recall, assessing our LINE assumptions are not on $Y$ alone!! (it's $Y|X$)\n\n    -   We can use `gladder()` to get a sense of what our\n        transformations will do to the data, but we need to check with\n        our residuals again!!\n\n-   Transformations usually work better if **all values** are positive (or\n    negative)\n\n-   If observation has a 0, then we cannot perform certain\n    transformations\n\n-   Log function only defined for positive values\n\n    -   We might take the $log(X+1)$ if $X$ includes a 0 value\n\n-   When we make cubic or square transformations, we MUST include the\n    original $X$ in the model\n\n    -   We do not do this for $Y$ though\n\n## Add quadratic and cubic transformations to dataset\n\n-   Helpful to make a new variable with the transformation in your\n    dataset\n\n```{r}\ngapm <- gapm %>% \n  mutate(LE_2 = LifeExpectancyYrs^2,\n         LE_3 = LifeExpectancyYrs^3,\n         FLR_2 = FemaleLiteracyRate^2,\n         FLR_3 = FemaleLiteracyRate^3)\n\ncolnames(gapm)\n```\n\n## We are going to compare a few different models with transformations\n\nWe are going to call life expectancy $LE$ and female literacy rate $FLR$\n\n-   Model 1: $LE = \\beta_0 + \\beta_1 FLR + \\epsilon$\n-   Model 2: $LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon$\n-   Model 3: $LE^3 = \\beta_0 + \\beta_1 FLR + \\epsilon$\n-   Model 4: $LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\epsilon$\n-   Model 5:\n    $LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon$\n-   Model 6:\n    $LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon$\n\n## Poll Everywhere Question 4\n\n## Compare Scatterplots: does linearity improve?\n\n```{r fig.width=10, fig.height=5}\n#| echo: false\n\nplot_m1 <- ggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod1: LE ~ FLR\")\n\nplot_m2 <- ggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LE_2)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod2: LE^2 ~ FLR\")\n\nplot_m3 <- ggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LE_3)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod3: LE^3 ~ FLR\")\n\nplot_m4 <- ggplot(gapm, aes(x = FLR_2,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod4: LE ~ FLR + FLR^2\")\n\nplot_m5 <- ggplot(gapm, aes(x = FLR_3,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod5: LE ~ FLR + FLR^2 + FLR^3\")\n\nplot_m6 <- ggplot(gapm, aes(x = FLR_3,\n                 y = LE_3)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod6: LE^3 ~ FLR + FLR^2 + FLR^3\")\n\ngrid.arrange(plot_m1, plot_m2, plot_m3, \n             plot_m4, plot_m5, plot_m6,\n             nrow = 2)\n\n```\n\n\n## Run models with transformations: examples\n\n**Model 2:** $LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon$\n\n```{r}\nmodel2 <- lm(LE_2 ~ FemaleLiteracyRate,\n             data = gapm)\n```\n\n```{r}\n#| echo: false\ntidy(model2) %>% gt() %>%\n  tab_options(table.font.size = 40) %>%\n  fmt_number(decimals = 3)\n```\n\n**Model 6:**\n$LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon$\n\n```{r}\nmodel6 <- lm(LE_3 ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\n```\n\n```{r}\n#| echo: false\ntidy(model6) %>% gt() %>%\n  tab_options(table.font.size = 40) %>%\n  fmt_number(decimals = 3)\n```\n\n```{r}\n#| echo: false\naug2 <- augment(model2)\naug6 <- augment(model6)\nmodel3 <- lm(LE_3 ~ FemaleLiteracyRate,\n             data = gapm)\n\naug3 <- augment(model3)\n```\n\n```{r}\n#| echo: false\nmodel4 <- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2,\n             data = gapm)\n\naug4 <- augment(model4)\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model4)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model4, which = 5)\n```\n\n```{r}\n#| echo: false\nmodel5 <- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\n\naug5 <- augment(model5)\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model5)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model5, which = 5)\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model6)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model6, which = 5)\n```\n\n## Normal Q-Q plots comparison\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| fig-align: center\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 2)\nplot(model2, which = 2)\nplot(model3, which = 2)\nplot(model4, which = 2)\nplot(model5, which = 2)\nplot(model6, which = 2)\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column\n```\n\n## Residual plots comparison\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| fig-align: center\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 1)\nplot(model2, which = 1)\nplot(model3, which = 1)\nplot(model4, which = 1)\nplot(model5, which = 1)\nplot(model6, which = 1)\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column\n```\n\n## Summary of transformations\n\n-   If the model without the transformation is **blatantly violating a\n    LINE assumption**\n    \n    -   Then a transformation is a good idea\n    -   If transformations do not help, then keep it untransformed\n\n \n\n-   If the model without a transformation is **not following the LINE assumptions very well, but is mostly okay**\n\n    -   Then try to avoid a transformation\n    -   Think about what predictors might need to be added\n    -   Especially if you keep seeing the same points as influential\n    \n \n\n-   If **interpretability** is important in your final work, then **transformations are not a great solution**\n\n## Models comparison {visibility=\"hidden\"}\n\n```{r}\n# library(gtsummary) for tbl_regression() and tbl_merge()\n\ntbl_model1 <- tbl_regression(model1)\n\ntbl_model2 <- tbl_regression(model2)\n\ntbl_model3 <- tbl_regression(model3)\n\ntbl_model4 <- tbl_regression(model4)\n\ntbl_model5 <- tbl_regression(model5)\n\ntbl_model6 <- tbl_regression(model6)\n\n# Compare models 1-3\ntbl_merge(\n  tbls = list(tbl_model1, tbl_model2, tbl_model3),\n  tab_spanner = c(\"Model 1: y=LE\", \"Model 2: y=LE^2\", \"Model 3: y=LE^3\")\n  )\n\n# Compare models 4-6\ntbl_merge(\n  tbls = list(tbl_model4, tbl_model5, tbl_model6),\n  tab_spanner = c(\"Model 4: y=LE\", \"Model 5: y=LE\", \"Model 6: y=LE^3\")\n  )\n```\n\n## Other fit statistics comparison {visibility=\"hidden\"}\n\n```{r}\nglance(model1) %>% gt()\nglance(model2) %>% gt()\nglance(model3) %>% gt()\nglance(model4) %>% gt()\nglance(model5) %>% gt()\nglance(model6) %>% gt()\n```\n\n## Example: Chapter 5 Problem 9 {visibility=\"hidden\"}\n\n-   In an experiment designed to describe the dose–response curve for\n    vitamin K, individual rats were depleted of their vitamin K reserves\n    and then fed dried liver for 4 days at different dosage levels.\n-   The response of each rat was measured as the concentration of a\n    clotting agent needed to clot a sample of its blood in 3 minutes.\n-   The results of the experiment on 12 rats are given in the following\n    table; **values are expressed in common logarithms for both dose and\n    response**.\n    -   *Note: by \"common logarithm\" the authors mean a base 10\n        logarithm*\n\n> Question: why did they choose a log-log transformation?\n\n```{r}\nrats <- read_excel(\"data/CH05Q09.xls\")\nglimpse(rats)\n\nloglog_plot <- ggplot(rats, aes(x = LOGDOSE, y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Transformed variables\")\nloglog_plot\n```\n\n## Reference: all run models {.smaller}\n\n::: columns\n::: column\nModel 2: $LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon$\n\n```{r}\nmodel2 <- lm(LE_2 ~ FemaleLiteracyRate,\n             data = gapm)\ntidy(model2) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model2)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model2, which = 5)\n```\n\nModel 3: $LE^3 \\sim FLR$\n\n```{r}\nmodel3 <- lm(LE_3 ~ FemaleLiteracyRate,\n             data = gapm)\ntidy(model3) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model3)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model3, which = 5)\n```\n\nModel 4: $LE \\sim FLR + FLR^2$\n\n```{r}\nmodel4 <- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2,\n             data = gapm)\ntidy(model4) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model4)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model4, which = 5)\n```\n:::\n\n::: column\nModel 5: $LE \\sim FLR + FLR^2 + FLR^3$\n\n```{r}\nmodel5 <- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\ntidy(model5) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model5)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model5, which = 5)\n```\n\nModel 6: $LE^3 \\sim FLR + FLR^2 + FLR^3$\n\n```{r}\nmodel6 <- lm(LE_3 ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\ntidy(model6) %>% gt()\n```\n\n```{r fig.height=5, fig.width=7}\n#| echo: false\n#| eval: false\nautoplot(model6)\n```\n\n```{r fig.height=3, fig.width=4}\n#| echo: false\n#| eval: false\nplot(model6, which = 5)\n```\n:::\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","highlight-style":"ayu","output-file":"08_SLR_Diag.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.25","auto-stretch":true,"title":"Lesson 8: SLR: Model Diagnostics","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"02/3/2025","editor":{"markdown":{"wrap":72}},"theme":"../simple_NW.scss","chalkboard":true,"slideNumber":true,"showSlideNumber":"all","width":1955,"height":1100,"footer":"Lesson 8: SLR 5"}}},"projectFormats":["html"]}