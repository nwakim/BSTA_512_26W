{"title":"Lesson 13: Model/Variable Selection","markdown":{"yaml":{"title":"Lesson 13: Model/Variable Selection","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"02/24/2025","format":{"revealjs":{"theme":"../simple_NW.scss","chalkboard":true,"slide-number":true,"show-slide-number":"all","width":1955,"height":1100,"footer":"Lesson 13: Model selection","highlight-style":"ayu","html-math-method":"mathjax"}},"execute":{"freeze":"auto"}},"headingText":"new packages","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\nlibrary(readxl)\n#----------\n# install.packages(\"describedata\")\nlibrary(describedata) # gladder()\nlibrary(gridExtra)   # grid.arrange()\nlibrary(ggfortify)  # autoplot(model)\n# New Day 6\nlibrary(gtsummary)\n\n# New Day 7\nlibrary(plotly) # for plot_ly() command\nlibrary(GGally) # for ggpairs() command \nlibrary(ggiraphExtra)   # for ggPredict() command\n\n# Load the data - update code if the csv file is not in the same location on your computer\n# If you need to download the file, please go to ur shared folder under Data > Slides\ngapm <- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\n\ngapm_sub <- gapm %>% # called it gapm2_sub3 to be consistent with Day 7 notes\n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, four_regions, FoodSupplykcPPD) %>%\n  mutate(four_regions = factor(four_regions, \n                               levels = c(\"africa\", \"americas\", \n                                          \"asia\", \"europe\"), \n                               labels = c(\"Africa\", \"Americas\", \n                                          \"Asia\", \"Europe\"))) %>%\n  rename(income_levels = `World bank, 4 income groups 2017`) %>%\n  mutate(income_levels1 = factor(income_levels, \n                                levels = c(\"Low income\", \n                                           \"Lower middle income\", \n                                           \"Upper middle income\", \n                                           \"High income\")), \n         income_levels2 = relevel(factor(income_levels, \n                                levels = c(\"Low income\", \n                                           \"Lower middle income\", \n                                           \"Upper middle income\", \n                                           \"High income\"), \n                                labels = c(\"Lower income\", \"Lower income\", \n                                            \"Higher income\", \"Higher income\")), \n                                ref = \"Lower income\"))\ngapm_ind = gapm_sub %>% select(LifeExpectancyYrs, country)\n```\n\n# Learning Objectives\n\n1.  Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\n2.  Explain the general process or idea behind different model selection techniques\n3.  Recognize common model fit statistics and understand what they measure\n\n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n\n```{css, echo=FALSE}\n.sourceCode code {\n  font-size: 1.3em;\n}\n```\n\n## Regression analysis process\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n\n\n# Learning Objectives\n\n::: lob\n1.  Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\n:::\n\n2.  Explain the general process or idea behind different model selection techniques\n3.  Recognize common model fit statistics and understand what they measure\n\n## Some important definitions\n\n-   **Model selection**: picking the \"best\" model from a set of possible models\n\n    -   Models will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n\n \n\n-   **Model selection strategies**: a process or framework that helps us pick our \"best\" model\n\n    -   These strategies often differ by the approach and criteria used to the determine the \"best\" model\n\n \n\n-   **Overfitting**: result of fitting a model so closely to our *particular* sample data that it cannot be generalized to other samples (or the population)\n\n \n\n\n- **Model parsimony**: model that uses the fewest possible parameters to explain the relationship \n\n## Why can't I just throw in all the variables into my model?\n\n-   First, let's think about the number of observations in our dataset\n\n    - In our Gapminder dataset, we have ~80 countries \n    - The closer the number of variables is to the number of observations: the model overfit the data and lose precision on coefficient estimates\n\n-   Extreme example: In the Gapminder dataset, I can use an indicator for each country\n\n    -   Remember that each country is an observation\n\n    -   So we have a perfectly fit model - a covariate for each observation\n\n    -   But we cannot generalize this to any other countries\n\n    -   And we haven't identified any meaningful relationships between life expectancy and other measured characteristics\n\n-   More covariates in the model is not always better\n\n    -   Overfitting the data limits our generalizability and prevents us from answering research questions\n\n## Think back to population model vs. estimated models\n\n- Population model = true, underlying relationship\n  - We've discussed this with a set group of covariates\n  - But we also do NOT know exactly what variables are at play in the true, underlying model\n  \n \n  \n- Estimated model = model estimated with subset of the variables at play and a subset of the population (aka sample)\n\n \n\n- Example: anti-fat bias\n  - Will do our best to estimate the association in our research question\n  - But we have two things working against us:\n    - We do NOT know the exact social complexities involved in this bias\n    - We cannot include all the variables that we have access to \n\n## Model Complexity vs. Parsimony\n\n::: columns\n::: {.column width=\"50%\"}\nSuppose we have $p = 30$ covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\n1.  We could fit a model using all of the covariates.\n    -   In this case, $\\widehat\\beta$ is unbiased for $\\beta$ (in a linear model fit using OLS). But $\\widehat\\beta$ has very high variance.\n2.  We could fit a model using only the five strongest covariates.\n    -   In this case, $\\widehat\\beta$ will more likely be biased for $\\beta$, but it will have lower variance (compared to the estimate including all covariates)\n    \n- Increasing the number of observations sometimes, but not always, helps with the first approach\n:::\n\n::: {.column width=\"50%\"}\n[![Source: http://scott.fortmann-roe.com/docs/BiasVariance.html](../img_slides/bullseye.png){fig-align=\"center\"}](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n:::\n:::\n\n## Bias-variance trade off\n\n::: columns\n::: {.column width=\"50%\"}\n-   Recall mean square error is a function of SSE (sum of squared residuals)\n\n    $$\n    MSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2 = \\dfrac{1}{n} SSE\n    $$\n\n-   MSE can also be written as a function of the bias and variance\n\n    $$\n    MSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n    $$\n\n-   For the same data:\n\n    -   More covariates in model: less bias, more variance\n\n    -   Less covariates in model: more bias, less variance\n\n-   Our goal: find a model with just the right amount of covariates to balance bias and vairance\n:::\n\n::: {.column width=\"50%\"}\n[![Source: http://scott.fortmann-roe.com/docs/BiasVariance.html](../img_slides/biasvariance_tradeoff.png){width=\"1000\"}](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n:::\n:::\n\n## \n\n![](../img_slides/all_mod_wrong.jpg)\n\n## Model Selection basics (slide adjusted from Jodi Lapidus)\n\n-   \"Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.\"\n\n    -   From: [Statistical Foundations for Model-Based Adjustments](https://www.annualreviews.org/doi/pdf/10.1146/annurev-publhealth-031914-122559)\n\n-   Not all statistical texts provide practical advice on model development\n\n    -   A lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\n    -   Other texts are sparse on details or incorporate simplistic approaches\n\n-   Model development strategy should **align with research goals**\n\n    -   Prediction vs. Estimating Association\n    -   Strategy may depend on study design and data set size\n\n## The goals of association vs. prediction\n\n::: columns\n::: column\n::: definition\n::: def-title\nAssociation / Explanatory / One variable's effect\n:::\n\n::: def-cont\n-   **Goal:** Understand one variable's (or a group of variable's) effect on the response after adjusting for other factors\n\n-   Mainly interpret the coefficient of the variable that is the focus of the study\n\n    -   Interpreting the coefficients of the other variables is not important, but can help bring context\n\n-   Any variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\n\n-   Example: How is body mass of a penguin associated with flipper length?\n:::\n:::\n:::\n\n::: column\n::: proposition\n::: prop-title\nPrediction\n:::\n\n::: prop-cont\n-   **Goal:** to calculate the most precise prediction of the response variable\n\n-   Interpreting coefficients is not important\n\n-   Choose only the variables that are strong predictors of the response variable\n\n    -   Excluding irrelevant variables can help reduce widths of the prediction intervals\n\n-   Example: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?\n:::\n:::\n:::\n:::\n\n## Model building for association vs. prediction\n\n[More information on the two analysis goals:](https://onlinelibrary.wiley.com/doi/10.1016/j.pmrj.2014.08.941)\n\n![](../img_slides/explanatory_v_prediction.png){fig-align=\"center\"}\n\n[If you ever get the chance, check out Dr. Kristin Sainani's series on Statistics](https://onlinelibrary.wiley.com/authored-by/Sainani/Kristin+L.?startPage=&SeriesKey=19341563)\n\n## Poll Everywhere Question 1\n\n## Model selection strategies for *continuous* outcomes\n\n::: columns\n::: column\n::: definition\n::: def-title\nAssociation / Explanatory / One variable's effect\n:::\n\n::: def-cont\n-   Selection of potential models is tied more with the research context with some incorporation of prediction scores\n\n-   Pre-specification of multivariable model\n\n-   Purposeful model selection\n\n    -   \"Risk factor modeling\"\n\n-   Change in Estimate (CIE) approaches\n\n    -   Will learn in Survival Analysis (BSTA 514)\n:::\n:::\n:::\n\n::: column\n::: proposition\n::: prop-title\nPrediction\n:::\n\n::: prop-cont\n-   Selection of potential models is fully dependent on prediction scores\n\n-   Automated strategies\n\n    -   Stepwise selection (forward/backward)\n        -   You'll see these a lot, but they're not really good methods\n    -   Best subset\n    -   Regularization techniques (LASSO, Ridge, Elastic net)\n:::\n:::\n:::\n:::\n\n-   For categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\n    -   Examples: Decision trees, Random forest, Neural networks, K-means\n\n# Learning Objectives\n\n1.  Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\n\n::: lob\n2.  Explain the general process or idea behind different model selection techniques\n:::\n\n3.  Recognize common model fit statistics and understand what they measure\n\n## Pre-specification of multivariable model (slide adapted from Jodi Lapidus)\n\n-   In a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts\n\n    - Basically: we completely define the model that we will fit (based on previous work and literature)\n  \n \n\n-   If we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance\n\n    -   Example: Comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\n\n    -   Variables such as study site, as well as any randomization stratification variables are common covariates.\n    \n    -   Partly to make sure that we are not adding in variables that make our regimen significant\n\n \n\n-   In these cases, only a limited number of multivariable models are fit and reported\n\n    -   Do not perform all the model building steps outlined in Hosmer and Lemeshow texts\n\n## Purposeful model selection (slide adapted from Jodi Lapidus)\n\n-   Can use this type of model selection for any type of regression\n\n-   Careful, well-thought out variable selection process\n\n    -   Considers both confounding and interaction, as well as checking model assumptions, fit, etc.\n\n-   Often a reasonable strategy, especially in epidemiology and more exploratory clinical studies\n\n    -   However, not always appropriate!\n    -   E.g. clinical trials with model specified in advance. (pre-specified model)\n\n \n\n-   **This is the selection process that we will focus on in this class!**\n    - Next lecture\n\n## Change in estimate (CIE) approach (slide adapted from Jodi Lapidus)\n\n-   CIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\n    -   Observed change is presumed to measure confounding by the covariate.\n\n-   What estimate?\n\n    -   Hosmer/Lemeshow (H/L) text suggest using coefficients from the model\n    -   We typically use the coefficient estimate from the explanatory variable that we are most interested in\n\n-   What magnitude change is ”important”?\n\n    -   H/L text suggest 10%\n\n-   One must choose an effect measure to judge change importance, where \"importance\" needs to be evaluated along a contextually meaningful scale\n\n-   Accurate assessment of confounding may require examining changes from removing entire sets of covariates\n\n    -   Add in or eliminate candidate confounders one at time?\n    -   Add in or eliminate candidate confounders in sets?\n\n## Stepwise selection (slide adapted from Adrianna Westbrook)\n\n-   This is an incredibly common approach that statisticians use, often because it is an older and more recognized method\n\n    -   BUT IT IS ALSO ONE OF THE WORST MODEL SELECTION STRATEGIES!!\n\n-   Major disadvantages to stepwise selection:\n\n    -   Prone to overfitting\n    -   Biased estimates\n    -   Cements the wrong idea that we are looking for our \"most significant\" covariates\n\n-   Predictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)\n\n## Stepwise selection: two common approaches\n\n-   I will introduce two of the approaches so that you understand the general process if a collaborator ever mentions stepwise selection\n\n-   Forward selection:\n\n    -   For $p$ covariates potential covariates, run all simple linear regressions:\n\n        -   $Y= \\beta_0 + \\beta_1 X_1 + \\epsilon$ through $Y= \\beta_0 + \\beta_1 X_{p} + \\epsilon$\n        -   Include the $X_i$ with the lowest p-value (assuming it is below the threshold)\n\n    -   Now run $Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_1 + \\epsilon$ through $Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_{p} + \\epsilon$ and enter the next $X_j$ with the lowest p-value\n\n    -   Continue process until no more predictors come back with a p-value below the threshold\n\n-   Backward selection:\n\n    -   Start with a full model ($Y= \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\epsilon$) and remove predictor with the highest p-value (assuming it is above the threshold)\n    -   Repeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)\n\n## Best subset (slide adapted from Adrianna Westbrook)\n\n-   I don't see this approach very often\n\n-   Quite literally making subsets of the data and using the \"best\" one\n\n-   General steps:\n\n    -   Run every possible model fitting 1 to all possible $p$ predictors/covariates\n    -   You can limit number of potential predictors\n    -   $2^p$ = total number of models where $p$ = number of predictors\n    -   You will get the best fitting model within each category (i.e., 1 predictor model, 2 predictor model,..., $p$ predictor model)\n    -   Then have to find the best fitting model between the best models from each category\n\n-   Major disadvantages to best subset:\n\n    -   Does not account for interactions\n    -   Needs to run a lot of models (takes A LOT of time)\n\n## Regularization techniques\n\n-   Regularization techniques (LASSO, ridge, elastic net) adds a penalization that shrinks (or regularizes) coefficients down to reduce overfitting\n- Maximize likelihood of a model, but includes a penalty for additional covariates\n\n|              |                                                           |                                                              |                                                                                       |\n|------------------|------------------|------------------|------------------|\n|              | LASSO (Least About Shrinkage and Selection Operator)      | Ridge                                                        | Elastic Net                                                                           |\n| Penalization | L-1 Norm, uses absolute value                             | L-2 Norm, uses squared value                                 | Best of both worlds, L-1 and L-2 used                                                 |\n| Pro's        | Reduces overfitting, will shrink coefficient to zero      | Reduces overfitting, handles collinearity, can handle k\\>n   | Reduces overfitting, handles collinearity, handles k\\>n, shrinks coefficients to zero |\n| Con's        | Cannot handle k\\>n, doesn’t handle multicollinearity well | Does not shrink coefficients to zero, difficult to interpret | More difficult for R to do than the other two (but not really that bad)               |\n\n## Poll Everywhere Question 2\n\n# Learning Objectives\n\n1.  Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\n2.  Explain the general process or idea behind different model selection techniques\n\n::: lob\n3.  Recognize common model fit statistics and understand what they measure\n:::\n\n## Introduction to model fit statistics\n\n-   So far we have compared models using the F-test\n\n-   The F-test is a great way to compare models that are **nested**\n\n    -   Basically, this means that the \"full\" model contains all the covariates that the \"reduced\" model contains\n    -   The full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\n-   What if we want to compare models that are not nested?\n\n    -   There is a special group of fit statistics that can help us compare models\n\n    -   Note: these are sometimes used in the model building process (within one strategy)\n\n        -   Helpful if we want to compare selected models across strategies\n\n        -   Helpful if we have a few \"final\" models with different covariates that we want to compare\n\n## Common model fit statistics\n\n-   The following model fit statistics combine information about the SSE, the number of parameters in the model, and the sample size\n\n-   For Mallow's Cp, AIC, and BIC: **smaller** values indicate better model fit!\n\n-   For Adjusted R-squared: **larger** values indicate better model fit!\n\n| Fit statistic                        | Equation                                                                              | R code                                                                             |\n|-------------------------|----------------------------------|-------------|\n| R-squared / Adjusted R-squared       | $Adj. R^2 = 1 - \\frac{SSE/(n-p-1)}{SSY/(n-1)}$                                        | [Within `summary(model_name)`](https://www.statology.org/adjusted-r-squared-in-r/) |\n| Mallow's $C_p$                       | $C_p = \\Bigg[ \\dfrac{\\widehat\\sigma^2_p}{\\widehat\\sigma^2_{max}} - 1 \\Bigg](n-p) + p$ | [`ols_mallows_cp()`](https://www.statology.org/how-to-calculate-mallows-cp-in-r/)  |\n| Akaike information criterion (AIC)   | $AIC = n\\log(SSE) - n \\log(n) + 2(p+1)$                                               | `AIC(model_name)`                                                                  |\n| Bayesian information criterion (BIC) | $BIC = n\\log(SSE) - n\\log(n) + log(n)\\cdot(p+1)$                                      | `BIC(model_name)`                                                                  |\n\n \n\n-   We don't need to know the exact formulas for them!\n\n## Common model fit statistics\n\n-   There is no hypothesis testing for these fit statistics\n\n    -   Only helpful if you are comparing models\n\n    -   Works for nested and non-nested models\n\n-   Common to report all or some of them\n\n-   All of the fit statistics will not necessarily reach a consensus about the best fitting model\n\n    -   Each weigh SSE, number of parameters, and number of observations differently\n\n![https://www.researchgate.net/figure/Model-Fit-Statistics_tbl1_308844501](../img_slides/Model-Fit-Statistics.png)\n\n## Example of a table for model fit statistics\n\n```{r}\n#| echo: true\n\nlibrary(olsrr)\nmodel1 = gapm_sub %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nmodel2 = gapm_sub %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\nmodel3 = gapm_sub %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + \n                           income_levels + four_regions)\n\nfit_stats = data.frame(Model = c(\"Model 1: FLR only\", \"Model 2: FLR + FS\", \"Model 3: FLR + FS + Income + WR\"), \n                       adj_R_2 = c(summary(model1)$adj.r.squared, summary(model2)$adj.r.squared, \n                                   summary(model3)$adj.r.squared), \n                       Cp = c(ols_mallows_cp(model1, model3), ols_mallows_cp(model2, model3), \n                              ols_mallows_cp(model3, model3)), \n                       AIC = c(AIC(model1), AIC(model2), AIC(model3)), \n                       BIC = c(BIC(model1), BIC(model2), BIC(model3)))\n\ncolnames(fit_stats) = c(\"Model\", \"Adj. R-squared\", \"Mallow's Cp\", \"AIC\", \"BIC\")\n\nfit_stats %>% gt() %>% tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\nlibrary(readxl)\n#----------\n# new packages\n# install.packages(\"describedata\")\nlibrary(describedata) # gladder()\nlibrary(gridExtra)   # grid.arrange()\nlibrary(ggfortify)  # autoplot(model)\n# New Day 6\nlibrary(gtsummary)\n\n# New Day 7\nlibrary(plotly) # for plot_ly() command\nlibrary(GGally) # for ggpairs() command \nlibrary(ggiraphExtra)   # for ggPredict() command\n\n# Load the data - update code if the csv file is not in the same location on your computer\n# If you need to download the file, please go to ur shared folder under Data > Slides\ngapm <- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\n\ngapm_sub <- gapm %>% # called it gapm2_sub3 to be consistent with Day 7 notes\n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, four_regions, FoodSupplykcPPD) %>%\n  mutate(four_regions = factor(four_regions, \n                               levels = c(\"africa\", \"americas\", \n                                          \"asia\", \"europe\"), \n                               labels = c(\"Africa\", \"Americas\", \n                                          \"Asia\", \"Europe\"))) %>%\n  rename(income_levels = `World bank, 4 income groups 2017`) %>%\n  mutate(income_levels1 = factor(income_levels, \n                                levels = c(\"Low income\", \n                                           \"Lower middle income\", \n                                           \"Upper middle income\", \n                                           \"High income\")), \n         income_levels2 = relevel(factor(income_levels, \n                                levels = c(\"Low income\", \n                                           \"Lower middle income\", \n                                           \"Upper middle income\", \n                                           \"High income\"), \n                                labels = c(\"Lower income\", \"Lower income\", \n                                            \"Higher income\", \"Higher income\")), \n                                ref = \"Lower income\"))\ngapm_ind = gapm_sub %>% select(LifeExpectancyYrs, country)\n```\n\n# Learning Objectives\n\n1.  Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\n2.  Explain the general process or idea behind different model selection techniques\n3.  Recognize common model fit statistics and understand what they measure\n\n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n\n```{css, echo=FALSE}\n.sourceCode code {\n  font-size: 1.3em;\n}\n```\n\n## Regression analysis process\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n\n\n# Learning Objectives\n\n::: lob\n1.  Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\n:::\n\n2.  Explain the general process or idea behind different model selection techniques\n3.  Recognize common model fit statistics and understand what they measure\n\n## Some important definitions\n\n-   **Model selection**: picking the \"best\" model from a set of possible models\n\n    -   Models will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n\n \n\n-   **Model selection strategies**: a process or framework that helps us pick our \"best\" model\n\n    -   These strategies often differ by the approach and criteria used to the determine the \"best\" model\n\n \n\n-   **Overfitting**: result of fitting a model so closely to our *particular* sample data that it cannot be generalized to other samples (or the population)\n\n \n\n\n- **Model parsimony**: model that uses the fewest possible parameters to explain the relationship \n\n## Why can't I just throw in all the variables into my model?\n\n-   First, let's think about the number of observations in our dataset\n\n    - In our Gapminder dataset, we have ~80 countries \n    - The closer the number of variables is to the number of observations: the model overfit the data and lose precision on coefficient estimates\n\n-   Extreme example: In the Gapminder dataset, I can use an indicator for each country\n\n    -   Remember that each country is an observation\n\n    -   So we have a perfectly fit model - a covariate for each observation\n\n    -   But we cannot generalize this to any other countries\n\n    -   And we haven't identified any meaningful relationships between life expectancy and other measured characteristics\n\n-   More covariates in the model is not always better\n\n    -   Overfitting the data limits our generalizability and prevents us from answering research questions\n\n## Think back to population model vs. estimated models\n\n- Population model = true, underlying relationship\n  - We've discussed this with a set group of covariates\n  - But we also do NOT know exactly what variables are at play in the true, underlying model\n  \n \n  \n- Estimated model = model estimated with subset of the variables at play and a subset of the population (aka sample)\n\n \n\n- Example: anti-fat bias\n  - Will do our best to estimate the association in our research question\n  - But we have two things working against us:\n    - We do NOT know the exact social complexities involved in this bias\n    - We cannot include all the variables that we have access to \n\n## Model Complexity vs. Parsimony\n\n::: columns\n::: {.column width=\"50%\"}\nSuppose we have $p = 30$ covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\n1.  We could fit a model using all of the covariates.\n    -   In this case, $\\widehat\\beta$ is unbiased for $\\beta$ (in a linear model fit using OLS). But $\\widehat\\beta$ has very high variance.\n2.  We could fit a model using only the five strongest covariates.\n    -   In this case, $\\widehat\\beta$ will more likely be biased for $\\beta$, but it will have lower variance (compared to the estimate including all covariates)\n    \n- Increasing the number of observations sometimes, but not always, helps with the first approach\n:::\n\n::: {.column width=\"50%\"}\n[![Source: http://scott.fortmann-roe.com/docs/BiasVariance.html](../img_slides/bullseye.png){fig-align=\"center\"}](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n:::\n:::\n\n## Bias-variance trade off\n\n::: columns\n::: {.column width=\"50%\"}\n-   Recall mean square error is a function of SSE (sum of squared residuals)\n\n    $$\n    MSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2 = \\dfrac{1}{n} SSE\n    $$\n\n-   MSE can also be written as a function of the bias and variance\n\n    $$\n    MSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n    $$\n\n-   For the same data:\n\n    -   More covariates in model: less bias, more variance\n\n    -   Less covariates in model: more bias, less variance\n\n-   Our goal: find a model with just the right amount of covariates to balance bias and vairance\n:::\n\n::: {.column width=\"50%\"}\n[![Source: http://scott.fortmann-roe.com/docs/BiasVariance.html](../img_slides/biasvariance_tradeoff.png){width=\"1000\"}](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n:::\n:::\n\n## \n\n![](../img_slides/all_mod_wrong.jpg)\n\n## Model Selection basics (slide adjusted from Jodi Lapidus)\n\n-   \"Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.\"\n\n    -   From: [Statistical Foundations for Model-Based Adjustments](https://www.annualreviews.org/doi/pdf/10.1146/annurev-publhealth-031914-122559)\n\n-   Not all statistical texts provide practical advice on model development\n\n    -   A lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\n    -   Other texts are sparse on details or incorporate simplistic approaches\n\n-   Model development strategy should **align with research goals**\n\n    -   Prediction vs. Estimating Association\n    -   Strategy may depend on study design and data set size\n\n## The goals of association vs. prediction\n\n::: columns\n::: column\n::: definition\n::: def-title\nAssociation / Explanatory / One variable's effect\n:::\n\n::: def-cont\n-   **Goal:** Understand one variable's (or a group of variable's) effect on the response after adjusting for other factors\n\n-   Mainly interpret the coefficient of the variable that is the focus of the study\n\n    -   Interpreting the coefficients of the other variables is not important, but can help bring context\n\n-   Any variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\n\n-   Example: How is body mass of a penguin associated with flipper length?\n:::\n:::\n:::\n\n::: column\n::: proposition\n::: prop-title\nPrediction\n:::\n\n::: prop-cont\n-   **Goal:** to calculate the most precise prediction of the response variable\n\n-   Interpreting coefficients is not important\n\n-   Choose only the variables that are strong predictors of the response variable\n\n    -   Excluding irrelevant variables can help reduce widths of the prediction intervals\n\n-   Example: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?\n:::\n:::\n:::\n:::\n\n## Model building for association vs. prediction\n\n[More information on the two analysis goals:](https://onlinelibrary.wiley.com/doi/10.1016/j.pmrj.2014.08.941)\n\n![](../img_slides/explanatory_v_prediction.png){fig-align=\"center\"}\n\n[If you ever get the chance, check out Dr. Kristin Sainani's series on Statistics](https://onlinelibrary.wiley.com/authored-by/Sainani/Kristin+L.?startPage=&SeriesKey=19341563)\n\n## Poll Everywhere Question 1\n\n## Model selection strategies for *continuous* outcomes\n\n::: columns\n::: column\n::: definition\n::: def-title\nAssociation / Explanatory / One variable's effect\n:::\n\n::: def-cont\n-   Selection of potential models is tied more with the research context with some incorporation of prediction scores\n\n-   Pre-specification of multivariable model\n\n-   Purposeful model selection\n\n    -   \"Risk factor modeling\"\n\n-   Change in Estimate (CIE) approaches\n\n    -   Will learn in Survival Analysis (BSTA 514)\n:::\n:::\n:::\n\n::: column\n::: proposition\n::: prop-title\nPrediction\n:::\n\n::: prop-cont\n-   Selection of potential models is fully dependent on prediction scores\n\n-   Automated strategies\n\n    -   Stepwise selection (forward/backward)\n        -   You'll see these a lot, but they're not really good methods\n    -   Best subset\n    -   Regularization techniques (LASSO, Ridge, Elastic net)\n:::\n:::\n:::\n:::\n\n-   For categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\n    -   Examples: Decision trees, Random forest, Neural networks, K-means\n\n# Learning Objectives\n\n1.  Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\n\n::: lob\n2.  Explain the general process or idea behind different model selection techniques\n:::\n\n3.  Recognize common model fit statistics and understand what they measure\n\n## Pre-specification of multivariable model (slide adapted from Jodi Lapidus)\n\n-   In a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts\n\n    - Basically: we completely define the model that we will fit (based on previous work and literature)\n  \n \n\n-   If we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance\n\n    -   Example: Comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\n\n    -   Variables such as study site, as well as any randomization stratification variables are common covariates.\n    \n    -   Partly to make sure that we are not adding in variables that make our regimen significant\n\n \n\n-   In these cases, only a limited number of multivariable models are fit and reported\n\n    -   Do not perform all the model building steps outlined in Hosmer and Lemeshow texts\n\n## Purposeful model selection (slide adapted from Jodi Lapidus)\n\n-   Can use this type of model selection for any type of regression\n\n-   Careful, well-thought out variable selection process\n\n    -   Considers both confounding and interaction, as well as checking model assumptions, fit, etc.\n\n-   Often a reasonable strategy, especially in epidemiology and more exploratory clinical studies\n\n    -   However, not always appropriate!\n    -   E.g. clinical trials with model specified in advance. (pre-specified model)\n\n \n\n-   **This is the selection process that we will focus on in this class!**\n    - Next lecture\n\n## Change in estimate (CIE) approach (slide adapted from Jodi Lapidus)\n\n-   CIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\n    -   Observed change is presumed to measure confounding by the covariate.\n\n-   What estimate?\n\n    -   Hosmer/Lemeshow (H/L) text suggest using coefficients from the model\n    -   We typically use the coefficient estimate from the explanatory variable that we are most interested in\n\n-   What magnitude change is ”important”?\n\n    -   H/L text suggest 10%\n\n-   One must choose an effect measure to judge change importance, where \"importance\" needs to be evaluated along a contextually meaningful scale\n\n-   Accurate assessment of confounding may require examining changes from removing entire sets of covariates\n\n    -   Add in or eliminate candidate confounders one at time?\n    -   Add in or eliminate candidate confounders in sets?\n\n## Stepwise selection (slide adapted from Adrianna Westbrook)\n\n-   This is an incredibly common approach that statisticians use, often because it is an older and more recognized method\n\n    -   BUT IT IS ALSO ONE OF THE WORST MODEL SELECTION STRATEGIES!!\n\n-   Major disadvantages to stepwise selection:\n\n    -   Prone to overfitting\n    -   Biased estimates\n    -   Cements the wrong idea that we are looking for our \"most significant\" covariates\n\n-   Predictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)\n\n## Stepwise selection: two common approaches\n\n-   I will introduce two of the approaches so that you understand the general process if a collaborator ever mentions stepwise selection\n\n-   Forward selection:\n\n    -   For $p$ covariates potential covariates, run all simple linear regressions:\n\n        -   $Y= \\beta_0 + \\beta_1 X_1 + \\epsilon$ through $Y= \\beta_0 + \\beta_1 X_{p} + \\epsilon$\n        -   Include the $X_i$ with the lowest p-value (assuming it is below the threshold)\n\n    -   Now run $Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_1 + \\epsilon$ through $Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_{p} + \\epsilon$ and enter the next $X_j$ with the lowest p-value\n\n    -   Continue process until no more predictors come back with a p-value below the threshold\n\n-   Backward selection:\n\n    -   Start with a full model ($Y= \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\epsilon$) and remove predictor with the highest p-value (assuming it is above the threshold)\n    -   Repeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)\n\n## Best subset (slide adapted from Adrianna Westbrook)\n\n-   I don't see this approach very often\n\n-   Quite literally making subsets of the data and using the \"best\" one\n\n-   General steps:\n\n    -   Run every possible model fitting 1 to all possible $p$ predictors/covariates\n    -   You can limit number of potential predictors\n    -   $2^p$ = total number of models where $p$ = number of predictors\n    -   You will get the best fitting model within each category (i.e., 1 predictor model, 2 predictor model,..., $p$ predictor model)\n    -   Then have to find the best fitting model between the best models from each category\n\n-   Major disadvantages to best subset:\n\n    -   Does not account for interactions\n    -   Needs to run a lot of models (takes A LOT of time)\n\n## Regularization techniques\n\n-   Regularization techniques (LASSO, ridge, elastic net) adds a penalization that shrinks (or regularizes) coefficients down to reduce overfitting\n- Maximize likelihood of a model, but includes a penalty for additional covariates\n\n|              |                                                           |                                                              |                                                                                       |\n|------------------|------------------|------------------|------------------|\n|              | LASSO (Least About Shrinkage and Selection Operator)      | Ridge                                                        | Elastic Net                                                                           |\n| Penalization | L-1 Norm, uses absolute value                             | L-2 Norm, uses squared value                                 | Best of both worlds, L-1 and L-2 used                                                 |\n| Pro's        | Reduces overfitting, will shrink coefficient to zero      | Reduces overfitting, handles collinearity, can handle k\\>n   | Reduces overfitting, handles collinearity, handles k\\>n, shrinks coefficients to zero |\n| Con's        | Cannot handle k\\>n, doesn’t handle multicollinearity well | Does not shrink coefficients to zero, difficult to interpret | More difficult for R to do than the other two (but not really that bad)               |\n\n## Poll Everywhere Question 2\n\n# Learning Objectives\n\n1.  Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\n2.  Explain the general process or idea behind different model selection techniques\n\n::: lob\n3.  Recognize common model fit statistics and understand what they measure\n:::\n\n## Introduction to model fit statistics\n\n-   So far we have compared models using the F-test\n\n-   The F-test is a great way to compare models that are **nested**\n\n    -   Basically, this means that the \"full\" model contains all the covariates that the \"reduced\" model contains\n    -   The full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\n-   What if we want to compare models that are not nested?\n\n    -   There is a special group of fit statistics that can help us compare models\n\n    -   Note: these are sometimes used in the model building process (within one strategy)\n\n        -   Helpful if we want to compare selected models across strategies\n\n        -   Helpful if we have a few \"final\" models with different covariates that we want to compare\n\n## Common model fit statistics\n\n-   The following model fit statistics combine information about the SSE, the number of parameters in the model, and the sample size\n\n-   For Mallow's Cp, AIC, and BIC: **smaller** values indicate better model fit!\n\n-   For Adjusted R-squared: **larger** values indicate better model fit!\n\n| Fit statistic                        | Equation                                                                              | R code                                                                             |\n|-------------------------|----------------------------------|-------------|\n| R-squared / Adjusted R-squared       | $Adj. R^2 = 1 - \\frac{SSE/(n-p-1)}{SSY/(n-1)}$                                        | [Within `summary(model_name)`](https://www.statology.org/adjusted-r-squared-in-r/) |\n| Mallow's $C_p$                       | $C_p = \\Bigg[ \\dfrac{\\widehat\\sigma^2_p}{\\widehat\\sigma^2_{max}} - 1 \\Bigg](n-p) + p$ | [`ols_mallows_cp()`](https://www.statology.org/how-to-calculate-mallows-cp-in-r/)  |\n| Akaike information criterion (AIC)   | $AIC = n\\log(SSE) - n \\log(n) + 2(p+1)$                                               | `AIC(model_name)`                                                                  |\n| Bayesian information criterion (BIC) | $BIC = n\\log(SSE) - n\\log(n) + log(n)\\cdot(p+1)$                                      | `BIC(model_name)`                                                                  |\n\n \n\n-   We don't need to know the exact formulas for them!\n\n## Common model fit statistics\n\n-   There is no hypothesis testing for these fit statistics\n\n    -   Only helpful if you are comparing models\n\n    -   Works for nested and non-nested models\n\n-   Common to report all or some of them\n\n-   All of the fit statistics will not necessarily reach a consensus about the best fitting model\n\n    -   Each weigh SSE, number of parameters, and number of observations differently\n\n![https://www.researchgate.net/figure/Model-Fit-Statistics_tbl1_308844501](../img_slides/Model-Fit-Statistics.png)\n\n## Example of a table for model fit statistics\n\n```{r}\n#| echo: true\n\nlibrary(olsrr)\nmodel1 = gapm_sub %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nmodel2 = gapm_sub %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\nmodel3 = gapm_sub %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + \n                           income_levels + four_regions)\n\nfit_stats = data.frame(Model = c(\"Model 1: FLR only\", \"Model 2: FLR + FS\", \"Model 3: FLR + FS + Income + WR\"), \n                       adj_R_2 = c(summary(model1)$adj.r.squared, summary(model2)$adj.r.squared, \n                                   summary(model3)$adj.r.squared), \n                       Cp = c(ols_mallows_cp(model1, model3), ols_mallows_cp(model2, model3), \n                              ols_mallows_cp(model3, model3)), \n                       AIC = c(AIC(model1), AIC(model2), AIC(model3)), \n                       BIC = c(BIC(model1), BIC(model2), BIC(model3)))\n\ncolnames(fit_stats) = c(\"Model\", \"Adj. R-squared\", \"Mallow's Cp\", \"AIC\", \"BIC\")\n\nfit_stats %>% gt() %>% tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","highlight-style":"ayu","output-file":"13_Model_selection.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.25","auto-stretch":true,"title":"Lesson 13: Model/Variable Selection","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"02/24/2025","theme":"../simple_NW.scss","chalkboard":true,"slideNumber":true,"showSlideNumber":"all","width":1955,"height":1100,"footer":"Lesson 13: Model selection"}}},"projectFormats":["html"]}