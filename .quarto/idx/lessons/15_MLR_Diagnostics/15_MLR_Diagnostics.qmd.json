{"title":"Lesson 15: MLR Model Diagnostics","markdown":{"yaml":{"title":"Lesson 15: MLR Model Diagnostics","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"03/05/2025","format":{"revealjs":{"theme":"../simple_NW.scss","chalkboard":true,"slide-number":true,"show-slide-number":"all","width":1955,"height":1100,"footer":"Lesson 15: MLR Diagnostics","highlight-style":"ayu","html-math-method":"mathjax"}},"execute":{"echo":true,"freeze":"auto"}},"headingText":"Learning Objectives","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)    \nlibrary(openintro)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra)   # grid.arrange()\nlibrary(readxl)\nlibrary(describedata) # gladder()\nlibrary(gridExtra)   # grid.arrange()\nlibrary(ggfortify)  # autoplot(model)\nlibrary(gtsummary)\n\nknitr::opts_chunk$set(echo = TRUE,\n                      message = FALSE, warning = FALSE)\nload(here(\"./lessons/15_MLR_Diagnostics/final_mod.rda\"))\ngapm <- read_csv(here(\"./data/lifeexp_femlit_2011.csv\"))\n\n```\n\n\n1.  Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to **evaluate LINE assumptions**, including residual plots and QQ-plots\n\n2.  Apply tools involving standardized residuals, leverage, and Cook's distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to **flag potentially influential points**\n\n3.  Use Variance Inflation Factor (VIF) and it's general form to **detect and correct multicollinearity**\n\n## Regression analysis process\n\n:::::::::::::::::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::::::::::::::::: columns\n:::::: {.column width=\"30%\"}\n::::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::::\n::::::\n\n::: {.column width=\"4%\"}\n:::\n\n:::::: {.column width=\"30%\"}\n::::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::::\n::::::\n\n::: {.column width=\"4%\"}\n:::\n\n:::::: {.column width=\"30%\"}\n::::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::::\n::::::\n:::::::::::::::::\n::::::::::::::::::\n\n:::::::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n:::::: RAP4-cont\n::::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::::\n::::::\n::::::::\n\n## Let's remind ourselves of the final model\n\n::::: columns\n::: {.column width=\"40%\"}\n-   Our **final model** contains\n\n    -   Female Literacy Rate `FLR`\n    -   CO2 Emissions in quartiles `CO2_q`\n    -   Income levels in groups assigned by Gapminder `income_levels1`\n    -   World regions `four_regions`\n    -   Membership of global and economic groups `members_oecd_g77`\n    -   Food Supply `FoodSupplykcPPD`\n    -   Clean Water Supply `WaterSupplePct`\n    -   No interactions\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| code-fold: true\n#| code-summary: \"Display regression table for final model\"\ntbl_regression(\n  final_model, \n  label = list(\n    FemaleLiteracyRate ~ \"Female literacy rate (%)\", \n    CO2_q ~ \"CO2 emissions quartiles\", \n    income_levels1 ~ \"Income levels\", \n    four_regions ~ \"World region\",\n    WaterSourcePrct ~ \"Access to omproved water (%)\",\n    FoodSupplykcPPD ~ \"Food supply (kcal PPD)\",\n    members_oecd_g77 ~ \"Intergovernmental group\"\n    )) %>% \n  as_gt() %>% \n  tab_options(table.font.size = 18) \n```\n:::\n:::::\n\n## It's a lot to visualize\n\n-   Part of the reason why we discussed model diagnostics in SLR was so that we could have accompanying visuals to help us understand\n\n \n\n-   With 7 variables in our final model, it is hard to visualize outliers and influential points\n\n \n\n-   I highly encourage you revisit the SLR lessons ([SLR: Checking model assumptions](../07_SLR_Assumptions/07_SLR_Assumptions.qmd) and [SLR: Diagnostics](../08_SLR_Diagnostics/08_SLR_Diagnostics.qmd) to help understand these notes\n\n## Remember our friend `augment()`?\n\n-   Run `final_model` through `augment()` (`final_model` is input)\n\n    -   So we assigned `final_model` as the output of the `lm()` function\n\n-   Will give us values about each observation in the context of the fitted regression model\n\n    -   cook's distance (`.cooksd`), $\\widehat{Y}_i$ (`.fitted`), leverage (`.hat`), residuals (`.resid`), std residuals (`.std.resid`)\n\n```{r}\naug = augment(final_model)\nhead(aug) %>% relocate(.fitted, .resid, .std.resid, .hat, .cooksd, .after = LifeExpectancyYrs)\n```\n\n[RDocumentation on the `augment()` function.](https://www.rdocumentation.org/packages/broom/versions/1.0.4/topics/augment.lm)\n\n# Learning Objectives\n\n::: lob\n1.  Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to **evaluate LINE assumptions**, including residual plots and QQ-plots\n:::\n\n2.  Apply tools involving standardized residuals, leverage, and Cook's distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to **flag potentially influential points**\n\n3.  Use Variance Inflation Factor (VIF) and it's general form to **detect and correct multicollinearity**\n\n## Summary of the assumptions and their diagnostic tool\n\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n| Assumption           | What needs to hold?                                                                                         | Diagnostic tool                 |\n+:=====================+:============================================================================================================+:================================+\n| Linearity            | -   Relationship between **each** $X$ and $Y$ is linear                                                     | -   Scatterplot of $Y$ vs. $X$  |\n|                      |                                                                                                             |                                 |\n| $\\text{}$            |                                                                                                             | $\\text{}$                       |\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n| Independence         | -   Observations are independent from each other                                                            | -   Study design                |\n|                      |                                                                                                             |                                 |\n| $\\text{}$            |                                                                                                             | $\\text{}$                       |\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n| Normality            | -   Residuals (and thus $Y|X_1, X_2, ..., X_p$) are normally distributed                                    | -   QQ plot of residuals        |\n|                      |                                                                                                             |                                 |\n|                      |                                                                                                             | -   Distribution of residuals   |\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n| Equality of variance | -   Variance of residuals (and thus $Y|X_1, X_2, ..., X_p$) is same across fitted values (homoscedasticity) | -   Residual plot               |\n|                      |                                                                                                             |                                 |\n|                      |                                                                                                             | $\\text{}$                       |\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n: {tbl-colwidths=\"\\[30, 60, 50\\]\"}\n\n## `autoplot()` to examine equality of variance and Normality\n\n```{r}\n#| fig-width: 12\n#| fig-height: 9\n#| fig-align: center\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))\n```\n\n\n## `autoplot()` to examine equality of variance and Normality\n\n::::: columns\n::: {.column width=\"60%\"}\n```{r}\n#| fig-width: 12\n#| fig-height: 9\n#| fig-align: center\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))\n```\n:::\n\n::: {.column width=\"40%\"}\nLooks like 3 obs are flagged:\n\n-   17: Cote d'Ivoire\n-   59: South Africa\n-   61: Kingdom of Eswatini (formerly Swaziland in 2011)\n\nWithout them, QQ-plot and residual plot look good\n\n-   Points on QQ-plot are close to identity line\n-   Residuals have pretty consistent spread across fitted values\n\nBut don't take them out!!!\n\n-   Instead, discuss what may be missing in our regression model that is not capturing the characteristics of these countries\n:::\n:::::\n\n## Poll Everywhere Question 1\n\n# Learning Objectives\n\n1.  Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to **evaluate LINE assumptions**, including residual plots and QQ-plots\n\n::: lob\n2.  Apply tools involving standardized residuals, leverage, and Cook's distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to **flag potentially influential points**\n:::\n\n3.  Use Variance Inflation Factor (VIF) and it's general form to **detect and correct multicollinearity**\n\n## Identifying outliers\n\n:::::::: columns\n:::::: {.column width=\"37%\"}\n::::: theorem\n::: thm-title\nInternally standardized residual\n:::\n\n::: thm-cont\n$$\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n$$\n:::\n:::::\n::::::\n\n::: {.column width=\"63%\"}\n-   We flag an observation if the standardized residual is \"large\"\n\n    -   Different sources will define \"large\" differently\n\n    -   PennState site uses $|r_i| > 3$\n\n    -   `autoplot()` shows the 3 observations with the highest standardized residuals\n\n    -   Other sources use $|r_i| > 2$, which is a little more conservative\n:::\n::::::::\n\n::::: columns\n::: {.column width=\"55%\"}\n \n\n```{r}\n#| fig-height: 4.5\n#| fig-width: 7\n#| fig-align: center\n#| eval: false\nggplot(data = aug) + \n  geom_histogram(aes(x = .std.resid))\n```\n:::\n\n::: {.column width=\"45%\"}\n```{r}\n#| fig-height: 5\n#| fig-width: 7.5\n#| fig-align: center\n#| echo: false\n\nggplot(data = aug) + \n  geom_histogram(aes(x = .std.resid))  +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25))  \n```\n:::\n:::::\n\n## Countries that are outliers ($|r_i| > 3$)\n\n-   We can identify the countries that are outliers\n\n```{r}\n#| echo: false\n\naug = aug %>%\n  mutate(country = gapm2$country) %>% relocate(country)\n```\n\n```{r}\naug %>% relocate(.std.resid, .after = country) %>%\n  filter(abs(.std.resid) > 3) %>% arrange(desc(abs(.std.resid)))\n```\n\n## Countries that are outliers ($|r_i| > 2$)\n\n-   We can identify the countries that are outliers\n\n```{r}\n#| echo: false\n\naug = aug %>%\n  mutate(country = gapm2$country) %>% relocate(country)\n```\n\n```{r}\naug %>% relocate(.std.resid, .after = country) %>%\n  filter(abs(.std.resid) > 2) %>% arrange(desc(abs(.std.resid)))\n```\n\n## Leverage $h_i$\n\n-   Values of leverage are: $0 \\leq h_i \\leq 1$\n\n-   We flag an observation if the leverage is \"high\"\n\n    -   **Only good for SLR:** Some textbooks use $h_i > 4/n$ where $n$ = sample size\n\n    -   **Only good for SLR:** Some people suggest $h_i > 6/n$\n\n    -   **Works for MLR:** $h_i > 3p/n$ where $p$ = number of regression coefficients\n\n```{r}\naug = aug %>% relocate(.hat, .after = FemaleLiteracyRate)\naug %>% arrange(desc(.hat)) %>% head(5)\n```\n\n## Countries with high leverage ($h_i > 3p/n$)\n\n-   We can look at the countries that have high leverage: there are NONE\n\n```{r}\n#| echo: false\n\n# names(gapm)\n# names(aug)\ngapm = gapm %>% mutate(.rownames = 1:n() %>% as.character())\n```\n\n```{r}\nn = nrow(gapm2); p = length(final_model$coefficients) - 1\naug %>% \n  filter(.hat > 3*p/n) %>%\n  arrange(desc(.hat))\n```\n\n## Identifying points with high Cook's distance\n\n::::: columns\n::: column\nThe Cook's distance for the $i^{th}$ observation is\n\n$$d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2$$ where $h_i$ is the leverage and $r_i$ is the studentized residual\n:::\n\n::: column\n-   Another rule for Cook's distance that is not strict:\n    -   Investigate observations that have $d_i > 1$\n-   Cook's distance values are already in the augment tibble: `.cooksd`\n:::\n:::::\n\n-   No countries with high Cook's distance\n\n```{r}\naug = aug %>% relocate(.cooksd, .after = country)\naug %>% arrange(desc(.cooksd)) %>% filter(.cooksd > 1)\n```\n\n## Plotting Cook's Distance\n\n```{r fig.height=4}\n#| fig-align: center\n#| fig-width: 7\n\n# plot(model) shows figures similar to autoplot() but adds Cook's distance\nplot(final_model, which = 4)\n```\n\n- Identify 3 highest Cook's distance: 59, 61, 62 (South Africa, Kingdom of Eswatini, Tajikistan)\n\n## How do we deal with influential points?\n\n-   If an observation is influential, we can **check data errors**:\n\n    -   Was there a data entry or collection problem?\n\n    -   If you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\n-   If an observation is influential, we can **check our model**:\n\n    -   Did you leave out any important predictors?\n\n    -   Should you consider adding some interaction terms?\n\n    -   Is there any nonlinearity that needs to be modeled?\n\n-   Basically, deleting an observation should be justified outside of the numbers!\n\n    -   If it's an honest data point, then it's giving us important information!\n\n-   **Means we will need to discuss the limitations of our model**\n\n    -   For example: Think about measurements that might help explain life expectancy that are NOT in our model\n\n-   [A really well thought out explanation from StackExchange](https://stats.stackexchange.com/questions/81058/how-to-handle-leverage-values)\n\n## Poll Everywhere Question 2\n\n## When we have detected problems in our model...\n\n-   We have talked about influential points\n-   We have talked about identifying issues with our LINE assumptions\n\n \n\nWhat are our options once we have identified issues in our linear regression model?\n\n-   Are we missing a crucial measure in our dataset?\n\n-   Try categorization or transformation (or numeric variables) if there is an issue with linearity or normality\n\n    -   Addressed in model selection\n\n-   Try a weighted least squares approach if unequal variance (oof, not enough time for us to get to)\n\n-   Try a robust estimation procedure if we have a lot of outlier issues (outside scope of class)\n\n# Learning Objectives\n\n1.  Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to **evaluate LINE assumptions**, including residual plots and QQ-plots\n\n2.  Apply tools involving standardized residuals, leverage, and Cook's distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to **flag potentially influential points**\n\n::: lob\n3.  Use Variance Inflation Factor (VIF) and it's general form to **detect and correct multicollinearity**\n:::\n\n## What is multicollinearity? (adapted from parts of [STAT 501 page](https://online.stat.psu.edu/stat501/lesson/12))\n\nSo far, we've been ignoring something very important: multicollinearity\n\n::::::::: columns\n::: {.column width=\"5%\"}\n:::\n\n:::::: {.column width=\"80%\"}\n::::: definition\n::: def-title\nMulticollinearity\n:::\n\n::: def-cont\nTwo or more covariates in a multivariable regression model are *highly* correlated\n:::\n:::::\n::::::\n\n::: {.column width=\"10%\"}\n:::\n:::::::::\n\n-   Types of multicollinearity\n\n    -   **Structural multicollinearity**\n\n        -   Mathematical artifact caused by creating new covariates from other covariates\n\n        -   For example: If we have age, and decide to transform age to include age-squared\n\n            -   Then we have age and age-squared in the model: age-squared is perfectly predicted by age!\n\n    -   **Data-based multicollinearity**\n\n        -   Result of a poorly designed experiment, reliance on purely observational data, or the inability to manipulate the system on which the data are collected\n        -   Or we are using two variables that are practically measuring the same thing\n\n## Poll Everywhere Question 3\n\n## Why is multicollinearity a problem?\n\nIn linear regression, **depending on the other predictors in the model**, the following will change: \n\n-   Estimated regression coefficient of any one variable \n\n    -   Not necessarily bad, but a big change might be an issue\n\n-   Hypothesis tests for any coefficient may yield different conclusions \n\n-   Contribution of any one predictor variable in reducing sum of squared errors\n\n \n\nWhen there is multicollinearity in our model:\n\n-   **Precision** of the estimated regression coefficients or correlated covariates **decreases a lot**\n\n    -   Basically, **standard error increases and confidence intervals get wider**, which means we're not as confident in our estimate anymore\n\n    -   Because highly correlated covariates are not adding much more information, but are constraining our model more\n\n## Did you notice anything about all the consequences of multicollinearity?\n\n-   All consequences relate to estimating a regression coefficient **precisely**\n\n    -   Recall that precision is linked to analysis **goals of association and interpretability**\n    -   See lesson on [Model Selection](../13_Model_selection/13_Model_selection.qmd)\n\n \n\n-   Multicollinearity is *not really an issue* when our **goal is prediction**\n\n    -   Highly correlated covariates/predictors will not hurt our prediction of an outcome\n\n## How do we detect multicollinearity?\n\n::::::::: columns\n::: {.column width=\"50%\"}\n-   **Variance inflation factors (VIF):** quantifies how much the variance of the estimated coefficient for covariate $k$ increases\n\n    -   Increases: from SLR with only covariate $k$ to MLR with all other covariates\n\n \n\n-   General rule of thumb\n    -   $VIF < 4$: Good!\n    -   $4 < VIF < 10$: Warrent investigation (but most people aren't investigating this...)\n\n    -   $VIF > 10$: Requires correction\n\n        -   Influencing regression coefficient estimates\n:::\n\n::: {.column width=\"10%\"}\n:::\n\n:::::: {.column width=\"40%\"}\n::::: proof1\n::: proof-title\nVIF\n:::\n\n::: proof-cont\n$$\nVIF = \\dfrac{1}{1-R_k^2}\n$$\n\n$R_k^2$ is the $R^2$-value obtained by regressing the $k^{th}$ covariate/predictor on the remaining predictors\n:::\n:::::\n::::::\n:::::::::\n\n## Let's apply it to our final model\n\n-   Naive way to calculate this in R:\n\n```{r}\nlibrary(rms)\nrms::vif(final_model)\n```\n\n-   All $VIF < 10$\n\n-   Problem: multi-level covariates (CO2 Emissions and income level) have different VIF's even though they should be considered one variable\n\n## Let's apply it to our final model *correctly* (1/2)\n\n-   Calculate the GVIF and, more importantly, the $GVIF^{1/(2\\cdot df)}$\n\n-   GVIF is the $R^2$-value for regressing a covariate's group indicators on the remaining covariates\n\n    -   Captures the correlation between covariates better\n\n-   $GVIF^{1/(2\\cdot df)}$ helps standardize GVIF based on how many levels each categorical covariate has\n\n    -   I'll refer to this as df-corrected GVIF or standardized GVIF\n\n    -   If continuous covariate, $GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}$\n\n```{r}\nlibrary(car)\ncar::vif(final_model)\n```\n\n## Let's apply it to our final model *correctly* (2/2)\n\n-   If continuous covariate, $GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}$\n\n-   So we can square $GVIF^{1/(2\\cdot df)}$ and set VIF rules\n\n-   OR: we can correct any $GVIF^{1/(2\\cdot df)} > \\sqrt{10} = 3.162$\n\n```{r}\ncar::vif(final_model)\n```\n\n-   All of these covariates are okay! No multicollinearity to correct in this dataset!\n\n## But what if we do need to make corrections for multicollinearity?\n\n-   We have been dealing with **data-based multicollinearity** in our example\n\n-   If we had issues with multicollinearity, then what are our options?\n\n    -   Remove the variable(s) with large VIF\n\n    -   Use expert knowledge in the field to decide\n\n-   If one variable has a large VIF, then there is usually another one or more variables with large VIFs\n\n    -   Basically, all the covariates that are correlated will have large VIFs\n\n-   Example: our two largest GVIFs were for world region and income levels\n\n    -   Hypothetical: their $GVIF^{1/(2\\cdot df)} > 3.162$\n\n    -   Remove one of them\n\n    -   I'm no expert, but from more of a data equity lens, there's a lot of generalizations made about world regions\n\n        -   I think relying on the income level of a country might give us more information as well\n\n## What about structural multicollinearity?\n\n-   **Structural multicollinearity**\n\n    -   Mathematical artifact caused by creating new covariates from other covariates\n\n \n\n-   For example: If we have age, and decide to transform age to include age-squared\n\n    -   Then we have age and age-squared in the model: age-squared is perfectly predicted by age!\n\n    -   By having the untransformed and transformed covariate in the model, they are inherently correlated!\n\n \n\n-   **Best practice to reduce the correlation: center you covariate**\n\n    -   By centering age, we no longer have a one-to-one connection between age and age-squared\n\n    -   If centered at 40yo: a 35 yo and a 45 yo will both have centered age of 5, and age-squared of 25\n\n \n\n-   [Check out the Penn State site](https://online.stat.psu.edu/stat501/lesson/12/12.6) for a work through of an example with VIFs\n\n## Summary of multicollinearity\n\n-   Correlated covariates/predictors will hurt our model's precision and interpretations of coefficients\n\n \n\n-   We need to check for multicollinearity by using VIFs or GVIFs\n\n \n\n-   If $VIF > 10$ or $GVIF^{1/(2\\cdot df)} > 3.162$, we need to do something about the covariates\n\n    -   Data based: remove one the of correlated variables\n\n    -   Structural based: centering usually fixes it\n\n## Regression analysis process\n\n:::::::::::::::::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::::::::::::::::: columns\n:::::: {.column width=\"30%\"}\n::::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::::\n::::::\n\n::: {.column width=\"4%\"}\n:::\n\n:::::: {.column width=\"30%\"}\n::::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::::\n::::::\n\n::: {.column width=\"4%\"}\n:::\n\n:::::: {.column width=\"30%\"}\n::::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::::\n::::::\n:::::::::::::::::\n::::::::::::::::::\n\n:::::::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n:::::: RAP4-cont\n::::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::::\n::::::\n::::::::\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)    \nlibrary(openintro)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra)   # grid.arrange()\nlibrary(readxl)\nlibrary(describedata) # gladder()\nlibrary(gridExtra)   # grid.arrange()\nlibrary(ggfortify)  # autoplot(model)\nlibrary(gtsummary)\n\nknitr::opts_chunk$set(echo = TRUE,\n                      message = FALSE, warning = FALSE)\nload(here(\"./lessons/15_MLR_Diagnostics/final_mod.rda\"))\ngapm <- read_csv(here(\"./data/lifeexp_femlit_2011.csv\"))\n\n```\n\n# Learning Objectives\n\n1.  Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to **evaluate LINE assumptions**, including residual plots and QQ-plots\n\n2.  Apply tools involving standardized residuals, leverage, and Cook's distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to **flag potentially influential points**\n\n3.  Use Variance Inflation Factor (VIF) and it's general form to **detect and correct multicollinearity**\n\n## Regression analysis process\n\n:::::::::::::::::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::::::::::::::::: columns\n:::::: {.column width=\"30%\"}\n::::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::::\n::::::\n\n::: {.column width=\"4%\"}\n:::\n\n:::::: {.column width=\"30%\"}\n::::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::::\n::::::\n\n::: {.column width=\"4%\"}\n:::\n\n:::::: {.column width=\"30%\"}\n::::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::::\n::::::\n:::::::::::::::::\n::::::::::::::::::\n\n:::::::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n:::::: RAP4-cont\n::::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::::\n::::::\n::::::::\n\n## Let's remind ourselves of the final model\n\n::::: columns\n::: {.column width=\"40%\"}\n-   Our **final model** contains\n\n    -   Female Literacy Rate `FLR`\n    -   CO2 Emissions in quartiles `CO2_q`\n    -   Income levels in groups assigned by Gapminder `income_levels1`\n    -   World regions `four_regions`\n    -   Membership of global and economic groups `members_oecd_g77`\n    -   Food Supply `FoodSupplykcPPD`\n    -   Clean Water Supply `WaterSupplePct`\n    -   No interactions\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| code-fold: true\n#| code-summary: \"Display regression table for final model\"\ntbl_regression(\n  final_model, \n  label = list(\n    FemaleLiteracyRate ~ \"Female literacy rate (%)\", \n    CO2_q ~ \"CO2 emissions quartiles\", \n    income_levels1 ~ \"Income levels\", \n    four_regions ~ \"World region\",\n    WaterSourcePrct ~ \"Access to omproved water (%)\",\n    FoodSupplykcPPD ~ \"Food supply (kcal PPD)\",\n    members_oecd_g77 ~ \"Intergovernmental group\"\n    )) %>% \n  as_gt() %>% \n  tab_options(table.font.size = 18) \n```\n:::\n:::::\n\n## It's a lot to visualize\n\n-   Part of the reason why we discussed model diagnostics in SLR was so that we could have accompanying visuals to help us understand\n\n \n\n-   With 7 variables in our final model, it is hard to visualize outliers and influential points\n\n \n\n-   I highly encourage you revisit the SLR lessons ([SLR: Checking model assumptions](../07_SLR_Assumptions/07_SLR_Assumptions.qmd) and [SLR: Diagnostics](../08_SLR_Diagnostics/08_SLR_Diagnostics.qmd) to help understand these notes\n\n## Remember our friend `augment()`?\n\n-   Run `final_model` through `augment()` (`final_model` is input)\n\n    -   So we assigned `final_model` as the output of the `lm()` function\n\n-   Will give us values about each observation in the context of the fitted regression model\n\n    -   cook's distance (`.cooksd`), $\\widehat{Y}_i$ (`.fitted`), leverage (`.hat`), residuals (`.resid`), std residuals (`.std.resid`)\n\n```{r}\naug = augment(final_model)\nhead(aug) %>% relocate(.fitted, .resid, .std.resid, .hat, .cooksd, .after = LifeExpectancyYrs)\n```\n\n[RDocumentation on the `augment()` function.](https://www.rdocumentation.org/packages/broom/versions/1.0.4/topics/augment.lm)\n\n# Learning Objectives\n\n::: lob\n1.  Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to **evaluate LINE assumptions**, including residual plots and QQ-plots\n:::\n\n2.  Apply tools involving standardized residuals, leverage, and Cook's distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to **flag potentially influential points**\n\n3.  Use Variance Inflation Factor (VIF) and it's general form to **detect and correct multicollinearity**\n\n## Summary of the assumptions and their diagnostic tool\n\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n| Assumption           | What needs to hold?                                                                                         | Diagnostic tool                 |\n+:=====================+:============================================================================================================+:================================+\n| Linearity            | -   Relationship between **each** $X$ and $Y$ is linear                                                     | -   Scatterplot of $Y$ vs. $X$  |\n|                      |                                                                                                             |                                 |\n| $\\text{}$            |                                                                                                             | $\\text{}$                       |\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n| Independence         | -   Observations are independent from each other                                                            | -   Study design                |\n|                      |                                                                                                             |                                 |\n| $\\text{}$            |                                                                                                             | $\\text{}$                       |\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n| Normality            | -   Residuals (and thus $Y|X_1, X_2, ..., X_p$) are normally distributed                                    | -   QQ plot of residuals        |\n|                      |                                                                                                             |                                 |\n|                      |                                                                                                             | -   Distribution of residuals   |\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n| Equality of variance | -   Variance of residuals (and thus $Y|X_1, X_2, ..., X_p$) is same across fitted values (homoscedasticity) | -   Residual plot               |\n|                      |                                                                                                             |                                 |\n|                      |                                                                                                             | $\\text{}$                       |\n+----------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------+\n: {tbl-colwidths=\"\\[30, 60, 50\\]\"}\n\n## `autoplot()` to examine equality of variance and Normality\n\n```{r}\n#| fig-width: 12\n#| fig-height: 9\n#| fig-align: center\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))\n```\n\n\n## `autoplot()` to examine equality of variance and Normality\n\n::::: columns\n::: {.column width=\"60%\"}\n```{r}\n#| fig-width: 12\n#| fig-height: 9\n#| fig-align: center\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))\n```\n:::\n\n::: {.column width=\"40%\"}\nLooks like 3 obs are flagged:\n\n-   17: Cote d'Ivoire\n-   59: South Africa\n-   61: Kingdom of Eswatini (formerly Swaziland in 2011)\n\nWithout them, QQ-plot and residual plot look good\n\n-   Points on QQ-plot are close to identity line\n-   Residuals have pretty consistent spread across fitted values\n\nBut don't take them out!!!\n\n-   Instead, discuss what may be missing in our regression model that is not capturing the characteristics of these countries\n:::\n:::::\n\n## Poll Everywhere Question 1\n\n# Learning Objectives\n\n1.  Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to **evaluate LINE assumptions**, including residual plots and QQ-plots\n\n::: lob\n2.  Apply tools involving standardized residuals, leverage, and Cook's distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to **flag potentially influential points**\n:::\n\n3.  Use Variance Inflation Factor (VIF) and it's general form to **detect and correct multicollinearity**\n\n## Identifying outliers\n\n:::::::: columns\n:::::: {.column width=\"37%\"}\n::::: theorem\n::: thm-title\nInternally standardized residual\n:::\n\n::: thm-cont\n$$\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n$$\n:::\n:::::\n::::::\n\n::: {.column width=\"63%\"}\n-   We flag an observation if the standardized residual is \"large\"\n\n    -   Different sources will define \"large\" differently\n\n    -   PennState site uses $|r_i| > 3$\n\n    -   `autoplot()` shows the 3 observations with the highest standardized residuals\n\n    -   Other sources use $|r_i| > 2$, which is a little more conservative\n:::\n::::::::\n\n::::: columns\n::: {.column width=\"55%\"}\n \n\n```{r}\n#| fig-height: 4.5\n#| fig-width: 7\n#| fig-align: center\n#| eval: false\nggplot(data = aug) + \n  geom_histogram(aes(x = .std.resid))\n```\n:::\n\n::: {.column width=\"45%\"}\n```{r}\n#| fig-height: 5\n#| fig-width: 7.5\n#| fig-align: center\n#| echo: false\n\nggplot(data = aug) + \n  geom_histogram(aes(x = .std.resid))  +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25))  \n```\n:::\n:::::\n\n## Countries that are outliers ($|r_i| > 3$)\n\n-   We can identify the countries that are outliers\n\n```{r}\n#| echo: false\n\naug = aug %>%\n  mutate(country = gapm2$country) %>% relocate(country)\n```\n\n```{r}\naug %>% relocate(.std.resid, .after = country) %>%\n  filter(abs(.std.resid) > 3) %>% arrange(desc(abs(.std.resid)))\n```\n\n## Countries that are outliers ($|r_i| > 2$)\n\n-   We can identify the countries that are outliers\n\n```{r}\n#| echo: false\n\naug = aug %>%\n  mutate(country = gapm2$country) %>% relocate(country)\n```\n\n```{r}\naug %>% relocate(.std.resid, .after = country) %>%\n  filter(abs(.std.resid) > 2) %>% arrange(desc(abs(.std.resid)))\n```\n\n## Leverage $h_i$\n\n-   Values of leverage are: $0 \\leq h_i \\leq 1$\n\n-   We flag an observation if the leverage is \"high\"\n\n    -   **Only good for SLR:** Some textbooks use $h_i > 4/n$ where $n$ = sample size\n\n    -   **Only good for SLR:** Some people suggest $h_i > 6/n$\n\n    -   **Works for MLR:** $h_i > 3p/n$ where $p$ = number of regression coefficients\n\n```{r}\naug = aug %>% relocate(.hat, .after = FemaleLiteracyRate)\naug %>% arrange(desc(.hat)) %>% head(5)\n```\n\n## Countries with high leverage ($h_i > 3p/n$)\n\n-   We can look at the countries that have high leverage: there are NONE\n\n```{r}\n#| echo: false\n\n# names(gapm)\n# names(aug)\ngapm = gapm %>% mutate(.rownames = 1:n() %>% as.character())\n```\n\n```{r}\nn = nrow(gapm2); p = length(final_model$coefficients) - 1\naug %>% \n  filter(.hat > 3*p/n) %>%\n  arrange(desc(.hat))\n```\n\n## Identifying points with high Cook's distance\n\n::::: columns\n::: column\nThe Cook's distance for the $i^{th}$ observation is\n\n$$d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2$$ where $h_i$ is the leverage and $r_i$ is the studentized residual\n:::\n\n::: column\n-   Another rule for Cook's distance that is not strict:\n    -   Investigate observations that have $d_i > 1$\n-   Cook's distance values are already in the augment tibble: `.cooksd`\n:::\n:::::\n\n-   No countries with high Cook's distance\n\n```{r}\naug = aug %>% relocate(.cooksd, .after = country)\naug %>% arrange(desc(.cooksd)) %>% filter(.cooksd > 1)\n```\n\n## Plotting Cook's Distance\n\n```{r fig.height=4}\n#| fig-align: center\n#| fig-width: 7\n\n# plot(model) shows figures similar to autoplot() but adds Cook's distance\nplot(final_model, which = 4)\n```\n\n- Identify 3 highest Cook's distance: 59, 61, 62 (South Africa, Kingdom of Eswatini, Tajikistan)\n\n## How do we deal with influential points?\n\n-   If an observation is influential, we can **check data errors**:\n\n    -   Was there a data entry or collection problem?\n\n    -   If you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\n-   If an observation is influential, we can **check our model**:\n\n    -   Did you leave out any important predictors?\n\n    -   Should you consider adding some interaction terms?\n\n    -   Is there any nonlinearity that needs to be modeled?\n\n-   Basically, deleting an observation should be justified outside of the numbers!\n\n    -   If it's an honest data point, then it's giving us important information!\n\n-   **Means we will need to discuss the limitations of our model**\n\n    -   For example: Think about measurements that might help explain life expectancy that are NOT in our model\n\n-   [A really well thought out explanation from StackExchange](https://stats.stackexchange.com/questions/81058/how-to-handle-leverage-values)\n\n## Poll Everywhere Question 2\n\n## When we have detected problems in our model...\n\n-   We have talked about influential points\n-   We have talked about identifying issues with our LINE assumptions\n\n \n\nWhat are our options once we have identified issues in our linear regression model?\n\n-   Are we missing a crucial measure in our dataset?\n\n-   Try categorization or transformation (or numeric variables) if there is an issue with linearity or normality\n\n    -   Addressed in model selection\n\n-   Try a weighted least squares approach if unequal variance (oof, not enough time for us to get to)\n\n-   Try a robust estimation procedure if we have a lot of outlier issues (outside scope of class)\n\n# Learning Objectives\n\n1.  Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to **evaluate LINE assumptions**, including residual plots and QQ-plots\n\n2.  Apply tools involving standardized residuals, leverage, and Cook's distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to **flag potentially influential points**\n\n::: lob\n3.  Use Variance Inflation Factor (VIF) and it's general form to **detect and correct multicollinearity**\n:::\n\n## What is multicollinearity? (adapted from parts of [STAT 501 page](https://online.stat.psu.edu/stat501/lesson/12))\n\nSo far, we've been ignoring something very important: multicollinearity\n\n::::::::: columns\n::: {.column width=\"5%\"}\n:::\n\n:::::: {.column width=\"80%\"}\n::::: definition\n::: def-title\nMulticollinearity\n:::\n\n::: def-cont\nTwo or more covariates in a multivariable regression model are *highly* correlated\n:::\n:::::\n::::::\n\n::: {.column width=\"10%\"}\n:::\n:::::::::\n\n-   Types of multicollinearity\n\n    -   **Structural multicollinearity**\n\n        -   Mathematical artifact caused by creating new covariates from other covariates\n\n        -   For example: If we have age, and decide to transform age to include age-squared\n\n            -   Then we have age and age-squared in the model: age-squared is perfectly predicted by age!\n\n    -   **Data-based multicollinearity**\n\n        -   Result of a poorly designed experiment, reliance on purely observational data, or the inability to manipulate the system on which the data are collected\n        -   Or we are using two variables that are practically measuring the same thing\n\n## Poll Everywhere Question 3\n\n## Why is multicollinearity a problem?\n\nIn linear regression, **depending on the other predictors in the model**, the following will change: \n\n-   Estimated regression coefficient of any one variable \n\n    -   Not necessarily bad, but a big change might be an issue\n\n-   Hypothesis tests for any coefficient may yield different conclusions \n\n-   Contribution of any one predictor variable in reducing sum of squared errors\n\n \n\nWhen there is multicollinearity in our model:\n\n-   **Precision** of the estimated regression coefficients or correlated covariates **decreases a lot**\n\n    -   Basically, **standard error increases and confidence intervals get wider**, which means we're not as confident in our estimate anymore\n\n    -   Because highly correlated covariates are not adding much more information, but are constraining our model more\n\n## Did you notice anything about all the consequences of multicollinearity?\n\n-   All consequences relate to estimating a regression coefficient **precisely**\n\n    -   Recall that precision is linked to analysis **goals of association and interpretability**\n    -   See lesson on [Model Selection](../13_Model_selection/13_Model_selection.qmd)\n\n \n\n-   Multicollinearity is *not really an issue* when our **goal is prediction**\n\n    -   Highly correlated covariates/predictors will not hurt our prediction of an outcome\n\n## How do we detect multicollinearity?\n\n::::::::: columns\n::: {.column width=\"50%\"}\n-   **Variance inflation factors (VIF):** quantifies how much the variance of the estimated coefficient for covariate $k$ increases\n\n    -   Increases: from SLR with only covariate $k$ to MLR with all other covariates\n\n \n\n-   General rule of thumb\n    -   $VIF < 4$: Good!\n    -   $4 < VIF < 10$: Warrent investigation (but most people aren't investigating this...)\n\n    -   $VIF > 10$: Requires correction\n\n        -   Influencing regression coefficient estimates\n:::\n\n::: {.column width=\"10%\"}\n:::\n\n:::::: {.column width=\"40%\"}\n::::: proof1\n::: proof-title\nVIF\n:::\n\n::: proof-cont\n$$\nVIF = \\dfrac{1}{1-R_k^2}\n$$\n\n$R_k^2$ is the $R^2$-value obtained by regressing the $k^{th}$ covariate/predictor on the remaining predictors\n:::\n:::::\n::::::\n:::::::::\n\n## Let's apply it to our final model\n\n-   Naive way to calculate this in R:\n\n```{r}\nlibrary(rms)\nrms::vif(final_model)\n```\n\n-   All $VIF < 10$\n\n-   Problem: multi-level covariates (CO2 Emissions and income level) have different VIF's even though they should be considered one variable\n\n## Let's apply it to our final model *correctly* (1/2)\n\n-   Calculate the GVIF and, more importantly, the $GVIF^{1/(2\\cdot df)}$\n\n-   GVIF is the $R^2$-value for regressing a covariate's group indicators on the remaining covariates\n\n    -   Captures the correlation between covariates better\n\n-   $GVIF^{1/(2\\cdot df)}$ helps standardize GVIF based on how many levels each categorical covariate has\n\n    -   I'll refer to this as df-corrected GVIF or standardized GVIF\n\n    -   If continuous covariate, $GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}$\n\n```{r}\nlibrary(car)\ncar::vif(final_model)\n```\n\n## Let's apply it to our final model *correctly* (2/2)\n\n-   If continuous covariate, $GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}$\n\n-   So we can square $GVIF^{1/(2\\cdot df)}$ and set VIF rules\n\n-   OR: we can correct any $GVIF^{1/(2\\cdot df)} > \\sqrt{10} = 3.162$\n\n```{r}\ncar::vif(final_model)\n```\n\n-   All of these covariates are okay! No multicollinearity to correct in this dataset!\n\n## But what if we do need to make corrections for multicollinearity?\n\n-   We have been dealing with **data-based multicollinearity** in our example\n\n-   If we had issues with multicollinearity, then what are our options?\n\n    -   Remove the variable(s) with large VIF\n\n    -   Use expert knowledge in the field to decide\n\n-   If one variable has a large VIF, then there is usually another one or more variables with large VIFs\n\n    -   Basically, all the covariates that are correlated will have large VIFs\n\n-   Example: our two largest GVIFs were for world region and income levels\n\n    -   Hypothetical: their $GVIF^{1/(2\\cdot df)} > 3.162$\n\n    -   Remove one of them\n\n    -   I'm no expert, but from more of a data equity lens, there's a lot of generalizations made about world regions\n\n        -   I think relying on the income level of a country might give us more information as well\n\n## What about structural multicollinearity?\n\n-   **Structural multicollinearity**\n\n    -   Mathematical artifact caused by creating new covariates from other covariates\n\n \n\n-   For example: If we have age, and decide to transform age to include age-squared\n\n    -   Then we have age and age-squared in the model: age-squared is perfectly predicted by age!\n\n    -   By having the untransformed and transformed covariate in the model, they are inherently correlated!\n\n \n\n-   **Best practice to reduce the correlation: center you covariate**\n\n    -   By centering age, we no longer have a one-to-one connection between age and age-squared\n\n    -   If centered at 40yo: a 35 yo and a 45 yo will both have centered age of 5, and age-squared of 25\n\n \n\n-   [Check out the Penn State site](https://online.stat.psu.edu/stat501/lesson/12/12.6) for a work through of an example with VIFs\n\n## Summary of multicollinearity\n\n-   Correlated covariates/predictors will hurt our model's precision and interpretations of coefficients\n\n \n\n-   We need to check for multicollinearity by using VIFs or GVIFs\n\n \n\n-   If $VIF > 10$ or $GVIF^{1/(2\\cdot df)} > 3.162$, we need to do something about the covariates\n\n    -   Data based: remove one the of correlated variables\n\n    -   Structural based: centering usually fixes it\n\n## Regression analysis process\n\n:::::::::::::::::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::::::::::::::::: columns\n:::::: {.column width=\"30%\"}\n::::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::::\n::::::\n\n::: {.column width=\"4%\"}\n:::\n\n:::::: {.column width=\"30%\"}\n::::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::::\n::::::\n\n::: {.column width=\"4%\"}\n:::\n\n:::::: {.column width=\"30%\"}\n::::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::::\n::::::\n:::::::::::::::::\n::::::::::::::::::\n\n:::::::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n:::::: RAP4-cont\n::::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::::\n::::::\n::::::::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","highlight-style":"ayu","output-file":"15_MLR_Diagnostics.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.25","auto-stretch":true,"title":"Lesson 15: MLR Model Diagnostics","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"03/05/2025","theme":"../simple_NW.scss","chalkboard":true,"slideNumber":true,"showSlideNumber":"all","width":1955,"height":1100,"footer":"Lesson 15: MLR Diagnostics"}}},"projectFormats":["html"]}