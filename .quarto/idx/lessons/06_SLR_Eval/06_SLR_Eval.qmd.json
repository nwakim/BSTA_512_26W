{"title":"Lesson 6: SLR: More inference","markdown":{"yaml":{"title":"Lesson 6: SLR: More inference","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"01/27/2025","categories":["Week 1"],"format":{"revealjs":{"theme":"../simple_NW.scss","chalkboard":true,"slide-number":true,"show-slide-number":"all","width":1955,"height":1100,"footer":"Lesson 6: SLR 3","html-math-method":"mathjax","highlight-style":"ayu"}},"execute":{"echo":true,"freeze":"auto"},"editor":{"markdown":{"wrap":72}}},"headingText":"terminal: for icons","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)    \nlibrary(openintro)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra) # NEW!!!\nlibrary(readxl)\n\n# quarto install extension quarto-ext/fontawesome\n\n# set ggplot theme for slides \ntheme_set(theme_gray(base_size = 22))\n# theme_update(text = element_text(size=16))  # set global text size for ggplots\n```\n\n```{r}\n#| include: false\n#| message: false\n#| warning: false\ngapm1 <- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\") \ngapm <- gapm1 %>% drop_na(LifeExpectancyYrs, FemaleLiteracyRate)\n```\n\n```{r}\n#| include: false\n#| message: false\n#| warning: false\n# Fit regression model:\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n\nreg_table = tidy(model1) %>% gt() %>% \n tab_options(table.font.size = 40) %>%\n fmt_number(decimals = 3)\n```\n\n# Learning Objectives\n\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n    \n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n\n4.  Calculate and interpret the coefficient of determination\n\n\n## So far in our regression example...\n\n::: columns\n::: {.column width=\"49%\"}\n**Lesson 3: SLR 1**\n\n-   Fit regression line\n-   Calculate slope & intercept\n-   Interpret slope & intercept\n\n**Lesson 4: SLR 2**\n\n-   Estimate variance of the residuals\n-   Inference for slope & intercept: CI, p-value\n-   Confidence bands of regression line for mean value of Y\\|X\n\n**Lesson 5: Categorical Covariates**\n\n-   Inference for different categories\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n```\n\n```{=tex}\n\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{LE}} &= 50.9 + 0.232 \\cdot \\text{FLR}\n\\end{aligned}\n```\n:::\n:::\n\n\n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n## Process of regression data analysis\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n\n# Learning Objectives\n\n::: lob\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n:::\n\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n    \n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n\n4.  Calculate and interpret the coefficient of determination\n\n\n## Getting to the F-test\n\nThe F statistic in linear regression is essentially a proportion of the\nvariance explained by the model vs. the variance not explained by the\nmodel\n\n1.  Start with visual of explained vs. unexplained variation\n\n2.  Figure out the mathematical representations of this variation\n\n3.  Look at the ANOVA table to establish key values measuring our\n    variance from our model\n\n4.  Build the F-test\n\n## Explained vs. Unexplained Variation\n\n```{r}\n#| echo: false\n#| fig-align: center\n\nregression_points <- augment(model1)\n# summary(model1)\n# sum(model1$residuals^2)\n\nmy <- mean(gapm$LifeExpectancyYrs, na.rm=T)\n\nggplot(regression_points, \n       aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_hline(yintercept = my, color = \"#A7EA52\", size = 3) +\n  geom_segment(aes(\n    xend = FemaleLiteracyRate, \n    yend = .fitted), \n    alpha = 1, \n    color = \"#4FADF3\", \n    size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F14124\", size=3) +\n  # > Color adjustments made here...\n  geom_point(color = \"black\", size = 4) +  # Color mapped here\n  #scale_color_gradient2(low = \"#213c96\", mid = \"white\", high = \"#F14124\") +  # Colors to use here\n    #guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 4) +\nlabs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Life expectancy vs. female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))  \n```\n\n$$ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\\\\n\\text{Total variation} & = \\text{Residual variation after regression} + \\text{Variation explained by regression}\n\\end{aligned}$$\n\n## More on the equation\n\n::: columns\n::: {.column width=\"40%\"}\n$$Y_i - \\overline{Y} = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})$$\n\n-   $Y_i - \\overline{Y}$ = the deviation of $Y_i$ around the mean\n    $\\overline{Y}$\n    -   (the **total** amount deviation)\n-   $Y_i - \\widehat{Y}_i$ = the deviation of the observation $Y$ around the\n    fitted regression line\n    -   (the amount deviation **unexplained** by the regression at $X_i$\n        ).\n-   $\\widehat{Y_i}- \\overline{Y}$ = the deviation of the fitted value\n    $\\widehat{Y}_i$ around the mean $\\overline{Y}$\n    -   (the amount deviation **explained** by the regression at $X_i$ )\n:::\n\n::: {.column width=\"60%\"}\n   \n\n![](../img_slides/SS.png){fig-align=\"center\"}\n:::\n:::\n\n## Another way of thinking about the different deviations\n\n::: columns\n::: {.column width=\"50%\"}\n![](../img_slides/SSY_SSE_SSR.png){fig-align=\"center\" width=\"1200\"}\n:::\n\n::: {.column width=\"50%\"}\n:::\n:::\n\n## Poll Everywhere Question 1\n\n## How is this actually calculated for our fitted model? (1/2)\n\n$$ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\\\\n\\text{Total variation} & = \\text{Variation explained by regression} + \\text{Residual variation after regression}\n\\end{aligned}$$\n\n$$\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY & = SSR + SSE \n\\end{aligned}$$\n$$\\text{Total Sum of Squares} = \\text{Sum of Squares explained by Regression} + \\text{Sum of Squares due to Error (residuals)}$$\n\n::: columns\n::: {.column width=\"9%\"}\n:::\n\n::: {.column width=\"82%\"}\nANOVA table:\n\n| Variation Source | df    | SS    | MS                      | test statistic        | p-value |\n|------------|------------|------------|------------|------------|------------|\n| Regression       | $1$   | $SSR$ | $MSR = \\frac{SSR}{1}$   | $F = \\frac{MSR}{MSE}$ |         |\n| Error            | $n-2$ | $SSE$ | $MSE = \\frac{SSE}{n-2}$ |                       |         |\n| Total            | $n-1$ | $SSY$ |                         |                       |         |\n:::\n\n::: {.column width=\"9%\"}\n:::\n:::\n\n## How is this actually calculated for our fitted model? (2/2)\n\n$$\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY & = SSR + SSE \n\\end{aligned}$$\n$$\\text{Total Sum of Squares} = \\text{Sum of Squares explained by Regression} + \\text{Sum of Squares due to Error (residuals)}$$\n\n::: columns\n::: {.column width=\"9%\"}\n:::\n\n::: {.column width=\"82%\"}\nANOVA table:\n\n| Variation Source | df    | SS    | MS                      | test statistic        | p-value |\n|------------|------------|------------|------------|------------|------------|\n| Regression       | $1$   | $SSR$ | $MSR = \\frac{SSR}{1}$   | $F = \\frac{MSR}{MSE}$ |         |\n| Error            | $n-2$ | $SSE$ | $MSE = \\frac{SSE}{n-2}$ |                       |         |\n| Total            | $n-1$ | $SSY$ |                         |                       |         |\n:::\n\n::: {.column width=\"9%\"}\n:::\n:::\n\n \n\n::: hl3\n**F-statistic**: Proportion of variation that is explained by the model to variation not explained by the model\n:::\n\n## Analysis of Variance (ANOVA) table in R\n\n```{r}\n# Fit regression model:\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n\nanova(model1)\nanova(model1) %>% tidy() %>% gt() %>%\n   tab_options(table.font.size = 40) %>%\n   fmt_number(decimals = 3)\n```\n\n# Learning Objectives\n\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n\n::: lob\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n:::\n    \n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n\n4.  Calculate and interpret the coefficient of determination\n\n\n## What is the F statistic testing?\n\n$$F = \\frac{MSR}{MSE}$$\n\n-   It can be shown that\n\n$$E(MSE)=\\sigma^2\\ \\text{and}\\ E(MSR) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2$$\n\n-   Recall that $\\sigma^2$ is the variance of the population residuals\n-   Thus if\n    -   $\\beta_1 = 0$, then\n        $F \\approx \\frac{\\widehat{\\sigma}^2}{\\widehat{\\sigma}^2} = 1$\n    -   $\\beta_1 \\neq 0$, then\n        $F \\approx \\frac{\\widehat{\\sigma}^2 + \\widehat{\\beta}_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2}{\\widehat{\\sigma}^2} > 1$\n-   So the $F$ statistic can also be used to test $\\beta_1$\n\n## F-test vs. t-test for the population slope\n\nThe square of a $t$-distribution with $df = \\nu$ is an $F$-distribution\nwith $df = 1, \\nu$\n\n$$T_{\\nu}^2 \\sim F_{1,\\nu}$$\n\n-   We can use either F-test or t-test to run the following hypothesis\n    test:\n\n```{=tex}\n\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\n```\n\n-   Note that the F-test does not support one-sided alternative tests,\n    but the t-test does!\n    \n    - F-test cannot handle alternatives like $\\beta_1 > 0$ nor $\\beta_2 < 0$\n\n## Planting a seed about the F-test\n\nWe can think about the hypothesis test for the slope...\n\n::: columns\n::: {.column width=\"17%\"}\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nNull $H_0$\n:::\n\n::: proof-cont\n$\\beta_1=0$\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nAlternative $H_1$\n:::\n\n::: def-cont\n$\\beta_1\\neq0$\n:::\n:::\n:::\n:::\n\nin a slightly different way...\n\n::: columns\n::: {.column width=\"17%\"}\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nNull model ($\\beta_1=0$)\n:::\n\n::: proof-cont\n-   $Y = \\beta_0 + \\epsilon$\n-   Smaller (reduced) model\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nAlternative model ($\\beta_1\\neq0$)\n:::\n\n::: def-cont\n-   $Y = \\beta_0 + \\beta_1 X + \\epsilon$\n-   Larger (full) model\n:::\n:::\n:::\n:::\n\n-   In multiple linear regression, we can start using this framework to\n    test multiple coefficient parameters at once\n\n    -   Decide whether or not to reject the smaller reduced model in\n        favor of the larger full model\n\n    -   Cannot do this with the t-test when we have multiple coefficients!\n\n## Poll Everywhere Question 2\n\n## F-test: general steps for hypothesis test for population **slope** $\\beta_1$\n\n::: columns\n::: {.column width=\"48%\"}\n::: highlight-container\n::: highlight\n1.  For today's class, we are assuming that we have met the underlying\n    assumptions\n:::\n:::\n\n::: highlight-container\n::: highlight\n2.  State the null hypothesis.\n:::\n:::\n\nOften, we are curious if the coefficient is 0 or not:\n\n```{=tex}\n\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\n```\n::: highlight-container\n::: highlight\n3.  Specify the significance level.\n:::\n:::\n\nOften we use $\\alpha = 0.05$\n\n::: highlight-container\n::: highlight\n4.  Specify the test statistic and its distribution under the null\n:::\n:::\n\nThe test statistic is $F$, and follows an F-distribution with numerator\n$df=1$ and denominator $df=n-2$.\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n::: highlight-container\n::: highlight\n5.  Compute the value of the test statistic\n:::\n:::\n\nThe calculated **test statistic** for $\\widehat\\beta_1$ is\n\n$$F = \\frac{MSR}{MSE}$$\n\n::: highlight-container\n::: highlight\n6.  Calculate the p-value\n:::\n:::\n\nWe are generally calculating: $P(F_{1, n-2} > F)$\n\n::: highlight-container\n::: highlight\n7.  Write conclusion for hypothesis test\n:::\n:::\n\n-   Reject: $P(F_{1, n-2} > F) < \\alpha$\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at\nthe $100\\alpha\\%$ significiance level. There is\n(sufficient/insufficient) evidence that there is significant association\nbetween ($Y$) and ($X$) (p-value = $P(F_{1, n-2} > F)$).\n:::\n:::\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$\n\n-   Steps 1-4 are setting up our hypothesis test: not much change from\n    the general steps\n\n::: highlight-container\n::: highlight\n1.  For today's class, we are assuming that we have met the underlying\n    assumptions\n:::\n:::\n\n::: highlight-container\n::: highlight\n2.  State the null hypothesis.\n:::\n:::\n\nWe are testing if the slope is 0 or not:\n\n```{=tex}\n\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\n```\n::: highlight-container\n::: highlight\n3.  Specify the significance level.\n:::\n:::\n\nOften we use $\\alpha = 0.05$\n\n::: highlight-container\n::: highlight\n4.  Specify the test statistic and its distribution under the null\n:::\n:::\n\n::: columns\n::: {.column width=\"70%\"}\nThe test statistic is $F$, and follows an F-distribution with numerator\n$df=1$ and denominator $df=n-2 = 80-2$.\n:::\n\n::: {.column width=\"30%\"}\n```{r}\nnobs(model1)\n```\n:::\n:::\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (2/4)\n\n::: highlight-container\n::: highlight\n5.  Compute the value of the test statistic\n:::\n:::\n\n```{r}\nanova(model1) %>% tidy() %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\n-   **Option 1:** Calculate the test statistic using the values in the\n    ANOVA table\n\n$$F = \\frac{MSR}{MSE} = \\frac{2052.81}{37.73}=54.414$$\n\n-   **Option 2:** Get the test statistic value (F) from the ANOVA table\n\n::: hl\nI tend to skip this step because I can do it all with step 6\n:::\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (3/4)\n\n::: highlight-container\n::: highlight\n6.  Calculate the p-value\n:::\n:::\n\n-   As per Step 4, test statistic $F$ can be modeled by a\n    $F$-distribution with $df1 = 1$ and $df2 = n-2$.\n\n    -   We had 80 countries' data, so $n=80$\n\n-   **Option 1:** Use `pf()` and our calculated test statistic\n\n```{r}\n# p-value is ALWAYS the right tail for F-test\npf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)\n```\n\n-   **Option 2:** Use the ANOVA table\n\n```{r}\nanova(model1) %>% tidy() %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (4/4)\n\n::: highlight-container\n::: highlight\n7.  Write conclusion for the hypothesis test\n:::\n:::\n\nWe reject the null hypothesis that the slope is 0 at the $5\\%$\nsignificance level. There is sufficient evidence that there is\nsignificant association between female life expectancy and female\nliteracy rates (p-value \\< 0.0001).\n\n## Did you notice anything about the p-value?\n\nThe p-value of the t-test and F-test are the same!!\n\n-   For the t-test:\n\n```{r}\ntidy(model1) %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\n-   For the F-test:\n\n```{r}\nanova(model1) %>% tidy() %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\nThis is true when we use the F-test for a single coefficient!\n\n# Learning Objectives\n\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n\n::: lob\n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n:::\n\n4.  Calculate and interpret the coefficient of determination\n\n\n## Testing association between continuous outcome and categorical variable\n\n- Before we used the F-test (or t-test) to determine association between two continuous variables\n- We CANNOT use the t-test to determine association between a continuous outcome and a multi-level categorical variable\n- We CAN use the F-test to do this!\n\n \n\n- We can use the t-test or F-test for a categorical variable with only 2 levels\n\n## Poll Everywhere Question 3\n    \n## Building a very important toolkit: three types of tests\n\n::: fact\n::: fact-title\nOverall test (in a couple classes)\n:::\n\n::: fact-cont\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n:::\n:::\n\n::: example\n::: ex-title\nTest for addition of a single variable (covariate subset test)\n:::\n\n::: ex-cont\nDoes the addition of one particular covariate add significantly to the prediction of Y achieved by other covariates already present in the model?\n:::\n:::\n\n::: proposition\n::: prop-title\nTest for addition of group of variables (covariate subset test) (in a couple classes)\n:::\n\n::: prop-cont\nDoes the addition of some group of covariates add significantly to the prediction of Y achieved by other covariates already present in the model?\n:::\n:::\n\n## When running a F-test for linear models...\n\n-   We need to define a larger, full model (more parameters)\n-   We need to define a smaller, reduced model (fewer parameters)\n-   Use the F-statistic to decide whether or not we reject the smaller model\n    -   The F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n::: columns\n::: {.column width=\"30%\"}\n \n\n$$\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n$$\n:::\n\n::: {.column width=\"70%\"}\n-   $SSE(R) \\geq SSE(F)$\n-   Numerator measures difference in **unexplained** variation between the models\n    -   Big difference = added parameters greatly reduce the unexplained variation (increase explained variation)\n    -   Smaller difference = added parameters don't reduce the unexplained variation\n-   Take ratio of difference to the unexplained variation in the full model\n:::\n:::\n\n## We can extend our look at the F-test\n\nWe can create a hypothesis test for more than one coefficient at a time...\n\n::: columns\n::: {.column width=\"17%\"}\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nNull $H_0$\n:::\n\n::: proof-cont\n$\\beta_1=\\beta_2=0$\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nAlternative $H_1$\n:::\n\n::: def-cont\n$\\beta_1\\neq0$ and/or $\\beta_2\\neq0$\n:::\n:::\n:::\n:::\n\nin a slightly different way...\n\n::: columns\n::: {.column width=\"17%\"}\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nNull model\n:::\n\n::: proof-cont\n-   $Y = \\beta_0 + \\epsilon$\n-   Smaller (reduced) model\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nAlternative\\* model\n:::\n\n::: def-cont\n-   $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon$\n-   Larger (full) model\n:::\n:::\n:::\n:::\n\n\\*This is **not quite** the alternative, but if we reject the null, then this is the model we move forward with\n\n## Let's say we want to test the association between life expectancy and world region\n\n```{r}\n#| echo: false\nmeans_LE = gapm %>%\n  group_by(four_regions) %>%\n  summarise(mean = mean(LifeExpectancyYrs))\n```\n\n::: columns\n::: {.column width=\"50%\"}\n\n$$\\begin{aligned}\n\\widehat{\\textrm{LE}} = & `r round(means_LE$mean[1], 2)` + `r round(means_LE$mean[2]-means_LE$mean[1], 2)` \\cdot I(\\text{Americas}) + \\\\ &`r round(means_LE$mean[3]-means_LE$mean[1], 2)` \\cdot I(\\text{Asia}) + `r round(means_LE$mean[4]-means_LE$mean[1], 2)` \\cdot I(\\text{Europe}) \\\\\n\\widehat{\\textrm{LE}} = & \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\\\ & \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\n\\end{aligned}$$\n\n- We need to figure out if the model with world region explains significantly more variation than the model without world region!\n:::\n\n::: {.column width=\"50%\"}\n```{r fig.height=8, fig.width=8, warning=F, fig.align='center'}\n#| echo: false\nggplot(gapm, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 22), \n        axis.text = element_text(size = 22), \n        title = element_text(size = 22))\n```\n:::\n:::\n\n\n## F-test: general steps for hypothesis test for $j$-level categorical variable\n\n::: columns\n::: {.column width=\"48%\"}\n::: highlight-container\n::: highlight\n1.  For today's class, we are assuming that we have met the underlying\n    assumptions\n:::\n:::\n\n::: highlight-container\n::: highlight\n2.  State the null hypothesis.\n:::\n:::\n\nOften, we are curious if the coefficient is 0 or not:\n\n```{=tex}\n\\begin{align}\nH_0 :& \\beta_1 = ... = \\beta_j = 0\\\\\n\\text{vs. } H_A:& \\beta_1 \\neq 0 \\text{ and/or } \\\\\n&\\beta_2 \\neq 0 ... \\text{ and/or } \\beta_j \\neq 0\n\\end{align}\n```\n::: highlight-container\n::: highlight\n3.  Specify the significance level.\n:::\n:::\n\nOften we use $\\alpha = 0.05$\n\n::: highlight-container\n::: highlight\n4.  Specify the test statistic and its distribution under the null\n:::\n:::\n\nThe test statistic is $F$, and follows an F-distribution with numerator\n$df=1$ and denominator $df=n-(k+1)$.\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n::: highlight-container\n::: highlight\n5.  Compute the value of the test statistic\n:::\n:::\n\nThe calculated **test statistic** for $\\widehat\\beta_1$ is\n\n$$F = \\frac{MSR}{MSE}$$\n\n::: highlight-container\n::: highlight\n6.  Calculate the p-value\n:::\n:::\n\nWe are generally calculating: $P(F_{1, n-(j+1)} > F)$\n\n::: highlight-container\n::: highlight\n7.  Write conclusion for hypothesis test\n:::\n:::\n\n-   Reject: $P(F_{1, n-(j+1)} > F) < \\alpha$\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at\nthe $100\\alpha\\%$ significiance level. There is\n(sufficient/insufficient) evidence that there is significant association\nbetween ($Y$) and ($X$) (p-value = $P(F_{1, n-(j+1)} > F)$).\n:::\n:::\n\n## Life expectancy example: hypothesis test for world region\n\n-   Steps 1-4 are setting up our hypothesis test: not much change from\n    the general steps\n\n::: highlight-container\n::: highlight\n1.  For today's class, we are assuming that we have met the underlying\n    assumptions\n:::\n:::\n\n::: highlight-container\n::: highlight\n2.  State the null hypothesis.\n:::\n:::\n\nWe are testing if the slope is 0 or not:\n\n```{=tex}\n\\begin{align}\nH_0 :& \\beta_1 = \\beta_2 = \\beta_3 0\\\\\n\\text{vs. } H_A:& \\beta_1 \\neq 0 \\text{ and/or } \\beta_2 \\neq 0 \\text{ and/or } \\beta_3 \\neq 0\n\\end{align}\n```\n::: highlight-container\n::: highlight\n3.  Specify the significance level.\n:::\n:::\n\nOften we use $\\alpha = 0.05$\n\n::: highlight-container\n::: highlight\n4.  Specify the test statistic and its distribution under the null\n:::\n:::\n\n::: columns\n::: {.column width=\"70%\"}\nThe test statistic is $F$, and follows an F-distribution with numerator\n$df=j$ and denominator $df=n-(j+1) = 80-(3+1)$.\n:::\n\n::: {.column width=\"30%\"}\n```{r}\nnobs(model1)\n```\n:::\n:::\n\n## Life expectancy example: hypothesis test for world region (2/4) \n::: highlight-container\n::: highlight\n5.  Compute the value of the test statistic\n:::\n:::\n\n```{r}\nmodel2 <- gapm %>% lm(formula = LifeExpectancyYrs ~ four_regions)\nanova(model2) %>% tidy() %>% gt() %>% tab_options(table.font.size = 40)\n```\n\n```{r}\n#| echo: false\nanova_wr = anova(model2) %>% tidy()\n```\n\n-   **Option 1:** Calculate the test statistic using the values in the\n    ANOVA table\n\n$$F = \\frac{MSR}{MSE} = \\frac{`r anova_wr$meansq[1]`}{`r anova_wr$meansq[2]`} = `r anova_wr$statistic[1]`$$\n\n-   **Option 2:** Get the test statistic value (F) from the ANOVA table\n\n::: hl\nI tend to skip this step because I can do it all with step 6\n:::\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (3/4)\n\n::: highlight-container\n::: highlight\n6.  Calculate the p-value\n:::\n:::\n\n-   As per Step 4, test statistic $F$ can be modeled by a\n    $F$-distribution with $df1 = 3$ and $df2 = n-4$.\n\n    -   We had 80 countries' data, so $n=80$\n\n-   **Option 1:** Use `pf()` and our calculated test statistic\n\n```{r}\n# p-value is ALWAYS the right tail for F-test\npf(33.5331, df1 = 3, df2 = 76, lower.tail = FALSE)\n```\n\n-   **Option 2:** Use the ANOVA table\n\n```{r}\nanova(model2) %>% tidy() %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (4/4)\n\n::: highlight-container\n::: highlight\n7.  Write conclusion for the hypothesis test\n:::\n:::\n\nWe reject the null hypothesis that all three coefficients are equal to 0 at the $5\\%$\nsignificance level. There is sufficient evidence that there is\nassociation between female life expectancy and the country's world region (p-value \\< 0.0001).\n\n\n# Learning Objectives\n\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n    \n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n\n::: lob\n4.  Calculate and interpret the coefficient of determination\n:::\n\n## Correlation coefficient from 511\n\n::: columns\n::: {.column width=\"45%\"}\nCorrelation coefficient $r$ can tell us about the strength of a\nrelationship **between two continuous variables**\n\n-   If $r = -1$, then there is a perfect negative linear relationship\n    between $X$ and $Y$\n\n-   If $r = 1$, then there is a perfect positive linear relationship\n    between $X$ and $Y$\n\n-   If $r = 0$, then there is no linear relationship between $X$ and $Y$\n\nNote: All other values of $r$ tell us that the relationship between $X$\nand $Y$ is not perfect. The closer $r$ is to 0, the weaker the linear\nrelationship.\n:::\n\n::: {.column width=\"55%\"}\n![](../img_slides/corr_coef.png){fig-align=\"center\" width=\"878\"}\n:::\n:::\n\n## Correlation coefficient ($r$ or $R$)\n\n::: columns\n::: column\nThe (Pearson) correlation coefficient $r$ of variables $X$ and $Y$ can\nbe computed using the formula:\n\n$$\\begin{aligned}\nr  & = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\Big(\\sum_{i=1}^n (X_i - \\overline{X})^2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2\\Big)^{1/2}} \\\\\n& = \\frac{SSXY}{\\sqrt{SSX \\cdot SSY}}\n\\end{aligned}$$\n\nwe have the relationship\n\n$$\\widehat{\\beta}_1 = r\\frac{SSY}{SSX},\\ \\ \\text{or},\\ \\  r = \\widehat{\\beta}_1\\frac{SSX}{SSY}$$\n:::\n::: column\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\nmx <- mean(gapm$FemaleLiteracyRate, na.rm=T)\nmy <- mean(gapm$LifeExpectancyYrs, na.rm=T)\n\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_vline(xintercept = mx, color = \"#FF8021\", size = 3) +\n   geom_hline(yintercept = my, color = \"#A7EA52\", size = 3) +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n```\n:::\n:::\n\n## Coefficient of determination: $R^2$\n\nIt can be shown that the square of the correlation coefficient $r$ is\nequal to\n\n$$R^2 = \\frac{SSR}{SSY} = \\frac{SSY - SSE}{SSY}$$\n\n-   $R^2$ is called the **coefficient of determination**.\n-   **Interpretation**: The proportion of variation in the $Y$ values explained by the\n        regression model\n-   $R^2$ measures the strength of the **linear** relationship between $X$\n    and $Y$:\n    -   $R^2 = \\pm 1$: Perfect relationship\n        -   Happens when $SSE = 0$, i.e. no error, all points on the\n            line\n    -   $R^2 = 0$: No relationship\n        -   Happens when $SSY = SSE$, i.e. using the line doesn't not\n            improve model fit over using $\\overline{Y}$ to model the $Y$\n            values.\n\n## Life expectancy example: correlation coeffiicent $r$ and coefficient of determination $R^2$\n\n```{r}\n(r = cor(x = gapm$LifeExpectancyYrs, y = gapm$FemaleLiteracyRate, \n         use =  \"complete.obs\"))\n```\n::: columns\n::: {.column width=\"78%\"}\n```{r}\nsummary(model1) # for R^2 value\n```\n:::\n\n::: {.column width=\"22%\"}\n\n```{r}\nr^2\n```\n   \n\n::: fact\n::: fact-title\nInterpretation\n:::\n\n::: fact-cont\n41.1% of the variation in countries' life expectancy is\nexplained by the linear model with female literacy rate as the\nindependent variable.\n:::\n:::\n:::\n:::\n\n## What does $R^2$ not measure?\n\n::: columns\n::: {.column width=\"37%\"}\n-   $R^2$ is not a measure of the magnitude of the slope of the\n    regression line\n\n    -   Example: can have $R^2 = 1$ for many different slopes!!\n\n-   $R^2$ is not a measure of the appropriateness of the straight-line\n    model\n\n    -   Example: figure\n:::\n\n::: {.column width=\"63%\"}\n![](../img_slides/anscombe.png){fig-align=\"center\"}\n:::\n:::","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)    \nlibrary(openintro)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra) # NEW!!!\nlibrary(readxl)\n\n# terminal: for icons\n# quarto install extension quarto-ext/fontawesome\n\n# set ggplot theme for slides \ntheme_set(theme_gray(base_size = 22))\n# theme_update(text = element_text(size=16))  # set global text size for ggplots\n```\n\n```{r}\n#| include: false\n#| message: false\n#| warning: false\ngapm1 <- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\") \ngapm <- gapm1 %>% drop_na(LifeExpectancyYrs, FemaleLiteracyRate)\n```\n\n```{r}\n#| include: false\n#| message: false\n#| warning: false\n# Fit regression model:\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n\nreg_table = tidy(model1) %>% gt() %>% \n tab_options(table.font.size = 40) %>%\n fmt_number(decimals = 3)\n```\n\n# Learning Objectives\n\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n    \n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n\n4.  Calculate and interpret the coefficient of determination\n\n\n## So far in our regression example...\n\n::: columns\n::: {.column width=\"49%\"}\n**Lesson 3: SLR 1**\n\n-   Fit regression line\n-   Calculate slope & intercept\n-   Interpret slope & intercept\n\n**Lesson 4: SLR 2**\n\n-   Estimate variance of the residuals\n-   Inference for slope & intercept: CI, p-value\n-   Confidence bands of regression line for mean value of Y\\|X\n\n**Lesson 5: Categorical Covariates**\n\n-   Inference for different categories\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n```\n\n```{=tex}\n\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{LE}} &= 50.9 + 0.232 \\cdot \\text{FLR}\n\\end{aligned}\n```\n:::\n:::\n\n\n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n## Process of regression data analysis\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n-   Prediction of new $Y$ given $X$\n:::\n:::\n:::\n:::\n\n\n# Learning Objectives\n\n::: lob\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n:::\n\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n    \n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n\n4.  Calculate and interpret the coefficient of determination\n\n\n## Getting to the F-test\n\nThe F statistic in linear regression is essentially a proportion of the\nvariance explained by the model vs. the variance not explained by the\nmodel\n\n1.  Start with visual of explained vs. unexplained variation\n\n2.  Figure out the mathematical representations of this variation\n\n3.  Look at the ANOVA table to establish key values measuring our\n    variance from our model\n\n4.  Build the F-test\n\n## Explained vs. Unexplained Variation\n\n```{r}\n#| echo: false\n#| fig-align: center\n\nregression_points <- augment(model1)\n# summary(model1)\n# sum(model1$residuals^2)\n\nmy <- mean(gapm$LifeExpectancyYrs, na.rm=T)\n\nggplot(regression_points, \n       aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_hline(yintercept = my, color = \"#A7EA52\", size = 3) +\n  geom_segment(aes(\n    xend = FemaleLiteracyRate, \n    yend = .fitted), \n    alpha = 1, \n    color = \"#4FADF3\", \n    size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F14124\", size=3) +\n  # > Color adjustments made here...\n  geom_point(color = \"black\", size = 4) +  # Color mapped here\n  #scale_color_gradient2(low = \"#213c96\", mid = \"white\", high = \"#F14124\") +  # Colors to use here\n    #guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 4) +\nlabs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Life expectancy vs. female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))  \n```\n\n$$ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\\\\n\\text{Total variation} & = \\text{Residual variation after regression} + \\text{Variation explained by regression}\n\\end{aligned}$$\n\n## More on the equation\n\n::: columns\n::: {.column width=\"40%\"}\n$$Y_i - \\overline{Y} = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})$$\n\n-   $Y_i - \\overline{Y}$ = the deviation of $Y_i$ around the mean\n    $\\overline{Y}$\n    -   (the **total** amount deviation)\n-   $Y_i - \\widehat{Y}_i$ = the deviation of the observation $Y$ around the\n    fitted regression line\n    -   (the amount deviation **unexplained** by the regression at $X_i$\n        ).\n-   $\\widehat{Y_i}- \\overline{Y}$ = the deviation of the fitted value\n    $\\widehat{Y}_i$ around the mean $\\overline{Y}$\n    -   (the amount deviation **explained** by the regression at $X_i$ )\n:::\n\n::: {.column width=\"60%\"}\n   \n\n![](../img_slides/SS.png){fig-align=\"center\"}\n:::\n:::\n\n## Another way of thinking about the different deviations\n\n::: columns\n::: {.column width=\"50%\"}\n![](../img_slides/SSY_SSE_SSR.png){fig-align=\"center\" width=\"1200\"}\n:::\n\n::: {.column width=\"50%\"}\n:::\n:::\n\n## Poll Everywhere Question 1\n\n## How is this actually calculated for our fitted model? (1/2)\n\n$$ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\\\\n\\text{Total variation} & = \\text{Variation explained by regression} + \\text{Residual variation after regression}\n\\end{aligned}$$\n\n$$\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY & = SSR + SSE \n\\end{aligned}$$\n$$\\text{Total Sum of Squares} = \\text{Sum of Squares explained by Regression} + \\text{Sum of Squares due to Error (residuals)}$$\n\n::: columns\n::: {.column width=\"9%\"}\n:::\n\n::: {.column width=\"82%\"}\nANOVA table:\n\n| Variation Source | df    | SS    | MS                      | test statistic        | p-value |\n|------------|------------|------------|------------|------------|------------|\n| Regression       | $1$   | $SSR$ | $MSR = \\frac{SSR}{1}$   | $F = \\frac{MSR}{MSE}$ |         |\n| Error            | $n-2$ | $SSE$ | $MSE = \\frac{SSE}{n-2}$ |                       |         |\n| Total            | $n-1$ | $SSY$ |                         |                       |         |\n:::\n\n::: {.column width=\"9%\"}\n:::\n:::\n\n## How is this actually calculated for our fitted model? (2/2)\n\n$$\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY & = SSR + SSE \n\\end{aligned}$$\n$$\\text{Total Sum of Squares} = \\text{Sum of Squares explained by Regression} + \\text{Sum of Squares due to Error (residuals)}$$\n\n::: columns\n::: {.column width=\"9%\"}\n:::\n\n::: {.column width=\"82%\"}\nANOVA table:\n\n| Variation Source | df    | SS    | MS                      | test statistic        | p-value |\n|------------|------------|------------|------------|------------|------------|\n| Regression       | $1$   | $SSR$ | $MSR = \\frac{SSR}{1}$   | $F = \\frac{MSR}{MSE}$ |         |\n| Error            | $n-2$ | $SSE$ | $MSE = \\frac{SSE}{n-2}$ |                       |         |\n| Total            | $n-1$ | $SSY$ |                         |                       |         |\n:::\n\n::: {.column width=\"9%\"}\n:::\n:::\n\n \n\n::: hl3\n**F-statistic**: Proportion of variation that is explained by the model to variation not explained by the model\n:::\n\n## Analysis of Variance (ANOVA) table in R\n\n```{r}\n# Fit regression model:\nmodel1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n\nanova(model1)\nanova(model1) %>% tidy() %>% gt() %>%\n   tab_options(table.font.size = 40) %>%\n   fmt_number(decimals = 3)\n```\n\n# Learning Objectives\n\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n\n::: lob\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n:::\n    \n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n\n4.  Calculate and interpret the coefficient of determination\n\n\n## What is the F statistic testing?\n\n$$F = \\frac{MSR}{MSE}$$\n\n-   It can be shown that\n\n$$E(MSE)=\\sigma^2\\ \\text{and}\\ E(MSR) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2$$\n\n-   Recall that $\\sigma^2$ is the variance of the population residuals\n-   Thus if\n    -   $\\beta_1 = 0$, then\n        $F \\approx \\frac{\\widehat{\\sigma}^2}{\\widehat{\\sigma}^2} = 1$\n    -   $\\beta_1 \\neq 0$, then\n        $F \\approx \\frac{\\widehat{\\sigma}^2 + \\widehat{\\beta}_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2}{\\widehat{\\sigma}^2} > 1$\n-   So the $F$ statistic can also be used to test $\\beta_1$\n\n## F-test vs. t-test for the population slope\n\nThe square of a $t$-distribution with $df = \\nu$ is an $F$-distribution\nwith $df = 1, \\nu$\n\n$$T_{\\nu}^2 \\sim F_{1,\\nu}$$\n\n-   We can use either F-test or t-test to run the following hypothesis\n    test:\n\n```{=tex}\n\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\n```\n\n-   Note that the F-test does not support one-sided alternative tests,\n    but the t-test does!\n    \n    - F-test cannot handle alternatives like $\\beta_1 > 0$ nor $\\beta_2 < 0$\n\n## Planting a seed about the F-test\n\nWe can think about the hypothesis test for the slope...\n\n::: columns\n::: {.column width=\"17%\"}\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nNull $H_0$\n:::\n\n::: proof-cont\n$\\beta_1=0$\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nAlternative $H_1$\n:::\n\n::: def-cont\n$\\beta_1\\neq0$\n:::\n:::\n:::\n:::\n\nin a slightly different way...\n\n::: columns\n::: {.column width=\"17%\"}\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nNull model ($\\beta_1=0$)\n:::\n\n::: proof-cont\n-   $Y = \\beta_0 + \\epsilon$\n-   Smaller (reduced) model\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nAlternative model ($\\beta_1\\neq0$)\n:::\n\n::: def-cont\n-   $Y = \\beta_0 + \\beta_1 X + \\epsilon$\n-   Larger (full) model\n:::\n:::\n:::\n:::\n\n-   In multiple linear regression, we can start using this framework to\n    test multiple coefficient parameters at once\n\n    -   Decide whether or not to reject the smaller reduced model in\n        favor of the larger full model\n\n    -   Cannot do this with the t-test when we have multiple coefficients!\n\n## Poll Everywhere Question 2\n\n## F-test: general steps for hypothesis test for population **slope** $\\beta_1$\n\n::: columns\n::: {.column width=\"48%\"}\n::: highlight-container\n::: highlight\n1.  For today's class, we are assuming that we have met the underlying\n    assumptions\n:::\n:::\n\n::: highlight-container\n::: highlight\n2.  State the null hypothesis.\n:::\n:::\n\nOften, we are curious if the coefficient is 0 or not:\n\n```{=tex}\n\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\n```\n::: highlight-container\n::: highlight\n3.  Specify the significance level.\n:::\n:::\n\nOften we use $\\alpha = 0.05$\n\n::: highlight-container\n::: highlight\n4.  Specify the test statistic and its distribution under the null\n:::\n:::\n\nThe test statistic is $F$, and follows an F-distribution with numerator\n$df=1$ and denominator $df=n-2$.\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n::: highlight-container\n::: highlight\n5.  Compute the value of the test statistic\n:::\n:::\n\nThe calculated **test statistic** for $\\widehat\\beta_1$ is\n\n$$F = \\frac{MSR}{MSE}$$\n\n::: highlight-container\n::: highlight\n6.  Calculate the p-value\n:::\n:::\n\nWe are generally calculating: $P(F_{1, n-2} > F)$\n\n::: highlight-container\n::: highlight\n7.  Write conclusion for hypothesis test\n:::\n:::\n\n-   Reject: $P(F_{1, n-2} > F) < \\alpha$\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at\nthe $100\\alpha\\%$ significiance level. There is\n(sufficient/insufficient) evidence that there is significant association\nbetween ($Y$) and ($X$) (p-value = $P(F_{1, n-2} > F)$).\n:::\n:::\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$\n\n-   Steps 1-4 are setting up our hypothesis test: not much change from\n    the general steps\n\n::: highlight-container\n::: highlight\n1.  For today's class, we are assuming that we have met the underlying\n    assumptions\n:::\n:::\n\n::: highlight-container\n::: highlight\n2.  State the null hypothesis.\n:::\n:::\n\nWe are testing if the slope is 0 or not:\n\n```{=tex}\n\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\n```\n::: highlight-container\n::: highlight\n3.  Specify the significance level.\n:::\n:::\n\nOften we use $\\alpha = 0.05$\n\n::: highlight-container\n::: highlight\n4.  Specify the test statistic and its distribution under the null\n:::\n:::\n\n::: columns\n::: {.column width=\"70%\"}\nThe test statistic is $F$, and follows an F-distribution with numerator\n$df=1$ and denominator $df=n-2 = 80-2$.\n:::\n\n::: {.column width=\"30%\"}\n```{r}\nnobs(model1)\n```\n:::\n:::\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (2/4)\n\n::: highlight-container\n::: highlight\n5.  Compute the value of the test statistic\n:::\n:::\n\n```{r}\nanova(model1) %>% tidy() %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\n-   **Option 1:** Calculate the test statistic using the values in the\n    ANOVA table\n\n$$F = \\frac{MSR}{MSE} = \\frac{2052.81}{37.73}=54.414$$\n\n-   **Option 2:** Get the test statistic value (F) from the ANOVA table\n\n::: hl\nI tend to skip this step because I can do it all with step 6\n:::\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (3/4)\n\n::: highlight-container\n::: highlight\n6.  Calculate the p-value\n:::\n:::\n\n-   As per Step 4, test statistic $F$ can be modeled by a\n    $F$-distribution with $df1 = 1$ and $df2 = n-2$.\n\n    -   We had 80 countries' data, so $n=80$\n\n-   **Option 1:** Use `pf()` and our calculated test statistic\n\n```{r}\n# p-value is ALWAYS the right tail for F-test\npf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)\n```\n\n-   **Option 2:** Use the ANOVA table\n\n```{r}\nanova(model1) %>% tidy() %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (4/4)\n\n::: highlight-container\n::: highlight\n7.  Write conclusion for the hypothesis test\n:::\n:::\n\nWe reject the null hypothesis that the slope is 0 at the $5\\%$\nsignificance level. There is sufficient evidence that there is\nsignificant association between female life expectancy and female\nliteracy rates (p-value \\< 0.0001).\n\n## Did you notice anything about the p-value?\n\nThe p-value of the t-test and F-test are the same!!\n\n-   For the t-test:\n\n```{r}\ntidy(model1) %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\n-   For the F-test:\n\n```{r}\nanova(model1) %>% tidy() %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\nThis is true when we use the F-test for a single coefficient!\n\n# Learning Objectives\n\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n\n::: lob\n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n:::\n\n4.  Calculate and interpret the coefficient of determination\n\n\n## Testing association between continuous outcome and categorical variable\n\n- Before we used the F-test (or t-test) to determine association between two continuous variables\n- We CANNOT use the t-test to determine association between a continuous outcome and a multi-level categorical variable\n- We CAN use the F-test to do this!\n\n \n\n- We can use the t-test or F-test for a categorical variable with only 2 levels\n\n## Poll Everywhere Question 3\n    \n## Building a very important toolkit: three types of tests\n\n::: fact\n::: fact-title\nOverall test (in a couple classes)\n:::\n\n::: fact-cont\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n:::\n:::\n\n::: example\n::: ex-title\nTest for addition of a single variable (covariate subset test)\n:::\n\n::: ex-cont\nDoes the addition of one particular covariate add significantly to the prediction of Y achieved by other covariates already present in the model?\n:::\n:::\n\n::: proposition\n::: prop-title\nTest for addition of group of variables (covariate subset test) (in a couple classes)\n:::\n\n::: prop-cont\nDoes the addition of some group of covariates add significantly to the prediction of Y achieved by other covariates already present in the model?\n:::\n:::\n\n## When running a F-test for linear models...\n\n-   We need to define a larger, full model (more parameters)\n-   We need to define a smaller, reduced model (fewer parameters)\n-   Use the F-statistic to decide whether or not we reject the smaller model\n    -   The F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n::: columns\n::: {.column width=\"30%\"}\n \n\n$$\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n$$\n:::\n\n::: {.column width=\"70%\"}\n-   $SSE(R) \\geq SSE(F)$\n-   Numerator measures difference in **unexplained** variation between the models\n    -   Big difference = added parameters greatly reduce the unexplained variation (increase explained variation)\n    -   Smaller difference = added parameters don't reduce the unexplained variation\n-   Take ratio of difference to the unexplained variation in the full model\n:::\n:::\n\n## We can extend our look at the F-test\n\nWe can create a hypothesis test for more than one coefficient at a time...\n\n::: columns\n::: {.column width=\"17%\"}\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nNull $H_0$\n:::\n\n::: proof-cont\n$\\beta_1=\\beta_2=0$\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nAlternative $H_1$\n:::\n\n::: def-cont\n$\\beta_1\\neq0$ and/or $\\beta_2\\neq0$\n:::\n:::\n:::\n:::\n\nin a slightly different way...\n\n::: columns\n::: {.column width=\"17%\"}\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nNull model\n:::\n\n::: proof-cont\n-   $Y = \\beta_0 + \\epsilon$\n-   Smaller (reduced) model\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nAlternative\\* model\n:::\n\n::: def-cont\n-   $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon$\n-   Larger (full) model\n:::\n:::\n:::\n:::\n\n\\*This is **not quite** the alternative, but if we reject the null, then this is the model we move forward with\n\n## Let's say we want to test the association between life expectancy and world region\n\n```{r}\n#| echo: false\nmeans_LE = gapm %>%\n  group_by(four_regions) %>%\n  summarise(mean = mean(LifeExpectancyYrs))\n```\n\n::: columns\n::: {.column width=\"50%\"}\n\n$$\\begin{aligned}\n\\widehat{\\textrm{LE}} = & `r round(means_LE$mean[1], 2)` + `r round(means_LE$mean[2]-means_LE$mean[1], 2)` \\cdot I(\\text{Americas}) + \\\\ &`r round(means_LE$mean[3]-means_LE$mean[1], 2)` \\cdot I(\\text{Asia}) + `r round(means_LE$mean[4]-means_LE$mean[1], 2)` \\cdot I(\\text{Europe}) \\\\\n\\widehat{\\textrm{LE}} = & \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\\\ & \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\n\\end{aligned}$$\n\n- We need to figure out if the model with world region explains significantly more variation than the model without world region!\n:::\n\n::: {.column width=\"50%\"}\n```{r fig.height=8, fig.width=8, warning=F, fig.align='center'}\n#| echo: false\nggplot(gapm, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 22), \n        axis.text = element_text(size = 22), \n        title = element_text(size = 22))\n```\n:::\n:::\n\n\n## F-test: general steps for hypothesis test for $j$-level categorical variable\n\n::: columns\n::: {.column width=\"48%\"}\n::: highlight-container\n::: highlight\n1.  For today's class, we are assuming that we have met the underlying\n    assumptions\n:::\n:::\n\n::: highlight-container\n::: highlight\n2.  State the null hypothesis.\n:::\n:::\n\nOften, we are curious if the coefficient is 0 or not:\n\n```{=tex}\n\\begin{align}\nH_0 :& \\beta_1 = ... = \\beta_j = 0\\\\\n\\text{vs. } H_A:& \\beta_1 \\neq 0 \\text{ and/or } \\\\\n&\\beta_2 \\neq 0 ... \\text{ and/or } \\beta_j \\neq 0\n\\end{align}\n```\n::: highlight-container\n::: highlight\n3.  Specify the significance level.\n:::\n:::\n\nOften we use $\\alpha = 0.05$\n\n::: highlight-container\n::: highlight\n4.  Specify the test statistic and its distribution under the null\n:::\n:::\n\nThe test statistic is $F$, and follows an F-distribution with numerator\n$df=1$ and denominator $df=n-(k+1)$.\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n::: highlight-container\n::: highlight\n5.  Compute the value of the test statistic\n:::\n:::\n\nThe calculated **test statistic** for $\\widehat\\beta_1$ is\n\n$$F = \\frac{MSR}{MSE}$$\n\n::: highlight-container\n::: highlight\n6.  Calculate the p-value\n:::\n:::\n\nWe are generally calculating: $P(F_{1, n-(j+1)} > F)$\n\n::: highlight-container\n::: highlight\n7.  Write conclusion for hypothesis test\n:::\n:::\n\n-   Reject: $P(F_{1, n-(j+1)} > F) < \\alpha$\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at\nthe $100\\alpha\\%$ significiance level. There is\n(sufficient/insufficient) evidence that there is significant association\nbetween ($Y$) and ($X$) (p-value = $P(F_{1, n-(j+1)} > F)$).\n:::\n:::\n\n## Life expectancy example: hypothesis test for world region\n\n-   Steps 1-4 are setting up our hypothesis test: not much change from\n    the general steps\n\n::: highlight-container\n::: highlight\n1.  For today's class, we are assuming that we have met the underlying\n    assumptions\n:::\n:::\n\n::: highlight-container\n::: highlight\n2.  State the null hypothesis.\n:::\n:::\n\nWe are testing if the slope is 0 or not:\n\n```{=tex}\n\\begin{align}\nH_0 :& \\beta_1 = \\beta_2 = \\beta_3 0\\\\\n\\text{vs. } H_A:& \\beta_1 \\neq 0 \\text{ and/or } \\beta_2 \\neq 0 \\text{ and/or } \\beta_3 \\neq 0\n\\end{align}\n```\n::: highlight-container\n::: highlight\n3.  Specify the significance level.\n:::\n:::\n\nOften we use $\\alpha = 0.05$\n\n::: highlight-container\n::: highlight\n4.  Specify the test statistic and its distribution under the null\n:::\n:::\n\n::: columns\n::: {.column width=\"70%\"}\nThe test statistic is $F$, and follows an F-distribution with numerator\n$df=j$ and denominator $df=n-(j+1) = 80-(3+1)$.\n:::\n\n::: {.column width=\"30%\"}\n```{r}\nnobs(model1)\n```\n:::\n:::\n\n## Life expectancy example: hypothesis test for world region (2/4) \n::: highlight-container\n::: highlight\n5.  Compute the value of the test statistic\n:::\n:::\n\n```{r}\nmodel2 <- gapm %>% lm(formula = LifeExpectancyYrs ~ four_regions)\nanova(model2) %>% tidy() %>% gt() %>% tab_options(table.font.size = 40)\n```\n\n```{r}\n#| echo: false\nanova_wr = anova(model2) %>% tidy()\n```\n\n-   **Option 1:** Calculate the test statistic using the values in the\n    ANOVA table\n\n$$F = \\frac{MSR}{MSE} = \\frac{`r anova_wr$meansq[1]`}{`r anova_wr$meansq[2]`} = `r anova_wr$statistic[1]`$$\n\n-   **Option 2:** Get the test statistic value (F) from the ANOVA table\n\n::: hl\nI tend to skip this step because I can do it all with step 6\n:::\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (3/4)\n\n::: highlight-container\n::: highlight\n6.  Calculate the p-value\n:::\n:::\n\n-   As per Step 4, test statistic $F$ can be modeled by a\n    $F$-distribution with $df1 = 3$ and $df2 = n-4$.\n\n    -   We had 80 countries' data, so $n=80$\n\n-   **Option 1:** Use `pf()` and our calculated test statistic\n\n```{r}\n# p-value is ALWAYS the right tail for F-test\npf(33.5331, df1 = 3, df2 = 76, lower.tail = FALSE)\n```\n\n-   **Option 2:** Use the ANOVA table\n\n```{r}\nanova(model2) %>% tidy() %>% gt() %>%\n  tab_options(table.font.size = 40)\n```\n\n## Life expectancy example: hypothesis test for population **slope** $\\beta_1$ (4/4)\n\n::: highlight-container\n::: highlight\n7.  Write conclusion for the hypothesis test\n:::\n:::\n\nWe reject the null hypothesis that all three coefficients are equal to 0 at the $5\\%$\nsignificance level. There is sufficient evidence that there is\nassociation between female life expectancy and the country's world region (p-value \\< 0.0001).\n\n\n# Learning Objectives\n\n1.  Identify different sources of variation in an Analysis of Variance\n    (ANOVA) table\n\n2.  Using the F-test, determine if there is enough evidence that\n    population slope $\\beta_1$ is not 0\n    \n3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable\n\n::: lob\n4.  Calculate and interpret the coefficient of determination\n:::\n\n## Correlation coefficient from 511\n\n::: columns\n::: {.column width=\"45%\"}\nCorrelation coefficient $r$ can tell us about the strength of a\nrelationship **between two continuous variables**\n\n-   If $r = -1$, then there is a perfect negative linear relationship\n    between $X$ and $Y$\n\n-   If $r = 1$, then there is a perfect positive linear relationship\n    between $X$ and $Y$\n\n-   If $r = 0$, then there is no linear relationship between $X$ and $Y$\n\nNote: All other values of $r$ tell us that the relationship between $X$\nand $Y$ is not perfect. The closer $r$ is to 0, the weaker the linear\nrelationship.\n:::\n\n::: {.column width=\"55%\"}\n![](../img_slides/corr_coef.png){fig-align=\"center\" width=\"878\"}\n:::\n:::\n\n## Correlation coefficient ($r$ or $R$)\n\n::: columns\n::: column\nThe (Pearson) correlation coefficient $r$ of variables $X$ and $Y$ can\nbe computed using the formula:\n\n$$\\begin{aligned}\nr  & = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\Big(\\sum_{i=1}^n (X_i - \\overline{X})^2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2\\Big)^{1/2}} \\\\\n& = \\frac{SSXY}{\\sqrt{SSX \\cdot SSY}}\n\\end{aligned}$$\n\nwe have the relationship\n\n$$\\widehat{\\beta}_1 = r\\frac{SSY}{SSX},\\ \\ \\text{or},\\ \\  r = \\widehat{\\beta}_1\\frac{SSX}{SSY}$$\n:::\n::: column\n```{r}\n#| fig-height: 8\n#| fig-width: 11\n#| echo: false\n\nmx <- mean(gapm$FemaleLiteracyRate, na.rm=T)\nmy <- mean(gapm$LifeExpectancyYrs, na.rm=T)\n\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_vline(xintercept = mx, color = \"#FF8021\", size = 3) +\n   geom_hline(yintercept = my, color = \"#A7EA52\", size = 3) +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n```\n:::\n:::\n\n## Coefficient of determination: $R^2$\n\nIt can be shown that the square of the correlation coefficient $r$ is\nequal to\n\n$$R^2 = \\frac{SSR}{SSY} = \\frac{SSY - SSE}{SSY}$$\n\n-   $R^2$ is called the **coefficient of determination**.\n-   **Interpretation**: The proportion of variation in the $Y$ values explained by the\n        regression model\n-   $R^2$ measures the strength of the **linear** relationship between $X$\n    and $Y$:\n    -   $R^2 = \\pm 1$: Perfect relationship\n        -   Happens when $SSE = 0$, i.e. no error, all points on the\n            line\n    -   $R^2 = 0$: No relationship\n        -   Happens when $SSY = SSE$, i.e. using the line doesn't not\n            improve model fit over using $\\overline{Y}$ to model the $Y$\n            values.\n\n## Life expectancy example: correlation coeffiicent $r$ and coefficient of determination $R^2$\n\n```{r}\n(r = cor(x = gapm$LifeExpectancyYrs, y = gapm$FemaleLiteracyRate, \n         use =  \"complete.obs\"))\n```\n::: columns\n::: {.column width=\"78%\"}\n```{r}\nsummary(model1) # for R^2 value\n```\n:::\n\n::: {.column width=\"22%\"}\n\n```{r}\nr^2\n```\n   \n\n::: fact\n::: fact-title\nInterpretation\n:::\n\n::: fact-cont\n41.1% of the variation in countries' life expectancy is\nexplained by the linear model with female literacy rate as the\nindependent variable.\n:::\n:::\n:::\n:::\n\n## What does $R^2$ not measure?\n\n::: columns\n::: {.column width=\"37%\"}\n-   $R^2$ is not a measure of the magnitude of the slope of the\n    regression line\n\n    -   Example: can have $R^2 = 1$ for many different slopes!!\n\n-   $R^2$ is not a measure of the appropriateness of the straight-line\n    model\n\n    -   Example: figure\n:::\n\n::: {.column width=\"63%\"}\n![](../img_slides/anscombe.png){fig-align=\"center\"}\n:::\n:::"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","highlight-style":"ayu","output-file":"06_SLR_Eval.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.25","auto-stretch":true,"title":"Lesson 6: SLR: More inference","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"01/27/2025","categories":["Week 1"],"editor":{"markdown":{"wrap":72}},"theme":"../simple_NW.scss","chalkboard":true,"slideNumber":true,"showSlideNumber":"all","width":1955,"height":1100,"footer":"Lesson 6: SLR 3"}}},"projectFormats":["html"]}