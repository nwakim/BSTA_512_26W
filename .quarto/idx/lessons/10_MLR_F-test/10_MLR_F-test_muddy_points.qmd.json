{"title":"Muddy Points","markdown":{"yaml":{"title":"Muddy Points","subtitle":"Lesson 10: MLR: Using the F-test","date-modified":"today","format":{"html":{"link-external-newwindow":true}},"editor_options":{"chunk_output_type":"console"}},"headingText":"Muddy Points from Winter 2025","containsRefs":false,"markdown":"\n\n\n### 1. The muddiest point I had was what is the point or purpose of trying to the \"explain the unexplained.\" What does it mean to greatly reduce the unexplained variation or increase the explained variation?\n\nWhen we measure an outcome, there is variance in the measurement. Is that variance just because there's measurement error? Of is that variance coming from some underlying characteristics that might change the value for different observations. \n\nIn reference to our lab with IAT score, we saw a large spread of IAT scores among participants. What causes the variance across people? Is it there attitudes or beliefs? Is it their other characteristics? If there is a set pattern connecting something like age to IAT score, then age explains some of the variance in IAT score. Maybe a higher IAT score is more likely to map to a lower age. Maybe a lower IAT score is morely to map to an older age. Some of the variance of IAT score is explained by age. Thus, age will reduce the unexplained variation in IAT score. If age reduces a decent amount of the IAT score's unexplained variation then they will have a significant association.\n\n### 2. I was wondering why the SSR is always smaller than the SSY.\n\nWe can start with the equation: $$SSY = SSR + SSE$$.\n\nWe can also subtract $SSE$ from each side to get: $$SSR = SSY - SSE$$\n\nIf $SSE \\geq 0$, then $$SSR \\leq SSY$$ \n\n### 3. Someday I want to understand the theory behind degrees of freedom :(\n\n[From Wikipedia:](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) The number of independent pieces of information that go into the estimate of a parameter is called the degrees of freedom.\n\nIndependent pieces of information will be the observations with outcomes! So we can start with the sample size!\n\nAny given estimate, like a coefficient or the F-statistic, will be calculated using the independent information (observations) and constructed from the dependent parameters in a model. \n\nThus, degrees of freedom will be the total number of independent observations subtracting the number of parameters you are estimating (# coefficients in the model - 1)\n\n#### Why \"# coefficients in the model - 1\"?\n\nLet's say we have 6 coefficients in a model. All these coefficients are estimated. However, they are work together in one model, so if we know 5 out of 6 coefficient estimates, then we know the 6th. Thus, we only have 5 parameter estimates that limit our degreeds of freedom. \n\n### 4. The purpose of using a reduced model vs a larger (full) model.\n\nWe want to compare different models to see if the larger model is a \"better fit\" for our outcome. By \"better fit\" I mean that it explains significantly more variation than the reduced model. \n\nUsing reduced and full models helps us remember that we are comparing models, not just testing ceofficients. \n\n## Muddy Points from Winter 2024\n\n### 1. We keep getting back to $\\widehat{Y}$, $Y_i$, and $\\overline{Y}$ and their relationship to the population parameter estimates. Can you clarify this?\n\nI think it'll be helpful to use the dataset I created from our quiz. I still think this relationship is best communicated with simple linear regression. What you didn't see on the quiz was that I simulated the data:\n\n```{r}\nset.seed(444) # Set the seed so that every time I run this I get the same results\nx = runif(n=200, min = 40, max = 85) # I am sampling 200 points from a uniform distribution with minimum value 40 and maximum value 85\ny = rnorm(n=200, 215 - 0.85*x, 13) # Then I can construct my y-observations based on x. Notice that 215 is the true, underlying intercept and -0.85 is the true underlying slope\ndf = data.frame(Age = x, HR = y) # Then I combine these into a dataframe\n```\n\nThen we can look at the scatterplot:\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1, colour=\"#F14124\") +\n  labs(x = \"Age (years)\", \n       y = \"Peak exercise heart rate (bpm)\",\n       title = \"Peak exercise heart rate vs. Age\") +\n    theme(axis.title = element_text(size = 11), \n        axis.text = element_text(size = 11), \n        title = element_text(size = 11))\n```\n\n-   Each point represents an observation $(X_i, Y_i)$. That is where we get $Y_i$ from\n\n-   The red line represents $\\widehat{Y}$. We can look at each $\\widehat{Y}|X$, so we look at the expected $Y$ at a specific age like 70 years old.\n\n-   Now we need to find $\\overline{Y}$. This does not take $X$ into account. So we can look at the observed $Y$'s and find the mean\n\n    ```{r}\n    #| fig-align: center\n    #| warning: false\n    #| fig-width: 4\n    #| fig-height: 3\n\n    ggplot(df, aes(HR)) + geom_histogram()\n    mean(df$HR)\n    ```\n\nThen we can draw a line on the scatterplot for $\\overline{Y}$:\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1, colour=\"#F14124\") +\n  labs(x = \"Age (years)\", \n       y = \"Peak exercise heart rate (bpm)\",\n       title = \"Peak exercise heart rate vs. Age\") +\n    theme(axis.title = element_text(size = 11), \n        axis.text = element_text(size = 11), \n        title = element_text(size = 11)) +\n  geom_hline(yintercept = mean(df$HR), linewidth = 1, colour=\"green\")\n```\n\nWhen we talk about SSY (total variation), we can think of the histogram of the Y's\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\nggplot(df, aes(HR)) + geom_histogram() + xlim(100, 225)\n```\n\nThen the total variation of these observed values is related to the $\\sum_{i=1}^n (Y_i - \\overline{Y})^2$. Let's plot $Y_i - \\overline{Y}$:\n\n```{r}\n#| echo: false\n#| message: false\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\nlibrary(readxl)\n```\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\n\ndf = df %>% mutate(y_center = HR - mean(HR))\nggplot(df, aes(y_center)) + geom_histogram() + xlim(-60, 50)+ylim(0, 35)\n```\n\nHowever, we can fit a regression line to show the relationship between Y and X. For every observation $X_i$ there is a specific $\\widehat{Y}$ from the regression line. So if we take the difference between the mean Y and the fitted Y, then we get the variation that is explained by the regression.\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\nmod1 = lm(HR ~ Age, data = df)\naug1 = augment(mod1)\ndf = df %>% mutate(fitted_y = aug1$.fitted, \n                   diff_mean_fit = fitted_y - mean(HR))\nggplot(df, aes(diff_mean_fit)) + geom_histogram() + xlim(-60, 50)+ylim(0, 35)\n```\n\nIn the plot above, there is variation! And it means that some of the variation in the plot of Y alone is actually coming from this variation explained by the regression model!!\n\nBut there is left over variation that is not explained by the model... What is that? It's related to our residuals: $\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i$\n\nSo we'll calculate the residuals (or more appropriately, use the calculation of the residuals that R gave us)\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\nmod1 = lm(HR ~ Age, data = df)\naug1 = augment(mod1)\ndf = df %>% mutate(diff_y_fitted = aug1$.resid)\nggplot(df, aes(diff_y_fitted)) + geom_histogram() + xlim(-60, 50) +ylim(0, 35)\n```\n\nOur aim in regression (through ordinary least squares) is to minimize the variance in the above plot. The more variance our model can explain, the less variance in the residuals. In SLR, we can only explain so much variance with a single predictor. As we include more predictors in our model, the model has the opportunity to explain even MORE variance.\n\n### 2. I feel that I am understanding and beginning to memorize the \"processes\" but failing to understand the \"how/when/why\" we apply certain models. Like, if you let me loose into the world tomorrow, I feel I would not be able to implement anything that I have learned thus far out in the wild.\n\nI feel you. Today was meant to establish the tools and the process for the hypothesis tests. In the next few classes we are going to shape the how/when/why. There's so many that we can't cover them in one class. So I thought it would be best to introduce the tools on their own, and then discuss how we use each one.\n\n### 3. I'm struggling with how to use the SSE, SSR, and SSY graphs but I think I just need to spend more time with them.\n\nWe're going to keep talking about this! I think I have a good visual explanation that will help us connect some ideas!\n\n### 4. Using R to conduct each F-test\n\nI show this a little bit for the overall test, but let's explicitly write it out for the example with the group of covariates.\n\nLet's say our proposed model is:\n\n$$\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\beta_3 WS + \\epsilon$$\n\nAnd we want to see if it fits the data significantly better than:\n\n$$LE = \\beta_0 + \\beta_1 FLR + \\epsilon$$\n\nWe need to fit both models first:\n\n```{r}\n#| echo: false\ngapm <- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \ngapm_sub2 = gapm %>%\n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, \n          FoodSupplykcPPD, WaterSourcePrct)\n```\n\n```{r}\n# Reduced model\nmod_red3 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub2)\n\n# Full model\nmod_full3 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct,\n               data = gapm_sub2)\n```\n\nAnd then all we need to do is call each model into the `anova()` function! The order of the models in the function will not matter for the F-test.\n\n```{r}\nanova(mod_red3, mod_full3) %>% tidy() %>% gt() %>% fmt_number(decimals = 2)\n```\n\nAnd then we have all the information we need for our conclusion! Because the statistic is 25.47 and its corresponding p-value \\< 0.001, we reject the null!\n\n### 5. Calculating the F-statistic\n\nWe will not need to know exactly how to calculate the F-statistic! We just need to understand the context of the calculation. With that poll everywhere question, I just wanted you to have a moment to interact with the fact the the F-test is measuring the difference in the sum of squares of the error.\n\n### 6. Why is SSE of the reduced model always greater than or equal to SSE of the full model?\n\nThe more variables that we have in the model, the more variation in our outcome we can explain. The worst case scenario is that the added variable does not help explain variation, then we are left with the same SSE as a model without the variable. If the variable add any information about our outcome, then we decrease the SSE.\n\n### 7. How many variables would be too many to include in MLR? What if they all help explain the regression?\n\nWhat a fun question! The maximum number of variables that you can have in a model is the number of observations that you have. At that point, you can have a variable that is an indicator of each observation. Basically, a singular mapping from each individual observation to its respective outcome.\n\nAnd this will explain the variation in Y perfectly!! But it won't illuminate useful information. We cannot generalize it to the population. So with the number of variables that we include in the model, we need to balance generalizability and reduction of error.\n\n","srcMarkdownNoYaml":"\n\n## Muddy Points from Winter 2025\n\n### 1. The muddiest point I had was what is the point or purpose of trying to the \"explain the unexplained.\" What does it mean to greatly reduce the unexplained variation or increase the explained variation?\n\nWhen we measure an outcome, there is variance in the measurement. Is that variance just because there's measurement error? Of is that variance coming from some underlying characteristics that might change the value for different observations. \n\nIn reference to our lab with IAT score, we saw a large spread of IAT scores among participants. What causes the variance across people? Is it there attitudes or beliefs? Is it their other characteristics? If there is a set pattern connecting something like age to IAT score, then age explains some of the variance in IAT score. Maybe a higher IAT score is more likely to map to a lower age. Maybe a lower IAT score is morely to map to an older age. Some of the variance of IAT score is explained by age. Thus, age will reduce the unexplained variation in IAT score. If age reduces a decent amount of the IAT score's unexplained variation then they will have a significant association.\n\n### 2. I was wondering why the SSR is always smaller than the SSY.\n\nWe can start with the equation: $$SSY = SSR + SSE$$.\n\nWe can also subtract $SSE$ from each side to get: $$SSR = SSY - SSE$$\n\nIf $SSE \\geq 0$, then $$SSR \\leq SSY$$ \n\n### 3. Someday I want to understand the theory behind degrees of freedom :(\n\n[From Wikipedia:](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) The number of independent pieces of information that go into the estimate of a parameter is called the degrees of freedom.\n\nIndependent pieces of information will be the observations with outcomes! So we can start with the sample size!\n\nAny given estimate, like a coefficient or the F-statistic, will be calculated using the independent information (observations) and constructed from the dependent parameters in a model. \n\nThus, degrees of freedom will be the total number of independent observations subtracting the number of parameters you are estimating (# coefficients in the model - 1)\n\n#### Why \"# coefficients in the model - 1\"?\n\nLet's say we have 6 coefficients in a model. All these coefficients are estimated. However, they are work together in one model, so if we know 5 out of 6 coefficient estimates, then we know the 6th. Thus, we only have 5 parameter estimates that limit our degreeds of freedom. \n\n### 4. The purpose of using a reduced model vs a larger (full) model.\n\nWe want to compare different models to see if the larger model is a \"better fit\" for our outcome. By \"better fit\" I mean that it explains significantly more variation than the reduced model. \n\nUsing reduced and full models helps us remember that we are comparing models, not just testing ceofficients. \n\n## Muddy Points from Winter 2024\n\n### 1. We keep getting back to $\\widehat{Y}$, $Y_i$, and $\\overline{Y}$ and their relationship to the population parameter estimates. Can you clarify this?\n\nI think it'll be helpful to use the dataset I created from our quiz. I still think this relationship is best communicated with simple linear regression. What you didn't see on the quiz was that I simulated the data:\n\n```{r}\nset.seed(444) # Set the seed so that every time I run this I get the same results\nx = runif(n=200, min = 40, max = 85) # I am sampling 200 points from a uniform distribution with minimum value 40 and maximum value 85\ny = rnorm(n=200, 215 - 0.85*x, 13) # Then I can construct my y-observations based on x. Notice that 215 is the true, underlying intercept and -0.85 is the true underlying slope\ndf = data.frame(Age = x, HR = y) # Then I combine these into a dataframe\n```\n\nThen we can look at the scatterplot:\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1, colour=\"#F14124\") +\n  labs(x = \"Age (years)\", \n       y = \"Peak exercise heart rate (bpm)\",\n       title = \"Peak exercise heart rate vs. Age\") +\n    theme(axis.title = element_text(size = 11), \n        axis.text = element_text(size = 11), \n        title = element_text(size = 11))\n```\n\n-   Each point represents an observation $(X_i, Y_i)$. That is where we get $Y_i$ from\n\n-   The red line represents $\\widehat{Y}$. We can look at each $\\widehat{Y}|X$, so we look at the expected $Y$ at a specific age like 70 years old.\n\n-   Now we need to find $\\overline{Y}$. This does not take $X$ into account. So we can look at the observed $Y$'s and find the mean\n\n    ```{r}\n    #| fig-align: center\n    #| warning: false\n    #| fig-width: 4\n    #| fig-height: 3\n\n    ggplot(df, aes(HR)) + geom_histogram()\n    mean(df$HR)\n    ```\n\nThen we can draw a line on the scatterplot for $\\overline{Y}$:\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1, colour=\"#F14124\") +\n  labs(x = \"Age (years)\", \n       y = \"Peak exercise heart rate (bpm)\",\n       title = \"Peak exercise heart rate vs. Age\") +\n    theme(axis.title = element_text(size = 11), \n        axis.text = element_text(size = 11), \n        title = element_text(size = 11)) +\n  geom_hline(yintercept = mean(df$HR), linewidth = 1, colour=\"green\")\n```\n\nWhen we talk about SSY (total variation), we can think of the histogram of the Y's\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\nggplot(df, aes(HR)) + geom_histogram() + xlim(100, 225)\n```\n\nThen the total variation of these observed values is related to the $\\sum_{i=1}^n (Y_i - \\overline{Y})^2$. Let's plot $Y_i - \\overline{Y}$:\n\n```{r}\n#| echo: false\n#| message: false\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\nlibrary(readxl)\n```\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\n\ndf = df %>% mutate(y_center = HR - mean(HR))\nggplot(df, aes(y_center)) + geom_histogram() + xlim(-60, 50)+ylim(0, 35)\n```\n\nHowever, we can fit a regression line to show the relationship between Y and X. For every observation $X_i$ there is a specific $\\widehat{Y}$ from the regression line. So if we take the difference between the mean Y and the fitted Y, then we get the variation that is explained by the regression.\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\nmod1 = lm(HR ~ Age, data = df)\naug1 = augment(mod1)\ndf = df %>% mutate(fitted_y = aug1$.fitted, \n                   diff_mean_fit = fitted_y - mean(HR))\nggplot(df, aes(diff_mean_fit)) + geom_histogram() + xlim(-60, 50)+ylim(0, 35)\n```\n\nIn the plot above, there is variation! And it means that some of the variation in the plot of Y alone is actually coming from this variation explained by the regression model!!\n\nBut there is left over variation that is not explained by the model... What is that? It's related to our residuals: $\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i$\n\nSo we'll calculate the residuals (or more appropriately, use the calculation of the residuals that R gave us)\n\n```{r}\n#| fig-align: center\n#| warning: false\n#| fig-width: 4\n#| fig-height: 3\nmod1 = lm(HR ~ Age, data = df)\naug1 = augment(mod1)\ndf = df %>% mutate(diff_y_fitted = aug1$.resid)\nggplot(df, aes(diff_y_fitted)) + geom_histogram() + xlim(-60, 50) +ylim(0, 35)\n```\n\nOur aim in regression (through ordinary least squares) is to minimize the variance in the above plot. The more variance our model can explain, the less variance in the residuals. In SLR, we can only explain so much variance with a single predictor. As we include more predictors in our model, the model has the opportunity to explain even MORE variance.\n\n### 2. I feel that I am understanding and beginning to memorize the \"processes\" but failing to understand the \"how/when/why\" we apply certain models. Like, if you let me loose into the world tomorrow, I feel I would not be able to implement anything that I have learned thus far out in the wild.\n\nI feel you. Today was meant to establish the tools and the process for the hypothesis tests. In the next few classes we are going to shape the how/when/why. There's so many that we can't cover them in one class. So I thought it would be best to introduce the tools on their own, and then discuss how we use each one.\n\n### 3. I'm struggling with how to use the SSE, SSR, and SSY graphs but I think I just need to spend more time with them.\n\nWe're going to keep talking about this! I think I have a good visual explanation that will help us connect some ideas!\n\n### 4. Using R to conduct each F-test\n\nI show this a little bit for the overall test, but let's explicitly write it out for the example with the group of covariates.\n\nLet's say our proposed model is:\n\n$$\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\beta_3 WS + \\epsilon$$\n\nAnd we want to see if it fits the data significantly better than:\n\n$$LE = \\beta_0 + \\beta_1 FLR + \\epsilon$$\n\nWe need to fit both models first:\n\n```{r}\n#| echo: false\ngapm <- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \ngapm_sub2 = gapm %>%\n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, \n          FoodSupplykcPPD, WaterSourcePrct)\n```\n\n```{r}\n# Reduced model\nmod_red3 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub2)\n\n# Full model\nmod_full3 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct,\n               data = gapm_sub2)\n```\n\nAnd then all we need to do is call each model into the `anova()` function! The order of the models in the function will not matter for the F-test.\n\n```{r}\nanova(mod_red3, mod_full3) %>% tidy() %>% gt() %>% fmt_number(decimals = 2)\n```\n\nAnd then we have all the information we need for our conclusion! Because the statistic is 25.47 and its corresponding p-value \\< 0.001, we reject the null!\n\n### 5. Calculating the F-statistic\n\nWe will not need to know exactly how to calculate the F-statistic! We just need to understand the context of the calculation. With that poll everywhere question, I just wanted you to have a moment to interact with the fact the the F-test is measuring the difference in the sum of squares of the error.\n\n### 6. Why is SSE of the reduced model always greater than or equal to SSE of the full model?\n\nThe more variables that we have in the model, the more variation in our outcome we can explain. The worst case scenario is that the added variable does not help explain variation, then we are left with the same SSE as a model without the variable. If the variable add any information about our outcome, then we decrease the SSE.\n\n### 7. How many variables would be too many to include in MLR? What if they all help explain the regression?\n\nWhat a fun question! The maximum number of variables that you can have in a model is the number of observations that you have. At that point, you can have a variable that is an indicator of each observation. Basically, a singular mapping from each individual observation to its respective outcome.\n\nAnd this will explain the variation in Y perfectly!! But it won't illuminate useful information. We cannot generalize it to the population. So with the number of variables that we include in the model, we need to balance generalizability and reduction of error.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","title-prefix":"","toc":true,"output-file":"10_MLR_F-test_muddy_points.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":["../../sandstone_NW.scss"],"title":"Muddy Points","subtitle":"Lesson 10: MLR: Using the F-test","date-modified":"today","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}