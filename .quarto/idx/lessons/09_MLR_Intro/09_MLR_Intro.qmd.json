{"title":"Lesson 9: Introduction to Multiple Linear Regression (MLR)","markdown":{"yaml":{"title":"Lesson 9: Introduction to Multiple Linear Regression (MLR)","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"02/05/2025","format":{"revealjs":{"theme":"../simple_NW.scss","chalkboard":true,"slide-number":true,"show-slide-number":"all","width":1955,"height":1100,"footer":"Lesson 9: MLR Intro","html-math-method":"mathjax","highlight-style":"ayu"}},"execute":{"echo":true,"freeze":"auto"}},"headingText":"new packages","containsRefs":false,"markdown":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\nlibrary(readxl)\n#----------\n# install.packages(\"describedata\")\nlibrary(describedata) # gladder()\nlibrary(gridExtra)   # grid.arrange()\nlibrary(ggfortify)  # autoplot(model)\n# New Day 6\nlibrary(gtsummary)\n\n# New Day 7\nlibrary(plotly) # for plot_ly() command\nlibrary(GGally) # for ggpairs() command \nlibrary(ggiraphExtra)   # for ggPredict() command\n\n```\n\n# Learning Objectives\n\n1.  Understand the population multiple linear regression model through equations and visuals.\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n\n\n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n\n## Reminder of what we learned in the context of SLR\n\n-   SLR helped us establish the foundation for a lot of regression\n\n    -   But we do not usually use SLR in analysis\n\n**What did we learn in SLR??**\n\n::: columns\n::: {.column width=\"31%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Ordinary least squares (OLS)\n-   `lm()` function in R\n:::\n:::\n:::\n\n::: {.column width=\"36%\"}\n::: RAP4\n::: RAP4-title\nModel Use\n:::\n\n::: RAP4-cont\n-   Inference for variance of residuals\n-   Hypothesis testing for coefficients\n-   Interpreting population coefficient estimates\n-   Calculated the expected mean for specific $X$ values\n-   Interpreted coefficient of determination\n:::\n:::\n:::\n\n::: {.column width=\"32%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation/Diagnostics\n:::\n\n::: RAP3-cont\n-   LINE Assumptions\n-   Influential points\n-   Data Transformations\n:::\n:::\n:::\n:::\n\n## Let's map that to our regression analysis process\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n:::\n:::\n:::\n:::\n\n## \n\n![](../img_slides/all_mod_wrong.jpg){fig-align=\"center\"}\n\n# Learning Objectives\n\n::: lob\n1.  Understand the population multiple linear regression model through equations and visuals.\n:::\n\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n\n## Simple Linear Regression vs. Multiple Linear Regression\n\n::: columns\n::: {.column width=\"50%\"}\n::: L1\nSimple Linear Regression\n:::\n\n \n\n::: L2\nWe use **one predictor** to try to explain the variance of the outcome\n\n$$\nY = \\beta_0 + \\beta_1 X + \\epsilon\n$$\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: E1\nMultiple Linear Regression\n:::\n\n \n\n::: E2\nWe use **multiple predictors** to try to explain the variance of the outcome\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_{k}X_{k}+ \\epsilon\n$$\n:::\n\n \n\n-   Has $k+1$ total coefficients (including intercept) for $k$ predictors/covariates\n-   Sometimes referred to as ***multivariable*** linear regression, but *never multivariate*\n:::\n:::\n\n \n\n-   The models have similar \"LINE\" assumptions and follow the same general diagnostic procedure\n\n## Population multiple regression model\n\n::: heq\n$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon$$\n:::\n\nor on the individual (observation) level:\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}+ \\ldots + \\beta_k x_{ik} + \\epsilon_i,\\ \\ \\text{for}\\ i = 1, 2, \\ldots, n$$\n\n::: columns\n::: column\n#### Observable sample data\n\n-   $Y$ is our dependent variable\n\n    -   Aka outcome or response variable\n\n-   $X_1, X_2, \\ldots, X_k$ are our $k$ independent variables\n\n    -   Aka predictors or covariates\n:::\n\n::: column\n#### Unobservable population parameters\n\n-   $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k$ are **unknown** population **parameters**\n    -   From our sample, we find the population parameter estimates: $\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k$\n-   $\\epsilon$ is the random error\n    -   And is still normally distributed\n    -   $\\epsilon \\sim N(0, \\sigma^2)$ where $\\sigma^2$ is the population parameter of the variance\n:::\n:::\n\n## Going back to our life expectancy example\n\n-   Let's say many other variables were measured for each country, including food supply\n\n    -   [**Food Supply**](https://www.fao.org/faostat/en/#home) (kilocalories per person per day, kc PPD): the average kilocalories consumed by a person each day.\n\n-   In SLR, we only had one predictor and one outcome in the model:\n\n    -   Outcome: [**Life expectancy**](https://www.gapminder.org/data/documentation/gd004/) = the average number of years a newborn child would live if current mortality patterns were to stay the same.\n\n    -   Predictor: [**Adult literacy rate**](http://data.uis.unesco.org/)(predictor) is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n \n\n-   Do we think adult female literacy rate is going to explain a lot of the variance of life expectancy between countries?\n\n## Loading the data\n\n```{r}\n# Load the data - update code if the file is not in the same location\n# on your computer\ngapm <- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\ngapm_sub <- gapm %>% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)\n  # above drops rows with NAs in any of the three variables\n\nglimpse(gapm_sub)\n```\n\n## Can we improve our model by adding food supply as a covariate?\n\n::: columns\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"80%\"}\n::: definition\n::: def-title\nSimple linear regression population model\n:::\n\n::: def-cont\n$$\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\epsilon\n\\end{aligned}$$\n:::\n:::\n\n::: fact\n::: fact-title\nMultiple linear regression population model (with added Food Supply)\n:::\n\n::: fact-cont\n$$\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\n\\end{aligned}$$\n:::\n:::\n:::\n\n::: {.column width=\"10%\"}\n:::\n:::\n\n## Visualize relationship between life expectancy, female literacy rate, and food supply\n\n```{r fig.width=8, fig.height=3}\n#| echo: false\n\nplot_LE_FLR <- ggplot(gapm_sub, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"#F14124\") +\n  labs(title = \"Life expectancy vs.\\n female literacy rate\", \n       x = \"Female Litaracy Rate (%)\", \n       y = \"Life Expectancy (yrs)\") +\n  theme(axis.title = element_text(size = 10), \n        axis.text = element_text(size = 10), \n        title = element_text(size = 10))\n\nplot_LE_FS <- ggplot(gapm_sub, aes(x = FoodSupplykcPPD,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"#F14124\") +\n  labs(title = \"Life expectancy vs. \\n food supply\", \n       x = \"Food supply (kcal PPD)\", \n       y = \"Life Expectancy (yrs)\") +\n  theme(axis.title = element_text(size = 10), \n        axis.text = element_text(size = 10), \n        title = element_text(size = 10))\n\nplot_FLR_FS <- ggplot(gapm_sub, aes(x = FoodSupplykcPPD,\n                 y = FemaleLiteracyRate)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"#F14124\") +\n  labs(title = \"Female literacy rate vs. \\n food supply\", \n       y = \"Female Litaracy Rate (%)\", \n       x = \"Food supply (kcal PPD)\") +\n  theme(axis.title = element_text(size = 10), \n        axis.text = element_text(size = 10), \n        title = element_text(size = 10))\n\ngrid.arrange(plot_LE_FLR, plot_LE_FS, plot_FLR_FS,\n             nrow = 1)\n```\n\n## Visualize relationship in 3-D\n\n::: columns\n::: {.column width=\"25%\"}\n:::\n\n::: {.column width=\"50%\"}\n```{r fig.height=8, fig.width=8, warning=F}\n#| echo: false\n#| fig-align: center\n# plotly package is require for plot_ly function, which is loaded at beginning of Rmd \n# z = response variable\n# x & y are predictor variables\n\ndim3_scatter <- plot_ly(gapm_sub, \n                       x = ~FemaleLiteracyRate, \n                       y = ~FoodSupplykcPPD, \n                       z = ~LifeExpectancyYrs) %>%\n  add_markers() %>%\n  layout(scene = list(xaxis = list(title = 'Female literacy rate'),\n                     yaxis = list(title = 'Food supply (kc PPD)'),\n                     zaxis = list(title = 'Life expectancy')))\ndim3_scatter\n```\n:::\n\n::: {.column width=\"25%\"}\n:::\n:::\n\n## Poll Everywhere Question 1\n\n# Learning Objectives\n\n1.  Understand the population multiple linear regression model through equations and visuals.\n\n::: lob\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n:::\n\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n\n## How do we fit a multiple linear regression model in R?\n\nNew population model for example:\n\n$$\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon$$\n\n```{r}\n# Fit regression model:\nmr1 <- gapm_sub %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\ntidy(mr1, conf.int=T) %>% gt() %>% tab_options(table.font.size = 35) %>% fmt_number(decimals = 3)\n```\n\n```{r}\n#| echo: false\n\nmr1_tidy = tidy(mr1, conf.int=T)\n```\n\nFitted multiple regression model:\n\n$$\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR} \n+ `r round(mr1_tidy$estimate[3], 3)`\\ \\text{FS}\n\\end{aligned}$$\n\n## Don't forget `summary()` to extract information!\n\n```{r}\nsummary(mr1)\n```\n\n## Visualize the fitted multiple regression model\n\n::: columns\n::: {.column width=\"40%\"}\n-   The fitted model equation $$\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X_1 + \\widehat{\\beta}_2 \\cdot X_2$$ has three variables ($Y, X_1,$ and $X_2$) and thus we need 3 dimensions to plot it\n\n \n\n-   Instead of a regression line, we get a **regression plane**\n    -   *See code in `.qmd`- file. I hid it from view in the html file.*\n:::\n\n::: {.column width=\"60%\"}\n```{r fig.height=10, fig.width=10, echo=F, warning=FALSE}\n#| fig-align: center\n# setup hideous grid required by plotly\nmr1 <- gapm_sub %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\nx_grid <- seq(from = min(gapm_sub$FemaleLiteracyRate, na.rm = TRUE), \n              to = max(gapm_sub$FemaleLiteracyRate, na.rm = TRUE), \n              length = 100)\ny_grid <- seq(from = min(gapm_sub$FoodSupplykcPPD, na.rm = TRUE), \n              to = max(gapm_sub$FoodSupplykcPPD, na.rm = TRUE), \n              length = 200)\nz_grid <- expand.grid(x_grid, y_grid) %>%\n  as_tibble() %>%\n  rename(x_grid = Var1, y_grid = Var2) %>%\n  mutate(z = coef(mr1)[1] + coef(mr1)[2]*x_grid + coef(mr1)[3]*y_grid) %>%\n  .[[\"z\"]] %>%\n  matrix(nrow=length(x_grid)) %>%\n  t()\n\n# Plot points\nplot_ly() %>%\n  add_markers(\n    x = gapm_sub$FemaleLiteracyRate,\n    y = gapm_sub$FoodSupplykcPPD,\n    z = gapm_sub$LifeExpectancyYrs,\n    hoverinfo = 'text',\n    text = ~paste(\"x1 - Female literacy rate: \",\n                  gapm_sub$FemaleLiteracyRate,\n                  \"</br> x2 - Food supply (kc PPD): \",\n                  gapm_sub$FoodSupplykcPPD,\n                  \"</br> y - Life expectancy: \",\n                  gapm_sub$LifeExpectancyYrs)\n  ) %>%\n  # Label axes\n  layout(\n    scene = list(\n      xaxis = list(title = \"x1 - Female literacy rate\"),\n      yaxis = list(title = \"x2 - Food supply (kc PPD)\"),\n      zaxis = list(title = \"y - Life expectancy\")\n    )\n  ) %>%\n  # Add regression plane\n  add_surface(\n    x = x_grid,\n    y = y_grid,\n    z = z_grid\n  )\n\n```\n:::\n:::\n\n## Regression lines for varying values of food supply\n\n$$\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR} \n+ `r round(mr1_tidy$estimate[3], 3)`\\ \\text{FS}\n\\end{aligned}$$\n\n::: columns\n::: {.column width=\"35%\"}\n-   Note: when the food supply is held constant but the female literacy rate varies...\n    -   then the outcome values change along a **line**\n-   Different values of food supply give different lines\n    -   The intercepts change, but\n    -   the slopes stay the same (parallel lines)\n:::\n\n::: {.column width=\"65%\"}\n```{r}\n#| fig-width: 6\n#| fig-height: 3.5\n#| fig-align: center\n\n(mr1_2d = ggPredict(mr1, interactive = T))\n```\n:::\n:::\n\n## How do we calculate the regression line for 3000 kc PPD food supply?\n\n::: columns\n::: {.column width=\"40%\"}\n \n\n$$\\begin{aligned}\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR} \n+ `r round(mr1_tidy$estimate[3], 3)`\\ \\text{FS}\\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR} \n+ `r round(mr1_tidy$estimate[3], 3)`\\cdot 3000\\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR}\n+ `r round(mr1_tidy$estimate[3]*3000, 3)` \\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1] + mr1_tidy$estimate[3]*3000, 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR}\n\\end{aligned}$$\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| fig-width: 6\n#| fig-height: 4\n#| fig-align: center\n\n(mr1_2d = ggPredict(mr1, interactive = T))\n```\n:::\n:::\n\n## Poll Everwhere Question 2\n\n# Learning Objectives\n\n1.  Understand the population multiple linear regression model through equations and visuals.\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n\n::: lob\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n:::\n\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n\n## Interpreting the estimated population coefficients\n\n-   For a population model: $$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon$$\n    -   Where $X_1$ and $X_2$ are continuous variables\n    -   No need to specify $Y$ because it required to be continuous in linear regression\n\n::: columns\n::: {.column width=\"33%\"}\n::: fact\n::: fact-title\nGeneral interpretation for $\\widehat{\\beta}_0$\n:::\n\n::: fact-cont\nThe expected $Y$-variable is ($\\widehat\\beta_0$ units) when the $X_1$-variable is 0 $X_1$-units and $X_2$-variable is 0 $X_1$-units (95% CI: LB, UB).\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nGeneral interpretation for $\\widehat{\\beta}_1$\n:::\n\n::: def-cont\nFor every increase of 1 $X_1$-unit in the $X_1$-variable, adjusting/controlling for $X_2$-variable, there is an expected increase/decrease of $|\\widehat\\beta_1|$ units in the $Y$-variable (95%: LB, UB).\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nGeneral interpretation for $\\widehat{\\beta}_2$\n:::\n\n::: proof-cont\nFor every increase of 1 $X_2$-unit in the $X_2$-variable, adjusting/controlling for $X_1$-variable, there is an expected increase/decrease of $|\\widehat\\beta_2|$ units in the $Y$-variable (95%: LB, UB).\n:::\n:::\n:::\n:::\n\n\n## Interpreting the estimated population coefficient: $\\widehat{\\beta}_0$ {#color-slide1}\n\n```{css, echo=FALSE}\n#color-slide1 h2 {\n color: #34AC8B;\n}\n```\n\n- For an estimated model: $\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_2 + \\widehat{\\beta}_2 X_2$\n\n$$\\begin{aligned}\n\\widehat{Y} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 0 + \\widehat{\\beta}_2 0\\\\\n\\widehat{Y} = &\\widehat{\\beta}_0 \\\\\n\\end{aligned}$$\n\n**Interpretation:** The expected $Y$-variable is ($\\widehat\\beta_0$ units) when the $X_1$-variable is 0 $X_1$-units and $X_2$-variable is 0 $X_1$-units (95% CI: LB, UB).\n\n## Interpreting the estimated population coefficient: $\\widehat{\\beta}_1$ {#color-slide2}\n\n```{css, echo=FALSE}\n#color-slide2 h2 {\n color: #4FADF3;\n}\n```\n\n- We will use: $x_{1a}$ and $x_{1b} = x_{1a} + 1$, with the implication that $\\Delta{x_1} = x_{1b} - x_{1a} = 1$\n\n- Our goal is to get to a statement with $\\widehat{\\beta}_1$ alone:\n\n$$\\begin{aligned}\n\\widehat{Y}|x_{1a} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{1b} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = & \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\bigg] - \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\bigg] \\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1 x_{1b} - \\widehat{\\beta}_1 x_{1a}\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1 (x_{1b} - x_{1a})\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1\\\\\n\\end{aligned}$$\n\n**Interpretation:** For every increase of 1 $X_1$-unit in the $X_1$-variable, adjusting/controlling for $X_2$-variable, there is an expected increase/decrease of $|\\widehat\\beta_1|$ units in the $Y$-variable (95%: LB, UB).\n\n\n## Interpreting the estimated population coefficient: $\\widehat{\\beta}_2$ {#color-slide3}\n\n```{css, echo=FALSE}\n#color-slide3 h2 {\n color: #FF8021;\n}\n```\n\n- We can so the same for $X_2$: $x_{2a}$ and $x_{2b} = x_{2a} + 1$, with the implication that $\\Delta{x_2} = x_{2b} - x_{2a} = 1$\n\n- Our goal is to get to a statement with $\\widehat{\\beta}_1$ alone:\n\n$$\\begin{aligned}\n\\widehat{Y}|x_{2a} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{2b} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = & \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 x_{2b} \\bigg] - \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 x_{2a}\\bigg] \\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2 x_{2b} - \\widehat{\\beta}_2 x_{2a}\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2 (x_{2b} - x_{2a})\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2\\\\\n\\end{aligned}$$\n\n**Interpretation:** For every increase of 1 $X_2$-unit in the $X_2$-variable, adjusting/controlling for $X_1$-variable, there is an expected increase/decrease of $|\\widehat\\beta_2|$ units in the $Y$-variable (95%: LB, UB).\n\n\n## Poll Everywhere Question 3\n\n## Getting these interpretations from our regression table\n\nWe fit the regression model in R and printed the regression table:\n\n```{r}\nmr1 <- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n```\n\n```{r}\n#| echo: false\ntidy(mr1, conf.int=T) %>% gt() %>% tab_options(table.font.size = 30) %>% fmt_number(decimals = 3)\n```\n\nFitted multiple regression model: $\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}$\n\n::: columns\n::: {.column width=\"33%\"}\n::: fact\n::: fact-title\nInterpretation for $\\widehat{\\beta}_0$\n:::\n\n::: fact-cont\nThe average life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 kcal PPD (95% CI: 24.674, 41.517).\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nInterpretation for $\\widehat{\\beta}_1$\n:::\n\n::: def-cont\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nInterpretation for $\\widehat{\\beta}_2$\n:::\n\n::: proof-cont\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012).\n:::\n:::\n:::\n:::\n\n## What we need in our interpretations of coefficients (reference)\n\n::: columns\n::: column\n-   Units of Y\n\n-   Units of X\n\n-   If discussing intercept: Mean or average or expected before Y\n\n-   If discussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\n    -   OR: Mean or average or expected before Y\n    -   NOT: predicted\n    -   Only need before difference or Y!!\n\n-   Confidence interval\n:::\n\n::: column\n-   If other covariates in the model\n\n    -   Discussing intercept: Must state that variables are equal to 0\n\n        -   or at their centered value if centered!\n\n    -   Discussing coefficient for covariate: Must state \"adjusting for all other variables\", \"Controlling for all other variables\", or \"Holding all other variables constant\"\n\n        -   If only one other variable in the model, then replace \"all other variables\" with the single variable name\n:::\n:::\n\n# Learning Objectives\n\n1.  Understand the population multiple linear regression model through equations and visuals.\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n\n::: lob\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n:::\n\n## How do we estimate the model parameters?\n\n-   We need to estimate the population model coefficients $\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k$\n-   This can be done using the **ordinary least-squares method**\n    -   Find the $\\widehat{\\beta}$ values that **minimize** the sum of squares due to error ($SSE$)\n\n$$ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0 +\\widehat{\\beta}_1 X_{i1}+ \\ldots+\\widehat{\\beta}_1 X_{ik}))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 X_{i1}- \\ldots-\\widehat{\\beta}_1 X_{ik})^2\n\\end{aligned}$$\n\n## Technical side note (not needed in our class)\n\n-   The equations for calculating the $\\boldsymbol{\\widehat{\\beta}}$ values is best done using matrix notation (not required for our class)\n\n-   We will be using `R` to get the coefficients instead of the equation (already did this a few slides back!)\n\n-   How we have represented the population regression model: $$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon$$\n\n::: columns\n::: {.column width=\"40%\"}\n-   How to represent population model with matrix notation:\n\n$$\\begin{aligned}\n\\boldsymbol{Y} &= \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\\\\n\\boldsymbol{Y}_{n \\times 1}& = \\boldsymbol{X}_{n \\times (k+1)}\\boldsymbol{\\beta}_{(k+1)\\times 1} + \\boldsymbol{\\epsilon}_{n \\times 1}\n\\end{aligned}$$\n\n- $\\boldsymbol{X}$ is often called the design matrix\n  - Each row represents an individual \n  - Each column represents a covariate\n:::\n\n::: {.column width=\"20%\"}\n$$\n\\boldsymbol{Y} = \\left[\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\n\\end{array} \\right]_{n \\times 1} \n$$ $$\n\\boldsymbol{\\epsilon} = \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n\n\\end{array} \\right]_{n \\times 1}  \n$$\n:::\n\n::: {.column width=\"40%\"}\n$$\n\\boldsymbol{X} = \\left[ \\begin{array}{ccccc} 1 &  X_{11} &  X_{12} & \\ldots & X_{1,k} \\\\\n1 &X_{21} &  X_{22} & \\ldots & X_{2,k} \\\\\n\\vdots&\\vdots & \\vdots &  \\ldots & \\vdots \\\\\n1 & X_{n1} &  X_{n2} & \\ldots & X_{n,k} \\end{array} \\right]_{n \\times (k+1)}\n$$\n\n$$\n\\boldsymbol{\\beta}  = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1\\\\  \\vdots \\\\\n\\beta_{k}\n\\end{array} \\right]_{(k+1)\\times 1}\n$$\n:::\n:::\n\n## LINE model assumptions\n\n::: columns\n::: column\n::: definition\n::: def-title\n**\\[L\\] Linearity** of relationship between variables\n:::\n\n::: def-cont\nThe mean value of $Y$ given any combination of $X_1, X_2, \\ldots, X_k$ values, is a linear function of $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k$:\n\n$$\\mu_{Y|X_1, \\ldots, X_k} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k$$\n:::\n:::\n:::\n\n::: column\n::: proof1\n::: proof-title\n**\\[I\\] Independence** of the $Y$ values\n:::\n\n::: proof-cont\nObservations ($X_1, X_2, \\ldots, X_k, Y$) are independent from one another\n:::\n:::\n:::\n\n::: column\n::: theorem\n::: thm-title\n**\\[N\\] Normality** of the $Y$'s given $X$ (residuals)\n:::\n\n::: thm-cont\n$Y$ has a normal distribution for any any combination of $X_1, X_2, \\ldots, X_k$ values\n\n-   Thus, the residuals are normally distributed\n:::\n:::\n:::\n\n::: column\n::: fact\n::: fact-title\n**\\[E\\] Equality** of variance of the residuals (homoscedasticity)\n:::\n\n::: fact-cont\nThe variance of $Y$ is the same for any any combination of $X_1, X_2, \\ldots, X_k$ values\n\n$$\\sigma^2_{Y|X_1, X_2, \\ldots, X_k} = Var(Y|X_1, X_2, \\ldots, X_k) = \\sigma^2$$\n:::\n:::\n:::\n:::\n\n## Summary of the LINE assumptions\n\n-   Equivalently, the **residuals** are independently and identically distributed (iid):\n    -   normal\n    -   with mean 0 and\n    -   constant variance $\\sigma^2$\n\n \n    \n- Residuals are still $\\widehat{\\epsilon}_i=Y_i - \\widehat{Y}_i$ for each observation\n  - It's just that $\\widehat{Y}_i$ is now calculated with many covariates ($X_1, X_2, \\ldots, X_k$)\n\n## Variation: Explained vs. Unexplained\n\n$$\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}$$\n\n-   $Y_i - \\overline{Y}$ = the deviation of $Y_i$ around the mean $\\overline{Y}$\n    -   the **total** amount deviation\n-   $\\widehat{Y}_i- \\overline{Y}$ = the deviation of the fitted value $\\widehat{Y}_i$ around the mean $\\overline{Y}$\n    -   the amount deviation **explained** by the regression at $X_{i1},\\ldots,X_{ik}$\n-   $Y_i - \\widehat{Y}_i$ = the deviation of the observation $Y$ around the fitted regression line\n    -   the amount deviation **unexplained** by the regression at $X_{i1},\\ldots,X_{ik}$\n\n## Poll Everywhere Question 4\n\n## Building the ANOVA table {visibility=\"hidden\"}\n\nANOVA table ($k$ = \\# of predictors, $n$ = \\# of observations)\n\n::: columns\n::: {.column width=\"8%\"}\n:::\n\n::: {.column width=\"84%\"}\n| Variation Source | df      | SS    | MS                        | test statistic        | p-value                 |\n|------------|------------|------------|------------|------------|------------|\n| Regression       | $k$     | $SSR$ | $MSR = \\frac{SSR}{k}$     | $F = \\frac{MSR}{MSE}$ | $P(F_{(k, n-k-1)}>F)$ |\n| Error            | $n-k-1$ | $SSE$ | $MSE = \\frac{SSE}{n-k-1}$ |                       |                         |\n| Total            | $n-1$   | $SSY$ |                           |                       |                         |\n:::\n:::\n\n \n\n```{r}\n#| eval: false\n#| echo: false\nanova(mr1) %>% tidy() %>% gt() %>%\n   tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n## SLR: Another way to think of SSY, SSR, and SSE\n\n-   Let's create a data frame of each component within the SS's\n\n    -   Deviation in SSY: $Y_i - \\overline{Y}$\n    -   Deviation in SSR: $\\widehat{Y}_i- \\overline{Y}$\n    -   Deviation in SSE: $Y_i - \\widehat{Y}_i$\n\n-   Using our simple linear regression model as an example:\n\n```{r}\nslr1 <- gapm_sub %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\naug_slr1 = augment(slr1)\nSS_dev_slr = gapm_sub %>% select(LifeExpectancyYrs) %>%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_slr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_slr1$.resid)\n```\n\n## *SLR*: Plot the components of each sum of squares\n\n```{r}\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Code to make the below plots\"\n\nSSY_plot = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n```\n\n::: columns\n::: column\n```{r}\n#| fig-align: right\n#| fig-width: 6\n#| fig-height: 8\n#| echo: false\n\nSSY_plot_slr = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))\nSSR_plot_slr = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))\nSSE_plot_slr = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i])) + theme(axis.title.x = element_text(size = 22))\ngrid.arrange(SSY_plot_slr, SSR_plot_slr, SSE_plot_slr, nrow = 3)\n```\n:::\n\n::: column\n$$SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = `r round(var(SS_dev_slr$SSY_dev), 2)`$$ \n\n \n\n$$SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = `r round(var(SS_dev_slr$SSR_dev), 2)`$$\n\n \n\n$$SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = `r round(var(SS_dev_slr$SSE_dev), 2)`$$\n:::\n:::\n\n## *MLR:* Another way to think of SSY, SSR, and SSE\n\n-   Let's create a data frame of each component within the SS's\n\n    -   Deviation in SSY: $Y_i - \\overline{Y}$\n    -   Deviation in SSR: $\\widehat{Y}_i- \\overline{Y}$\n    -   Deviation in SSE: $Y_i - \\widehat{Y}_i$\n\n-   Using our simple linear regression model as an example:\n\n```{r}\nmr1 <- gapm_sub %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\naug_mlr1 = augment(mr1)\nSS_df = gapm_sub %>% select(LifeExpectancyYrs) %>%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_mlr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_mlr1$.resid)\n```\n\n## *MLR:* Plot the components of each sum of squares\n\n```{r}\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Code to make the below plots\"\n\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n```\n\n::: columns\n::: column\n```{r}\n#| fig-align: right\n#| fig-width: 6\n#| fig-height: 8\n#| echo: false\n\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i])) + theme(axis.title.x = element_text(size = 22))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n```\n:::\n\n::: column\n$$SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = `r round(var(SS_df$SSY_dev), 2)`$$ \n\n \n\n$$SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = `r round(var(SS_df$SSR_dev), 2)`$$\n\n \n\n$$SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = `r round(var(SS_df$SSE_dev), 2)`$$\n:::\n:::\n\n## What did you notice in the plots?\n\n::: columns\n::: {.column width=\"50%\"}\n::: L1\nSimple Linear Regression\n:::\n:::\n::: {.column width=\"50%\"}\n::: E1\nMultiple Linear Regression\n:::\n:::\n:::\n\n::: columns\n::: {.column width=\"35%\"}\n```{r}\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 8\n#| echo: false\n\ngrid.arrange(SSY_plot_slr, SSR_plot_slr, SSE_plot_slr, nrow = 3)\n```\n:::\n\n::: {.column width=\"15%\"}\n$$SSY = `r round(var(SS_dev_slr$SSY_dev), 2)`$$ \n\n \n\n \n\n$$SSR = `r round(var(SS_dev_slr$SSR_dev), 2)`$$\n\n \n\n \n\n$$SSE =`r round(var(SS_dev_slr$SSE_dev), 2)`$$\n:::\n\n::: {.column width=\"35%\"}\n```{r}\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 8\n#| echo: false\n\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n```\n:::\n::: {.column width=\"15%\"}\n$$SSY = `r round(var(SS_df$SSY_dev), 2)`$$ \n\n \n\n \n\n$$SSR = `r round(var(SS_df$SSR_dev), 2)`$$\n\n \n\n \n\n$$SSE =`r round(var(SS_df$SSE_dev), 2)`$$\n:::\n:::\n\n- Next class: we can determine if model fit is better by comparing the SSE's of different models\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: \"setup\" \n#| include: false\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\nlibrary(readxl)\n#----------\n# new packages\n# install.packages(\"describedata\")\nlibrary(describedata) # gladder()\nlibrary(gridExtra)   # grid.arrange()\nlibrary(ggfortify)  # autoplot(model)\n# New Day 6\nlibrary(gtsummary)\n\n# New Day 7\nlibrary(plotly) # for plot_ly() command\nlibrary(GGally) # for ggpairs() command \nlibrary(ggiraphExtra)   # for ggPredict() command\n\n```\n\n# Learning Objectives\n\n1.  Understand the population multiple linear regression model through equations and visuals.\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n\n\n```{css, echo=FALSE}\n.reveal code {\n  max-height: 100% !important;\n}\n```\n\n## Reminder of what we learned in the context of SLR\n\n-   SLR helped us establish the foundation for a lot of regression\n\n    -   But we do not usually use SLR in analysis\n\n**What did we learn in SLR??**\n\n::: columns\n::: {.column width=\"31%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Ordinary least squares (OLS)\n-   `lm()` function in R\n:::\n:::\n:::\n\n::: {.column width=\"36%\"}\n::: RAP4\n::: RAP4-title\nModel Use\n:::\n\n::: RAP4-cont\n-   Inference for variance of residuals\n-   Hypothesis testing for coefficients\n-   Interpreting population coefficient estimates\n-   Calculated the expected mean for specific $X$ values\n-   Interpreted coefficient of determination\n:::\n:::\n:::\n\n::: {.column width=\"32%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation/Diagnostics\n:::\n\n::: RAP3-cont\n-   LINE Assumptions\n-   Influential points\n-   Data Transformations\n:::\n:::\n:::\n:::\n\n## Let's map that to our regression analysis process\n\n::: box\n![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"62.1%\" width=\"155\"} ![](../img_slides/arrow2.png){.absolute top=\"13.5%\" right=\"28.4%\" width=\"155\"}![](../img_slides/arrow_back4.png){.absolute top=\"7.5%\" right=\"30.5%\" width=\"820\"} ![](../img_slides/arrow_down.png){.absolute top=\"60.5%\" right=\"48%\" width=\"85\"}\n\n::: columns\n::: {.column width=\"30%\"}\n::: RAP1\n::: RAP1-title\nModel Selection\n:::\n\n::: RAP1-cont\n-   Building a model\n\n-   Selecting variables\n\n-   Prediction vs interpretation\n\n-   Comparing potential models\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP2\n::: RAP2-title\nModel Fitting\n:::\n\n::: RAP2-cont\n-   Find best fit line\n\n-   Using OLS in this class\n\n-   Parameter estimation\n\n-   Categorical covariates\n\n-   Interactions\n:::\n:::\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"30%\"}\n::: RAP3\n::: RAP3-title\nModel Evaluation\n:::\n\n::: RAP3-cont\n-   Evaluation of model fit\n-   Testing model assumptions\n-   Residuals\n-   Transformations\n-   Influential points\n-   Multicollinearity\n:::\n:::\n:::\n:::\n:::\n\n::: RAP4\n::: RAP4-title\nModel Use (Inference)\n:::\n\n::: RAP4-cont\n::: columns\n::: {.column width=\"50%\"}\n-   Inference for coefficients\n-   Hypothesis testing for coefficients\n:::\n\n::: {.column width=\"50%\"}\n-   Inference for expected $Y$ given $X$\n:::\n:::\n:::\n:::\n\n## \n\n![](../img_slides/all_mod_wrong.jpg){fig-align=\"center\"}\n\n# Learning Objectives\n\n::: lob\n1.  Understand the population multiple linear regression model through equations and visuals.\n:::\n\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n\n## Simple Linear Regression vs. Multiple Linear Regression\n\n::: columns\n::: {.column width=\"50%\"}\n::: L1\nSimple Linear Regression\n:::\n\n \n\n::: L2\nWe use **one predictor** to try to explain the variance of the outcome\n\n$$\nY = \\beta_0 + \\beta_1 X + \\epsilon\n$$\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: E1\nMultiple Linear Regression\n:::\n\n \n\n::: E2\nWe use **multiple predictors** to try to explain the variance of the outcome\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_{k}X_{k}+ \\epsilon\n$$\n:::\n\n \n\n-   Has $k+1$ total coefficients (including intercept) for $k$ predictors/covariates\n-   Sometimes referred to as ***multivariable*** linear regression, but *never multivariate*\n:::\n:::\n\n \n\n-   The models have similar \"LINE\" assumptions and follow the same general diagnostic procedure\n\n## Population multiple regression model\n\n::: heq\n$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon$$\n:::\n\nor on the individual (observation) level:\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}+ \\ldots + \\beta_k x_{ik} + \\epsilon_i,\\ \\ \\text{for}\\ i = 1, 2, \\ldots, n$$\n\n::: columns\n::: column\n#### Observable sample data\n\n-   $Y$ is our dependent variable\n\n    -   Aka outcome or response variable\n\n-   $X_1, X_2, \\ldots, X_k$ are our $k$ independent variables\n\n    -   Aka predictors or covariates\n:::\n\n::: column\n#### Unobservable population parameters\n\n-   $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k$ are **unknown** population **parameters**\n    -   From our sample, we find the population parameter estimates: $\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k$\n-   $\\epsilon$ is the random error\n    -   And is still normally distributed\n    -   $\\epsilon \\sim N(0, \\sigma^2)$ where $\\sigma^2$ is the population parameter of the variance\n:::\n:::\n\n## Going back to our life expectancy example\n\n-   Let's say many other variables were measured for each country, including food supply\n\n    -   [**Food Supply**](https://www.fao.org/faostat/en/#home) (kilocalories per person per day, kc PPD): the average kilocalories consumed by a person each day.\n\n-   In SLR, we only had one predictor and one outcome in the model:\n\n    -   Outcome: [**Life expectancy**](https://www.gapminder.org/data/documentation/gd004/) = the average number of years a newborn child would live if current mortality patterns were to stay the same.\n\n    -   Predictor: [**Adult literacy rate**](http://data.uis.unesco.org/)(predictor) is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n \n\n-   Do we think adult female literacy rate is going to explain a lot of the variance of life expectancy between countries?\n\n## Loading the data\n\n```{r}\n# Load the data - update code if the file is not in the same location\n# on your computer\ngapm <- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\ngapm_sub <- gapm %>% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)\n  # above drops rows with NAs in any of the three variables\n\nglimpse(gapm_sub)\n```\n\n## Can we improve our model by adding food supply as a covariate?\n\n::: columns\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"80%\"}\n::: definition\n::: def-title\nSimple linear regression population model\n:::\n\n::: def-cont\n$$\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\epsilon\n\\end{aligned}$$\n:::\n:::\n\n::: fact\n::: fact-title\nMultiple linear regression population model (with added Food Supply)\n:::\n\n::: fact-cont\n$$\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\n\\end{aligned}$$\n:::\n:::\n:::\n\n::: {.column width=\"10%\"}\n:::\n:::\n\n## Visualize relationship between life expectancy, female literacy rate, and food supply\n\n```{r fig.width=8, fig.height=3}\n#| echo: false\n\nplot_LE_FLR <- ggplot(gapm_sub, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"#F14124\") +\n  labs(title = \"Life expectancy vs.\\n female literacy rate\", \n       x = \"Female Litaracy Rate (%)\", \n       y = \"Life Expectancy (yrs)\") +\n  theme(axis.title = element_text(size = 10), \n        axis.text = element_text(size = 10), \n        title = element_text(size = 10))\n\nplot_LE_FS <- ggplot(gapm_sub, aes(x = FoodSupplykcPPD,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"#F14124\") +\n  labs(title = \"Life expectancy vs. \\n food supply\", \n       x = \"Food supply (kcal PPD)\", \n       y = \"Life Expectancy (yrs)\") +\n  theme(axis.title = element_text(size = 10), \n        axis.text = element_text(size = 10), \n        title = element_text(size = 10))\n\nplot_FLR_FS <- ggplot(gapm_sub, aes(x = FoodSupplykcPPD,\n                 y = FemaleLiteracyRate)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"#F14124\") +\n  labs(title = \"Female literacy rate vs. \\n food supply\", \n       y = \"Female Litaracy Rate (%)\", \n       x = \"Food supply (kcal PPD)\") +\n  theme(axis.title = element_text(size = 10), \n        axis.text = element_text(size = 10), \n        title = element_text(size = 10))\n\ngrid.arrange(plot_LE_FLR, plot_LE_FS, plot_FLR_FS,\n             nrow = 1)\n```\n\n## Visualize relationship in 3-D\n\n::: columns\n::: {.column width=\"25%\"}\n:::\n\n::: {.column width=\"50%\"}\n```{r fig.height=8, fig.width=8, warning=F}\n#| echo: false\n#| fig-align: center\n# plotly package is require for plot_ly function, which is loaded at beginning of Rmd \n# z = response variable\n# x & y are predictor variables\n\ndim3_scatter <- plot_ly(gapm_sub, \n                       x = ~FemaleLiteracyRate, \n                       y = ~FoodSupplykcPPD, \n                       z = ~LifeExpectancyYrs) %>%\n  add_markers() %>%\n  layout(scene = list(xaxis = list(title = 'Female literacy rate'),\n                     yaxis = list(title = 'Food supply (kc PPD)'),\n                     zaxis = list(title = 'Life expectancy')))\ndim3_scatter\n```\n:::\n\n::: {.column width=\"25%\"}\n:::\n:::\n\n## Poll Everywhere Question 1\n\n# Learning Objectives\n\n1.  Understand the population multiple linear regression model through equations and visuals.\n\n::: lob\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n:::\n\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n\n## How do we fit a multiple linear regression model in R?\n\nNew population model for example:\n\n$$\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon$$\n\n```{r}\n# Fit regression model:\nmr1 <- gapm_sub %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\ntidy(mr1, conf.int=T) %>% gt() %>% tab_options(table.font.size = 35) %>% fmt_number(decimals = 3)\n```\n\n```{r}\n#| echo: false\n\nmr1_tidy = tidy(mr1, conf.int=T)\n```\n\nFitted multiple regression model:\n\n$$\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR} \n+ `r round(mr1_tidy$estimate[3], 3)`\\ \\text{FS}\n\\end{aligned}$$\n\n## Don't forget `summary()` to extract information!\n\n```{r}\nsummary(mr1)\n```\n\n## Visualize the fitted multiple regression model\n\n::: columns\n::: {.column width=\"40%\"}\n-   The fitted model equation $$\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X_1 + \\widehat{\\beta}_2 \\cdot X_2$$ has three variables ($Y, X_1,$ and $X_2$) and thus we need 3 dimensions to plot it\n\n \n\n-   Instead of a regression line, we get a **regression plane**\n    -   *See code in `.qmd`- file. I hid it from view in the html file.*\n:::\n\n::: {.column width=\"60%\"}\n```{r fig.height=10, fig.width=10, echo=F, warning=FALSE}\n#| fig-align: center\n# setup hideous grid required by plotly\nmr1 <- gapm_sub %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\nx_grid <- seq(from = min(gapm_sub$FemaleLiteracyRate, na.rm = TRUE), \n              to = max(gapm_sub$FemaleLiteracyRate, na.rm = TRUE), \n              length = 100)\ny_grid <- seq(from = min(gapm_sub$FoodSupplykcPPD, na.rm = TRUE), \n              to = max(gapm_sub$FoodSupplykcPPD, na.rm = TRUE), \n              length = 200)\nz_grid <- expand.grid(x_grid, y_grid) %>%\n  as_tibble() %>%\n  rename(x_grid = Var1, y_grid = Var2) %>%\n  mutate(z = coef(mr1)[1] + coef(mr1)[2]*x_grid + coef(mr1)[3]*y_grid) %>%\n  .[[\"z\"]] %>%\n  matrix(nrow=length(x_grid)) %>%\n  t()\n\n# Plot points\nplot_ly() %>%\n  add_markers(\n    x = gapm_sub$FemaleLiteracyRate,\n    y = gapm_sub$FoodSupplykcPPD,\n    z = gapm_sub$LifeExpectancyYrs,\n    hoverinfo = 'text',\n    text = ~paste(\"x1 - Female literacy rate: \",\n                  gapm_sub$FemaleLiteracyRate,\n                  \"</br> x2 - Food supply (kc PPD): \",\n                  gapm_sub$FoodSupplykcPPD,\n                  \"</br> y - Life expectancy: \",\n                  gapm_sub$LifeExpectancyYrs)\n  ) %>%\n  # Label axes\n  layout(\n    scene = list(\n      xaxis = list(title = \"x1 - Female literacy rate\"),\n      yaxis = list(title = \"x2 - Food supply (kc PPD)\"),\n      zaxis = list(title = \"y - Life expectancy\")\n    )\n  ) %>%\n  # Add regression plane\n  add_surface(\n    x = x_grid,\n    y = y_grid,\n    z = z_grid\n  )\n\n```\n:::\n:::\n\n## Regression lines for varying values of food supply\n\n$$\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR} \n+ `r round(mr1_tidy$estimate[3], 3)`\\ \\text{FS}\n\\end{aligned}$$\n\n::: columns\n::: {.column width=\"35%\"}\n-   Note: when the food supply is held constant but the female literacy rate varies...\n    -   then the outcome values change along a **line**\n-   Different values of food supply give different lines\n    -   The intercepts change, but\n    -   the slopes stay the same (parallel lines)\n:::\n\n::: {.column width=\"65%\"}\n```{r}\n#| fig-width: 6\n#| fig-height: 3.5\n#| fig-align: center\n\n(mr1_2d = ggPredict(mr1, interactive = T))\n```\n:::\n:::\n\n## How do we calculate the regression line for 3000 kc PPD food supply?\n\n::: columns\n::: {.column width=\"40%\"}\n \n\n$$\\begin{aligned}\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR} \n+ `r round(mr1_tidy$estimate[3], 3)`\\ \\text{FS}\\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR} \n+ `r round(mr1_tidy$estimate[3], 3)`\\cdot 3000\\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR}\n+ `r round(mr1_tidy$estimate[3]*3000, 3)` \\\\\n\\widehat{\\text{LE}} &= `r round(mr1_tidy$estimate[1] + mr1_tidy$estimate[3]*3000, 3)` + `r round(mr1_tidy$estimate[2], 3)` \\ \\text{FLR}\n\\end{aligned}$$\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| fig-width: 6\n#| fig-height: 4\n#| fig-align: center\n\n(mr1_2d = ggPredict(mr1, interactive = T))\n```\n:::\n:::\n\n## Poll Everwhere Question 2\n\n# Learning Objectives\n\n1.  Understand the population multiple linear regression model through equations and visuals.\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n\n::: lob\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n:::\n\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n\n## Interpreting the estimated population coefficients\n\n-   For a population model: $$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon$$\n    -   Where $X_1$ and $X_2$ are continuous variables\n    -   No need to specify $Y$ because it required to be continuous in linear regression\n\n::: columns\n::: {.column width=\"33%\"}\n::: fact\n::: fact-title\nGeneral interpretation for $\\widehat{\\beta}_0$\n:::\n\n::: fact-cont\nThe expected $Y$-variable is ($\\widehat\\beta_0$ units) when the $X_1$-variable is 0 $X_1$-units and $X_2$-variable is 0 $X_1$-units (95% CI: LB, UB).\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nGeneral interpretation for $\\widehat{\\beta}_1$\n:::\n\n::: def-cont\nFor every increase of 1 $X_1$-unit in the $X_1$-variable, adjusting/controlling for $X_2$-variable, there is an expected increase/decrease of $|\\widehat\\beta_1|$ units in the $Y$-variable (95%: LB, UB).\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nGeneral interpretation for $\\widehat{\\beta}_2$\n:::\n\n::: proof-cont\nFor every increase of 1 $X_2$-unit in the $X_2$-variable, adjusting/controlling for $X_1$-variable, there is an expected increase/decrease of $|\\widehat\\beta_2|$ units in the $Y$-variable (95%: LB, UB).\n:::\n:::\n:::\n:::\n\n\n## Interpreting the estimated population coefficient: $\\widehat{\\beta}_0$ {#color-slide1}\n\n```{css, echo=FALSE}\n#color-slide1 h2 {\n color: #34AC8B;\n}\n```\n\n- For an estimated model: $\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_2 + \\widehat{\\beta}_2 X_2$\n\n$$\\begin{aligned}\n\\widehat{Y} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 0 + \\widehat{\\beta}_2 0\\\\\n\\widehat{Y} = &\\widehat{\\beta}_0 \\\\\n\\end{aligned}$$\n\n**Interpretation:** The expected $Y$-variable is ($\\widehat\\beta_0$ units) when the $X_1$-variable is 0 $X_1$-units and $X_2$-variable is 0 $X_1$-units (95% CI: LB, UB).\n\n## Interpreting the estimated population coefficient: $\\widehat{\\beta}_1$ {#color-slide2}\n\n```{css, echo=FALSE}\n#color-slide2 h2 {\n color: #4FADF3;\n}\n```\n\n- We will use: $x_{1a}$ and $x_{1b} = x_{1a} + 1$, with the implication that $\\Delta{x_1} = x_{1b} - x_{1a} = 1$\n\n- Our goal is to get to a statement with $\\widehat{\\beta}_1$ alone:\n\n$$\\begin{aligned}\n\\widehat{Y}|x_{1a} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{1b} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = & \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\bigg] - \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\bigg] \\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1 x_{1b} - \\widehat{\\beta}_1 x_{1a}\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1 (x_{1b} - x_{1a})\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1\\\\\n\\end{aligned}$$\n\n**Interpretation:** For every increase of 1 $X_1$-unit in the $X_1$-variable, adjusting/controlling for $X_2$-variable, there is an expected increase/decrease of $|\\widehat\\beta_1|$ units in the $Y$-variable (95%: LB, UB).\n\n\n## Interpreting the estimated population coefficient: $\\widehat{\\beta}_2$ {#color-slide3}\n\n```{css, echo=FALSE}\n#color-slide3 h2 {\n color: #FF8021;\n}\n```\n\n- We can so the same for $X_2$: $x_{2a}$ and $x_{2b} = x_{2a} + 1$, with the implication that $\\Delta{x_2} = x_{2b} - x_{2a} = 1$\n\n- Our goal is to get to a statement with $\\widehat{\\beta}_1$ alone:\n\n$$\\begin{aligned}\n\\widehat{Y}|x_{2a} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{2b} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = & \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 x_{2b} \\bigg] - \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 x_{2a}\\bigg] \\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2 x_{2b} - \\widehat{\\beta}_2 x_{2a}\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2 (x_{2b} - x_{2a})\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2\\\\\n\\end{aligned}$$\n\n**Interpretation:** For every increase of 1 $X_2$-unit in the $X_2$-variable, adjusting/controlling for $X_1$-variable, there is an expected increase/decrease of $|\\widehat\\beta_2|$ units in the $Y$-variable (95%: LB, UB).\n\n\n## Poll Everywhere Question 3\n\n## Getting these interpretations from our regression table\n\nWe fit the regression model in R and printed the regression table:\n\n```{r}\nmr1 <- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n```\n\n```{r}\n#| echo: false\ntidy(mr1, conf.int=T) %>% gt() %>% tab_options(table.font.size = 30) %>% fmt_number(decimals = 3)\n```\n\nFitted multiple regression model: $\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}$\n\n::: columns\n::: {.column width=\"33%\"}\n::: fact\n::: fact-title\nInterpretation for $\\widehat{\\beta}_0$\n:::\n\n::: fact-cont\nThe average life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 kcal PPD (95% CI: 24.674, 41.517).\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: definition\n::: def-title\nInterpretation for $\\widehat{\\beta}_1$\n:::\n\n::: def-cont\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n:::\n:::\n:::\n\n::: {.column width=\"33%\"}\n::: proof1\n::: proof-title\nInterpretation for $\\widehat{\\beta}_2$\n:::\n\n::: proof-cont\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012).\n:::\n:::\n:::\n:::\n\n## What we need in our interpretations of coefficients (reference)\n\n::: columns\n::: column\n-   Units of Y\n\n-   Units of X\n\n-   If discussing intercept: Mean or average or expected before Y\n\n-   If discussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\n    -   OR: Mean or average or expected before Y\n    -   NOT: predicted\n    -   Only need before difference or Y!!\n\n-   Confidence interval\n:::\n\n::: column\n-   If other covariates in the model\n\n    -   Discussing intercept: Must state that variables are equal to 0\n\n        -   or at their centered value if centered!\n\n    -   Discussing coefficient for covariate: Must state \"adjusting for all other variables\", \"Controlling for all other variables\", or \"Holding all other variables constant\"\n\n        -   If only one other variable in the model, then replace \"all other variables\" with the single variable name\n:::\n:::\n\n# Learning Objectives\n\n1.  Understand the population multiple linear regression model through equations and visuals.\n2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.\n3.  Interpret MLR (population) coefficient estimates with additional variable in model\n\n::: lob\n4.  Based off of previous SLR work, understand how the population MLR is estimated.\n:::\n\n## How do we estimate the model parameters?\n\n-   We need to estimate the population model coefficients $\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k$\n-   This can be done using the **ordinary least-squares method**\n    -   Find the $\\widehat{\\beta}$ values that **minimize** the sum of squares due to error ($SSE$)\n\n$$ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0 +\\widehat{\\beta}_1 X_{i1}+ \\ldots+\\widehat{\\beta}_1 X_{ik}))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 X_{i1}- \\ldots-\\widehat{\\beta}_1 X_{ik})^2\n\\end{aligned}$$\n\n## Technical side note (not needed in our class)\n\n-   The equations for calculating the $\\boldsymbol{\\widehat{\\beta}}$ values is best done using matrix notation (not required for our class)\n\n-   We will be using `R` to get the coefficients instead of the equation (already did this a few slides back!)\n\n-   How we have represented the population regression model: $$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon$$\n\n::: columns\n::: {.column width=\"40%\"}\n-   How to represent population model with matrix notation:\n\n$$\\begin{aligned}\n\\boldsymbol{Y} &= \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\\\\n\\boldsymbol{Y}_{n \\times 1}& = \\boldsymbol{X}_{n \\times (k+1)}\\boldsymbol{\\beta}_{(k+1)\\times 1} + \\boldsymbol{\\epsilon}_{n \\times 1}\n\\end{aligned}$$\n\n- $\\boldsymbol{X}$ is often called the design matrix\n  - Each row represents an individual \n  - Each column represents a covariate\n:::\n\n::: {.column width=\"20%\"}\n$$\n\\boldsymbol{Y} = \\left[\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\n\\end{array} \\right]_{n \\times 1} \n$$ $$\n\\boldsymbol{\\epsilon} = \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n\n\\end{array} \\right]_{n \\times 1}  \n$$\n:::\n\n::: {.column width=\"40%\"}\n$$\n\\boldsymbol{X} = \\left[ \\begin{array}{ccccc} 1 &  X_{11} &  X_{12} & \\ldots & X_{1,k} \\\\\n1 &X_{21} &  X_{22} & \\ldots & X_{2,k} \\\\\n\\vdots&\\vdots & \\vdots &  \\ldots & \\vdots \\\\\n1 & X_{n1} &  X_{n2} & \\ldots & X_{n,k} \\end{array} \\right]_{n \\times (k+1)}\n$$\n\n$$\n\\boldsymbol{\\beta}  = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1\\\\  \\vdots \\\\\n\\beta_{k}\n\\end{array} \\right]_{(k+1)\\times 1}\n$$\n:::\n:::\n\n## LINE model assumptions\n\n::: columns\n::: column\n::: definition\n::: def-title\n**\\[L\\] Linearity** of relationship between variables\n:::\n\n::: def-cont\nThe mean value of $Y$ given any combination of $X_1, X_2, \\ldots, X_k$ values, is a linear function of $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k$:\n\n$$\\mu_{Y|X_1, \\ldots, X_k} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k$$\n:::\n:::\n:::\n\n::: column\n::: proof1\n::: proof-title\n**\\[I\\] Independence** of the $Y$ values\n:::\n\n::: proof-cont\nObservations ($X_1, X_2, \\ldots, X_k, Y$) are independent from one another\n:::\n:::\n:::\n\n::: column\n::: theorem\n::: thm-title\n**\\[N\\] Normality** of the $Y$'s given $X$ (residuals)\n:::\n\n::: thm-cont\n$Y$ has a normal distribution for any any combination of $X_1, X_2, \\ldots, X_k$ values\n\n-   Thus, the residuals are normally distributed\n:::\n:::\n:::\n\n::: column\n::: fact\n::: fact-title\n**\\[E\\] Equality** of variance of the residuals (homoscedasticity)\n:::\n\n::: fact-cont\nThe variance of $Y$ is the same for any any combination of $X_1, X_2, \\ldots, X_k$ values\n\n$$\\sigma^2_{Y|X_1, X_2, \\ldots, X_k} = Var(Y|X_1, X_2, \\ldots, X_k) = \\sigma^2$$\n:::\n:::\n:::\n:::\n\n## Summary of the LINE assumptions\n\n-   Equivalently, the **residuals** are independently and identically distributed (iid):\n    -   normal\n    -   with mean 0 and\n    -   constant variance $\\sigma^2$\n\n \n    \n- Residuals are still $\\widehat{\\epsilon}_i=Y_i - \\widehat{Y}_i$ for each observation\n  - It's just that $\\widehat{Y}_i$ is now calculated with many covariates ($X_1, X_2, \\ldots, X_k$)\n\n## Variation: Explained vs. Unexplained\n\n$$\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}$$\n\n-   $Y_i - \\overline{Y}$ = the deviation of $Y_i$ around the mean $\\overline{Y}$\n    -   the **total** amount deviation\n-   $\\widehat{Y}_i- \\overline{Y}$ = the deviation of the fitted value $\\widehat{Y}_i$ around the mean $\\overline{Y}$\n    -   the amount deviation **explained** by the regression at $X_{i1},\\ldots,X_{ik}$\n-   $Y_i - \\widehat{Y}_i$ = the deviation of the observation $Y$ around the fitted regression line\n    -   the amount deviation **unexplained** by the regression at $X_{i1},\\ldots,X_{ik}$\n\n## Poll Everywhere Question 4\n\n## Building the ANOVA table {visibility=\"hidden\"}\n\nANOVA table ($k$ = \\# of predictors, $n$ = \\# of observations)\n\n::: columns\n::: {.column width=\"8%\"}\n:::\n\n::: {.column width=\"84%\"}\n| Variation Source | df      | SS    | MS                        | test statistic        | p-value                 |\n|------------|------------|------------|------------|------------|------------|\n| Regression       | $k$     | $SSR$ | $MSR = \\frac{SSR}{k}$     | $F = \\frac{MSR}{MSE}$ | $P(F_{(k, n-k-1)}>F)$ |\n| Error            | $n-k-1$ | $SSE$ | $MSE = \\frac{SSE}{n-k-1}$ |                       |                         |\n| Total            | $n-1$   | $SSY$ |                           |                       |                         |\n:::\n:::\n\n \n\n```{r}\n#| eval: false\n#| echo: false\nanova(mr1) %>% tidy() %>% gt() %>%\n   tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)\n```\n\n## SLR: Another way to think of SSY, SSR, and SSE\n\n-   Let's create a data frame of each component within the SS's\n\n    -   Deviation in SSY: $Y_i - \\overline{Y}$\n    -   Deviation in SSR: $\\widehat{Y}_i- \\overline{Y}$\n    -   Deviation in SSE: $Y_i - \\widehat{Y}_i$\n\n-   Using our simple linear regression model as an example:\n\n```{r}\nslr1 <- gapm_sub %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\naug_slr1 = augment(slr1)\nSS_dev_slr = gapm_sub %>% select(LifeExpectancyYrs) %>%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_slr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_slr1$.resid)\n```\n\n## *SLR*: Plot the components of each sum of squares\n\n```{r}\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Code to make the below plots\"\n\nSSY_plot = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n```\n\n::: columns\n::: column\n```{r}\n#| fig-align: right\n#| fig-width: 6\n#| fig-height: 8\n#| echo: false\n\nSSY_plot_slr = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))\nSSR_plot_slr = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))\nSSE_plot_slr = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i])) + theme(axis.title.x = element_text(size = 22))\ngrid.arrange(SSY_plot_slr, SSR_plot_slr, SSE_plot_slr, nrow = 3)\n```\n:::\n\n::: column\n$$SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = `r round(var(SS_dev_slr$SSY_dev), 2)`$$ \n\n \n\n$$SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = `r round(var(SS_dev_slr$SSR_dev), 2)`$$\n\n \n\n$$SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = `r round(var(SS_dev_slr$SSE_dev), 2)`$$\n:::\n:::\n\n## *MLR:* Another way to think of SSY, SSR, and SSE\n\n-   Let's create a data frame of each component within the SS's\n\n    -   Deviation in SSY: $Y_i - \\overline{Y}$\n    -   Deviation in SSR: $\\widehat{Y}_i- \\overline{Y}$\n    -   Deviation in SSE: $Y_i - \\widehat{Y}_i$\n\n-   Using our simple linear regression model as an example:\n\n```{r}\nmr1 <- gapm_sub %>% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\naug_mlr1 = augment(mr1)\nSS_df = gapm_sub %>% select(LifeExpectancyYrs) %>%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_mlr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_mlr1$.resid)\n```\n\n## *MLR:* Plot the components of each sum of squares\n\n```{r}\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Code to make the below plots\"\n\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n```\n\n::: columns\n::: column\n```{r}\n#| fig-align: right\n#| fig-width: 6\n#| fig-height: 8\n#| echo: false\n\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i])) + theme(axis.title.x = element_text(size = 22))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n```\n:::\n\n::: column\n$$SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = `r round(var(SS_df$SSY_dev), 2)`$$ \n\n \n\n$$SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = `r round(var(SS_df$SSR_dev), 2)`$$\n\n \n\n$$SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = `r round(var(SS_df$SSE_dev), 2)`$$\n:::\n:::\n\n## What did you notice in the plots?\n\n::: columns\n::: {.column width=\"50%\"}\n::: L1\nSimple Linear Regression\n:::\n:::\n::: {.column width=\"50%\"}\n::: E1\nMultiple Linear Regression\n:::\n:::\n:::\n\n::: columns\n::: {.column width=\"35%\"}\n```{r}\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 8\n#| echo: false\n\ngrid.arrange(SSY_plot_slr, SSR_plot_slr, SSE_plot_slr, nrow = 3)\n```\n:::\n\n::: {.column width=\"15%\"}\n$$SSY = `r round(var(SS_dev_slr$SSY_dev), 2)`$$ \n\n \n\n \n\n$$SSR = `r round(var(SS_dev_slr$SSR_dev), 2)`$$\n\n \n\n \n\n$$SSE =`r round(var(SS_dev_slr$SSE_dev), 2)`$$\n:::\n\n::: {.column width=\"35%\"}\n```{r}\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 8\n#| echo: false\n\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n```\n:::\n::: {.column width=\"15%\"}\n$$SSY = `r round(var(SS_df$SSY_dev), 2)`$$ \n\n \n\n \n\n$$SSR = `r round(var(SS_df$SSR_dev), 2)`$$\n\n \n\n \n\n$$SSE =`r round(var(SS_df$SSE_dev), 2)`$$\n:::\n:::\n\n- Next class: we can determine if model fit is better by comparing the SSE's of different models\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","highlight-style":"ayu","output-file":"09_MLR_Intro.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.25","auto-stretch":true,"title":"Lesson 9: Introduction to Multiple Linear Regression (MLR)","author":"Nicky Wakim","title-slide-attributes":{"data-background-color":"#213c96"},"date":"02/05/2025","theme":"../simple_NW.scss","chalkboard":true,"slideNumber":true,"showSlideNumber":"all","width":1955,"height":1100,"footer":"Lesson 9: MLR Intro"}}},"projectFormats":["html"]}